{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eae15b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b636134",
   "metadata": {},
   "source": [
    "## Optical digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f86ada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = './uci_repos/optical_digits/optdigits.tra'\n",
    "test_file = './uci_repos/optical_digits/optdigits.tes'\n",
    "column = ['pixel_'+str(i) for i in range(64)]\n",
    "column.append('digit_label')\n",
    "train_data = pd.read_csv(train_file, sep=',', header=None, names=column)\n",
    "test_data = pd.read_csv(test_file, sep=',', header=None, names=column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a777e21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel_0</th>\n",
       "      <th>pixel_1</th>\n",
       "      <th>pixel_2</th>\n",
       "      <th>pixel_3</th>\n",
       "      <th>pixel_4</th>\n",
       "      <th>pixel_5</th>\n",
       "      <th>pixel_6</th>\n",
       "      <th>pixel_7</th>\n",
       "      <th>pixel_8</th>\n",
       "      <th>pixel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_55</th>\n",
       "      <th>pixel_56</th>\n",
       "      <th>pixel_57</th>\n",
       "      <th>pixel_58</th>\n",
       "      <th>pixel_59</th>\n",
       "      <th>pixel_60</th>\n",
       "      <th>pixel_61</th>\n",
       "      <th>pixel_62</th>\n",
       "      <th>pixel_63</th>\n",
       "      <th>digit_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel_0  pixel_1  pixel_2  pixel_3  pixel_4  pixel_5  pixel_6  pixel_7  \\\n",
       "0        0        0        5       13        9        1        0        0   \n",
       "1        0        0        0       12       13        5        0        0   \n",
       "2        0        0        0        4       15       12        0        0   \n",
       "3        0        0        7       15       13        1        0        0   \n",
       "4        0        0        0        1       11        0        0        0   \n",
       "\n",
       "   pixel_8  pixel_9  ...  pixel_55  pixel_56  pixel_57  pixel_58  pixel_59  \\\n",
       "0        0        0  ...         0         0         0         6        13   \n",
       "1        0        0  ...         0         0         0         0        11   \n",
       "2        0        0  ...         0         0         0         0         3   \n",
       "3        0        8  ...         0         0         0         7        13   \n",
       "4        0        0  ...         0         0         0         0         2   \n",
       "\n",
       "   pixel_60  pixel_61  pixel_62  pixel_63  digit_label  \n",
       "0        10         0         0         0            0  \n",
       "1        16        10         0         0            1  \n",
       "2        11        16         9         0            2  \n",
       "3        13         9         0         0            3  \n",
       "4        16         4         0         0            4  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8240c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_all_labels(data, num=100, minor=[]):\n",
    "    # this function is to limit the number of labels that are used\n",
    "    # it returns the indexes according the labels\n",
    "    # data is an array of labels\n",
    "    '''\n",
    "\n",
    "    :param data: array of labels\n",
    "    :param num: number required\n",
    "    :param minor: list of minority indexes\n",
    "    :return: array of labels indexes\n",
    "    '''\n",
    "\n",
    "    labels = np.unique(data)\n",
    "    co_l = []\n",
    "    min_col =[]\n",
    "    if not minor:\n",
    "        for l in labels:\n",
    "            el_l = np.where(np.array(data) == l)\n",
    "            co_l.append(el_l[0])\n",
    "\n",
    "    else:\n",
    "        for l in labels:\n",
    "            if l in minor:\n",
    "                el_l = np.where(np.array(data) == l)\n",
    "                co_l.append((el_l[0])[:num])\n",
    "                min_col.append((el_l[0])[:num])\n",
    "            else:\n",
    "                el_l = np.where(np.array(data) == l)\n",
    "                co_l.append(el_l[0])\n",
    "    return co_l, min_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ff98f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_data.iloc[:,-1:].copy().to_numpy()\n",
    "train_images = train_data.iloc[:, 0:-1].copy().to_numpy()\n",
    "y_test = test_data.iloc[:,-1:].copy().to_numpy()\n",
    "test_images = test_data.iloc[:, 0:-1].copy().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b911ee5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0 : number of samples : 100\n",
      "class 1 : number of samples : 100\n",
      "class 2 : number of samples : 380\n",
      "class 3 : number of samples : 389\n",
      "class 4 : number of samples : 387\n",
      "class 5 : number of samples : 376\n",
      "class 6 : number of samples : 377\n",
      "class 7 : number of samples : 387\n",
      "class 8 : number of samples : 380\n",
      "class 9 : number of samples : 382\n"
     ]
    }
   ],
   "source": [
    "grouped_labels, min_label = group_all_labels(train_y, 100, [0, 1])\n",
    "gr_data = []\n",
    "gr_labels = [] \n",
    "for index, q in enumerate(grouped_labels):\n",
    "    print('class {} : number of samples : {}'.format(index,len(q)))\n",
    "    for r in q:\n",
    "        gr_data.append(train_images[r])\n",
    "        gr_labels.append(train_y[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fe6f415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2346dd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train image size :  (3258, 64)\n",
      "train y size :  (3258, 10)\n",
      "test image size :  (1797, 64)\n",
      "test y size :  (1797, 10)\n",
      "sample y : [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f93061a5640>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train_images = [np.reshape(x, (8,8, 1)) for x in train_images ]\n",
    "train_images = np.array(gr_data).astype(np.float32) / 16.0\n",
    "#test_images = [np.reshape(x, (8,8, 1)) for x in test_images ]\n",
    "test_images = np.array(test_images).astype(np.float32) / 16.0\n",
    "\n",
    "train_y = tf.keras.utils.to_categorical(gr_labels, num_classes=10, dtype='float32')\n",
    "test_y = tf.keras.utils.to_categorical(y_test, num_classes=10, dtype='float32')\n",
    "print('train image size : ', train_images.shape)\n",
    "print('train y size : ', train_y.shape)\n",
    "print('test image size : ', test_images.shape)\n",
    "print('test y size : ', test_y.shape)\n",
    "print('sample y :', train_y[0])\n",
    "plt.imshow(np.reshape(train_images[100],(8,8)), cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543bb96f",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d0db058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-08 09:44:42.406527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-08 09:44:42.415557: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2021-11-08 09:44:42.416692: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-11-08 09:44:42.418008: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "Loss: 5.3e+01\n",
      "Iter: 1000\n",
      "Loss: 2.4e+01\n",
      "Iter: 2000\n",
      "Loss: 2.2e+01\n",
      "Iter: 3000\n",
      "Loss: 2.2e+01\n",
      "Iter: 4000\n",
      "Loss: 2.2e+01\n",
      "Iter: 5000\n",
      "Loss: 2.2e+01\n",
      "Iter: 6000\n",
      "Loss: 2.2e+01\n",
      "Iter: 7000\n",
      "Loss: 2.2e+01\n",
      "Iter: 8000\n",
      "Loss: 2.3e+01\n",
      "Iter: 9000\n",
      "Loss: 2.1e+01\n",
      "Iter: 10000\n",
      "Loss: 2.2e+01\n",
      "Iter: 11000\n",
      "Loss: 2.1e+01\n",
      "Iter: 12000\n",
      "Loss: 2.2e+01\n",
      "Iter: 13000\n",
      "Loss: 2.1e+01\n",
      "Iter: 14000\n",
      "Loss: 2.1e+01\n",
      "Iter: 15000\n",
      "Loss: 2.1e+01\n",
      "Iter: 16000\n",
      "Loss: 2.1e+01\n",
      "Iter: 17000\n",
      "Loss: 2.1e+01\n",
      "Iter: 18000\n",
      "Loss: 2.1e+01\n",
      "Iter: 19000\n",
      "Loss: 2.2e+01\n",
      "Iter: 20000\n",
      "Loss: 2.1e+01\n",
      "Iter: 21000\n",
      "Loss: 2.2e+01\n",
      "Iter: 22000\n",
      "Loss: 2.2e+01\n",
      "Iter: 23000\n",
      "Loss: 2.1e+01\n",
      "Iter: 24000\n",
      "Loss: 2.2e+01\n",
      "Iter: 25000\n",
      "Loss: 2.2e+01\n",
      "Iter: 26000\n",
      "Loss: 2.1e+01\n",
      "Iter: 27000\n",
      "Loss: 2.2e+01\n",
      "Iter: 28000\n",
      "Loss: 2.2e+01\n",
      "Iter: 29000\n",
      "Loss: 2.1e+01\n",
      "Iter: 30000\n",
      "Loss: 2.1e+01\n",
      "Iter: 31000\n",
      "Loss: 2.2e+01\n",
      "Iter: 32000\n",
      "Loss: 2.1e+01\n",
      "Iter: 33000\n",
      "Loss: 2.2e+01\n",
      "Iter: 34000\n",
      "Loss: 2.2e+01\n",
      "Iter: 35000\n",
      "Loss: 2.3e+01\n",
      "Iter: 36000\n",
      "Loss: 2.2e+01\n",
      "Iter: 37000\n",
      "Loss: 2.2e+01\n",
      "Iter: 38000\n",
      "Loss: 2.2e+01\n",
      "Iter: 39000\n",
      "Loss: 2.1e+01\n",
      "Iter: 40000\n",
      "Loss: 2.1e+01\n",
      "Iter: 41000\n",
      "Loss: 2.2e+01\n",
      "Iter: 42000\n",
      "Loss: 2.2e+01\n",
      "Iter: 43000\n",
      "Loss: 2e+01\n",
      "Iter: 44000\n",
      "Loss: 2.1e+01\n",
      "Iter: 45000\n",
      "Loss: 2.1e+01\n",
      "Iter: 46000\n",
      "Loss: 2.1e+01\n",
      "Iter: 47000\n",
      "Loss: 2.1e+01\n",
      "Iter: 48000\n",
      "Loss: 2.1e+01\n",
      "Iter: 49000\n",
      "Loss: 2.1e+01\n",
      "Iter: 50000\n",
      "Loss: 2.1e+01\n",
      "Iter: 51000\n",
      "Loss: 2.1e+01\n",
      "Iter: 52000\n",
      "Loss: 2.2e+01\n",
      "Iter: 53000\n",
      "Loss: 2.2e+01\n",
      "Iter: 54000\n",
      "Loss: 2.1e+01\n",
      "Iter: 55000\n",
      "Loss: 2.2e+01\n",
      "Iter: 56000\n",
      "Loss: 2.2e+01\n",
      "Iter: 57000\n",
      "Loss: 2.1e+01\n",
      "Iter: 58000\n",
      "Loss: 2.2e+01\n",
      "Iter: 59000\n",
      "Loss: 2.2e+01\n",
      "Iter: 60000\n",
      "Loss: 2.1e+01\n",
      "Iter: 61000\n",
      "Loss: 2.1e+01\n",
      "Iter: 62000\n",
      "Loss: 2e+01\n",
      "Iter: 63000\n",
      "Loss: 2.1e+01\n",
      "Iter: 64000\n",
      "Loss: 2e+01\n",
      "Iter: 65000\n",
      "Loss: 2.2e+01\n",
      "Iter: 66000\n",
      "Loss: 2.2e+01\n",
      "Iter: 67000\n",
      "Loss: 2.1e+01\n",
      "Iter: 68000\n",
      "Loss: 2.1e+01\n",
      "Iter: 69000\n",
      "Loss: 2.2e+01\n",
      "Iter: 70000\n",
      "Loss: 2.1e+01\n",
      "Iter: 71000\n",
      "Loss: 2.2e+01\n",
      "Iter: 72000\n",
      "Loss: 2.1e+01\n",
      "Iter: 73000\n",
      "Loss: 2.2e+01\n",
      "Iter: 74000\n",
      "Loss: 2.1e+01\n",
      "Iter: 75000\n",
      "Loss: 2.1e+01\n",
      "Iter: 76000\n",
      "Loss: 2.1e+01\n",
      "Iter: 77000\n",
      "Loss: 2.1e+01\n",
      "Iter: 78000\n",
      "Loss: 2.1e+01\n",
      "Iter: 79000\n",
      "Loss: 2.1e+01\n",
      "Iter: 80000\n",
      "Loss: 2.1e+01\n",
      "Iter: 81000\n",
      "Loss: 2.1e+01\n",
      "Iter: 82000\n",
      "Loss: 2.2e+01\n",
      "Iter: 83000\n",
      "Loss: 2.2e+01\n",
      "Iter: 84000\n",
      "Loss: 2.1e+01\n",
      "Iter: 85000\n",
      "Loss: 2.1e+01\n",
      "Iter: 86000\n",
      "Loss: 2.1e+01\n",
      "Iter: 87000\n",
      "Loss: 2.1e+01\n",
      "Iter: 88000\n",
      "Loss: 2.1e+01\n",
      "Iter: 89000\n",
      "Loss: 2.1e+01\n",
      "Iter: 90000\n",
      "Loss: 2.2e+01\n",
      "Iter: 91000\n",
      "Loss: 2.1e+01\n",
      "Iter: 92000\n",
      "Loss: 2.1e+01\n",
      "Iter: 93000\n",
      "Loss: 2.1e+01\n",
      "Iter: 94000\n",
      "Loss: 2.1e+01\n",
      "Iter: 95000\n",
      "Loss: 2.1e+01\n",
      "Iter: 96000\n",
      "Loss: 2.1e+01\n",
      "Iter: 97000\n",
      "Loss: 2.1e+01\n",
      "Iter: 98000\n",
      "Loss: 2.2e+01\n",
      "Iter: 99000\n",
      "Loss: 2.1e+01\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1  as tf\n",
    "tf.disable_v2_behavior()\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "mb_size = 64\n",
    "z_dim = 10\n",
    "X_dim = 64\n",
    "y_dim = 10\n",
    "h_dim = 16\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    gs = gridspec.GridSpec(8, 8)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(8, 8), cmap='Greys_r')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random.normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "\n",
    "# Q(z|X) \n",
    "\n",
    "X = tf.keras.Input(shape=(X_dim,))\n",
    "c = tf.keras.Input(shape=(y_dim,))\n",
    "z = tf.keras.Input(shape=(z_dim,))\n",
    "\n",
    "Q_W1 = tf.Variable(xavier_init([X_dim + y_dim, h_dim]))\n",
    "Q_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "Q_W2_mu = tf.Variable(xavier_init([h_dim, z_dim]))\n",
    "Q_b2_mu = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "Q_W2_sigma = tf.Variable(xavier_init([h_dim, z_dim]))\n",
    "Q_b2_sigma = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "\n",
    "def Q(X, c):\n",
    "    inputs = tf.concat(axis=1, values=[X, c])\n",
    "    h = tf.nn.relu(tf.matmul(inputs, Q_W1) + Q_b1)\n",
    "    z_mu = tf.matmul(h, Q_W2_mu) + Q_b2_mu\n",
    "    z_logvar = tf.matmul(h, Q_W2_sigma) + Q_b2_sigma\n",
    "    return z_mu, z_logvar\n",
    "\n",
    "\n",
    "def sample_z(mu, log_var):\n",
    "    eps = tf.random.normal(shape=tf.shape(mu))\n",
    "    return mu + tf.exp(log_var / 2) * eps\n",
    "\n",
    "\n",
    "# P(X|z)\n",
    "\n",
    "P_W1 = tf.Variable(xavier_init([z_dim + y_dim, h_dim]))\n",
    "P_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "P_W2 = tf.Variable(xavier_init([h_dim, X_dim]))\n",
    "P_b2 = tf.Variable(tf.zeros(shape=[X_dim]))\n",
    "\n",
    "\n",
    "def P(z, c):\n",
    "    inputs = tf.concat(axis=1, values=[z, c])\n",
    "    h = tf.nn.relu(tf.matmul(inputs, P_W1) + P_b1)\n",
    "    logits = tf.matmul(h, P_W2) + P_b2\n",
    "    prob = tf.nn.sigmoid(logits)\n",
    "    return prob, logits\n",
    "\n",
    "z_mu, z_logvar = Q(X, c)\n",
    "z_sample = sample_z(z_mu, z_logvar)\n",
    "_, logits = P(z_sample, c)\n",
    "\n",
    "# Sampling from random z\n",
    "X_samples, _ = P(z, c)\n",
    "\n",
    "# E[log P(X|z)]\n",
    "recon_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=X), 1)\n",
    "kl_loss = 0.5 * tf.reduce_sum(tf.exp(z_logvar) + z_mu**2 - 1. - z_logvar, 1)\n",
    "\n",
    "# VAE loss\n",
    "vae_loss = tf.reduce_mean(recon_loss + kl_loss)\n",
    "\n",
    "# gradient step\n",
    "solver = tf.compat.v1.train.AdamOptimizer().minimize(vae_loss)\n",
    "sess = tf.compat.v1.Session ()\n",
    "sess.run(\n",
    "tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "# if not os.path.exists('samples_opt_100/'):\n",
    "#     os.makedirs('samples_opt_100/')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for it in range(100000):\n",
    "    ind = np.random.choice(train_images.shape[0], mb_size)\n",
    "    X_mb = np.array(train_images[ind])\n",
    "    y_mb = np.array(train_y[ind])\n",
    "    \n",
    "    _, loss = sess.run([solver, vae_loss], feed_dict={X: X_mb, c: y_mb})\n",
    "\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Loss: {:.2}'. format(loss))\n",
    "\n",
    "#         y = np.zeros(shape=[64, y_dim])\n",
    "#         y[:, np.random.randint(0, y_dim)] = 1.\n",
    "\n",
    "#         samples = sess.run(X_samples,\n",
    "#                            feed_dict={z: np.random.randn(64, z_dim), c: y})\n",
    "\n",
    "#         fig = plot(samples)\n",
    "#         plt.savefig('samples_opt_100/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "#         i += 1\n",
    "#         plt.close(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "424bbbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### generating sample outputs after training\n",
    "def generate_samples():\n",
    "    samples = []\n",
    "    gen_labels =[]\n",
    "    for r in range(10):\n",
    "        for index in range(2):\n",
    "            gen_labels = gen_labels + [index]*64\n",
    "            y = np.zeros([mb_size, y_dim])\n",
    "            y[range(mb_size), index] = 1\n",
    "            samples.extend(sess.run(X_samples,\n",
    "                                   feed_dict={z: np.random.randn(64, z_dim), c: y}))\n",
    "\n",
    "    gen_samples = np.array(samples)\n",
    "    gen_labels = np.array(gen_labels)\n",
    "    \n",
    "    return gen_samples, gen_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76387c9",
   "metadata": {},
   "source": [
    "# Visualize generated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e3c7c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1280, 64), (1280,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_samples, aug_labels = generate_samples()\n",
    "aug_samples.shape, aug_labels.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5fa24bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f92fd69b1f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(aug_labels[70])\n",
    "plt.imshow(np.reshape(aug_samples[70],(8,8)), cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35caa411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normiziling pixel between 0..16 similar to the original data.\n",
    "#norm_xtrain = (16*(gen_samples - np.min(gen_samples))/np.ptp(gen_samples)).astype(int) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a14657c",
   "metadata": {},
   "source": [
    "# MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "910f934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Input, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "def build_model(input_shape=(64,), num_classes=10):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_shape: shape of input_data\n",
    "    :param num_classes: number of classes\n",
    "    :return: keras.model.sequential compiled with categorical cross-entropy loss\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e59ca02",
   "metadata": {},
   "source": [
    "# Baseline Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6ffe7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_results(b_list, filename='default.csv'):\n",
    "    total_df = pd.DataFrame(b_list[0]).transpose()\n",
    "    print('number of runs: {}'.format(len(b_list)))\n",
    "    for r_dict in b_list[1:]:\n",
    "        temp = pd.DataFrame(r_dict).transpose()\n",
    "        total_df = total_df.add(temp)\n",
    "        \n",
    "    average_pd = total_df/10.0\n",
    "    average_pd.to_csv(filename, sep=',')\n",
    "    \n",
    "    return average_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6874de79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/layers/normalization/batch_normalization.py:520: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 3258 samples, validate on 1797 samples\n",
      "Epoch 1/2\n",
      "3258/3258 [==============================] - 1s 254us/sample - loss: 1.5467 - acc: 0.5292 - val_loss: 0.6493 - val_acc: 0.8063\n",
      "Epoch 2/2\n",
      "   8/3258 [..............................] - ETA: 0s - loss: 1.2202 - acc: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3258/3258 [==============================] - 1s 204us/sample - loss: 0.7028 - acc: 0.7772 - val_loss: 0.3740 - val_acc: 0.8837\n",
      "test loss:  0.3740360666611623\n",
      "test accuracy:  0.88369507\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 3258 samples, validate on 1797 samples\n",
      "Epoch 1/2\n",
      "3258/3258 [==============================] - 1s 255us/sample - loss: 1.6458 - acc: 0.4890 - val_loss: 0.7177 - val_acc: 0.7963\n",
      "Epoch 2/2\n",
      "   8/3258 [..............................] - ETA: 0s - loss: 1.1238 - acc: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3258/3258 [==============================] - 1s 210us/sample - loss: 0.7659 - acc: 0.7621 - val_loss: 0.4538 - val_acc: 0.8664\n",
      "test loss:  0.45377484361994047\n",
      "test accuracy:  0.86644405\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 3258 samples, validate on 1797 samples\n",
      "Epoch 1/2\n",
      "3072/3258 [===========================>..] - ETA: 0s - loss: 1.5080 - acc: 0.5293"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3258/3258 [==============================] - 1s 269us/sample - loss: 1.4713 - acc: 0.5378 - val_loss: 0.5519 - val_acc: 0.8442\n",
      "Epoch 2/2\n",
      "3258/3258 [==============================] - 1s 210us/sample - loss: 0.6774 - acc: 0.7845 - val_loss: 0.2961 - val_acc: 0.9182\n",
      "test loss:  0.29609036029347063\n",
      "test accuracy:  0.918197\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 3258 samples, validate on 1797 samples\n",
      "Epoch 1/2\n",
      "3064/3258 [===========================>..] - ETA: 0s - loss: 1.4644 - acc: 0.5441"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3258/3258 [==============================] - 1s 280us/sample - loss: 1.4230 - acc: 0.5562 - val_loss: 0.5685 - val_acc: 0.8347\n",
      "Epoch 2/2\n",
      "3258/3258 [==============================] - 1s 212us/sample - loss: 0.6770 - acc: 0.7897 - val_loss: 0.3445 - val_acc: 0.8959\n",
      "test loss:  0.3445104142701752\n",
      "test accuracy:  0.8959377\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 3258 samples, validate on 1797 samples\n",
      "Epoch 1/2\n",
      "2984/3258 [==========================>...] - ETA: 0s - loss: 1.4258 - acc: 0.5459"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3258/3258 [==============================] - 1s 293us/sample - loss: 1.3728 - acc: 0.5638 - val_loss: 0.5787 - val_acc: 0.8225\n",
      "Epoch 2/2\n",
      "3258/3258 [==============================] - 1s 219us/sample - loss: 0.6589 - acc: 0.7977 - val_loss: 0.3943 - val_acc: 0.8776\n",
      "test loss:  0.3942760984870954\n",
      "test accuracy:  0.8775737\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 3258 samples, validate on 1797 samples\n",
      "Epoch 1/2\n",
      "3176/3258 [============================>.] - ETA: 0s - loss: 1.5051 - acc: 0.5277"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3258/3258 [==============================] - 1s 312us/sample - loss: 1.4953 - acc: 0.5310 - val_loss: 0.5546 - val_acc: 0.8280\n",
      "Epoch 2/2\n",
      "3258/3258 [==============================] - 1s 225us/sample - loss: 0.6910 - acc: 0.7784 - val_loss: 0.3478 - val_acc: 0.8787\n",
      "test loss:  0.34775268860704445\n",
      "test accuracy:  0.8786867\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 3258 samples, validate on 1797 samples\n",
      "Epoch 1/2\n",
      "3104/3258 [===========================>..] - ETA: 0s - loss: 1.5651 - acc: 0.5193"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3258/3258 [==============================] - 1s 330us/sample - loss: 1.5267 - acc: 0.5310 - val_loss: 0.5630 - val_acc: 0.8408\n",
      "Epoch 2/2\n",
      "3258/3258 [==============================] - 1s 228us/sample - loss: 0.6964 - acc: 0.7784 - val_loss: 0.3136 - val_acc: 0.8943\n",
      "test loss:  0.3136024434569317\n",
      "test accuracy:  0.8942682\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 3258 samples, validate on 1797 samples\n",
      "Epoch 1/2\n",
      "3008/3258 [==========================>...] - ETA: 0s - loss: 1.5149 - acc: 0.5259"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3258/3258 [==============================] - 1s 341us/sample - loss: 1.4645 - acc: 0.5390 - val_loss: 0.6222 - val_acc: 0.8230\n",
      "Epoch 2/2\n",
      "3258/3258 [==============================] - 1s 241us/sample - loss: 0.7157 - acc: 0.7787 - val_loss: 0.3458 - val_acc: 0.8943\n",
      "test loss:  0.3458232584849423\n",
      "test accuracy:  0.8942682\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 3258 samples, validate on 1797 samples\n",
      "Epoch 1/2\n",
      "3216/3258 [============================>.] - ETA: 0s - loss: 1.4591 - acc: 0.5498"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3258/3258 [==============================] - 1s 353us/sample - loss: 1.4495 - acc: 0.5528 - val_loss: 0.5696 - val_acc: 0.8253\n",
      "Epoch 2/2\n",
      "3258/3258 [==============================] - 1s 242us/sample - loss: 0.7119 - acc: 0.7802 - val_loss: 0.4066 - val_acc: 0.8776\n",
      "test loss:  0.40660757647747453\n",
      "test accuracy:  0.8775737\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_27 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 3258 samples, validate on 1797 samples\n",
      "Epoch 1/2\n",
      "3160/3258 [============================>.] - ETA: 0s - loss: 1.5889 - acc: 0.5089"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3258/3258 [==============================] - 1s 366us/sample - loss: 1.5706 - acc: 0.5141 - val_loss: 0.7502 - val_acc: 0.7947\n",
      "Epoch 2/2\n",
      "3258/3258 [==============================] - 1s 246us/sample - loss: 0.7459 - acc: 0.7670 - val_loss: 0.4900 - val_acc: 0.8525\n",
      "test loss:  0.4900228850580814\n",
      "test accuracy:  0.85253197\n"
     ]
    }
   ],
   "source": [
    "baseline_list =[]\n",
    "for i in range(10):\n",
    "    model = build_model()\n",
    "    batch_size=8\n",
    "    epochs=2\n",
    "    history = model.fit(train_images, train_y, batch_size=batch_size, epochs=epochs, \n",
    "                        validation_data=(test_images, test_y))\n",
    "    score = model.evaluate(test_images, test_y, verbose=0)\n",
    "    print('test loss: ',score[0])\n",
    "    print('test accuracy: ', score[1] )\n",
    "    y_pred_oh = model.predict(test_images)\n",
    "    y_pred_baseline = y_pred_oh.argmax(axis=-1)\n",
    "    from sklearn.metrics import classification_report\n",
    "    baseline_list.append(classification_report(y_test, y_pred_baseline, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41707b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of runs: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.981067</td>\n",
       "      <td>0.931461</td>\n",
       "      <td>0.954938</td>\n",
       "      <td>178.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.913326</td>\n",
       "      <td>0.506044</td>\n",
       "      <td>0.641678</td>\n",
       "      <td>182.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.909180</td>\n",
       "      <td>0.942938</td>\n",
       "      <td>0.924867</td>\n",
       "      <td>177.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.907387</td>\n",
       "      <td>0.908743</td>\n",
       "      <td>0.907034</td>\n",
       "      <td>183.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.904440</td>\n",
       "      <td>0.970718</td>\n",
       "      <td>0.935437</td>\n",
       "      <td>181.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.901739</td>\n",
       "      <td>0.968681</td>\n",
       "      <td>0.933619</td>\n",
       "      <td>182.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.927349</td>\n",
       "      <td>0.966298</td>\n",
       "      <td>0.945800</td>\n",
       "      <td>181.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.942483</td>\n",
       "      <td>0.888268</td>\n",
       "      <td>0.913820</td>\n",
       "      <td>179.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.707861</td>\n",
       "      <td>0.868966</td>\n",
       "      <td>0.777202</td>\n",
       "      <td>174.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.827902</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.855142</td>\n",
       "      <td>180.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.883918</td>\n",
       "      <td>0.883918</td>\n",
       "      <td>0.883918</td>\n",
       "      <td>0.883918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.892273</td>\n",
       "      <td>0.884212</td>\n",
       "      <td>0.878954</td>\n",
       "      <td>1797.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.892820</td>\n",
       "      <td>0.883918</td>\n",
       "      <td>0.879025</td>\n",
       "      <td>1797.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.981067  0.931461  0.954938   178.000000\n",
       "1              0.913326  0.506044  0.641678   182.000000\n",
       "2              0.909180  0.942938  0.924867   177.000000\n",
       "3              0.907387  0.908743  0.907034   183.000000\n",
       "4              0.904440  0.970718  0.935437   181.000000\n",
       "5              0.901739  0.968681  0.933619   182.000000\n",
       "6              0.927349  0.966298  0.945800   181.000000\n",
       "7              0.942483  0.888268  0.913820   179.000000\n",
       "8              0.707861  0.868966  0.777202   174.000000\n",
       "9              0.827902  0.890000  0.855142   180.000000\n",
       "accuracy       0.883918  0.883918  0.883918     0.883918\n",
       "macro avg      0.892273  0.884212  0.878954  1797.000000\n",
       "weighted avg   0.892820  0.883918  0.879025  1797.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_process_results(baseline_list, 'results_csv/opt_baseline_cnn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06f3e94",
   "metadata": {},
   "source": [
    "# Augmentation Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c58216af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 4538 samples, validate on 1797 samples\n",
      "Epoch 1/2\n",
      "4536/4538 [============================>.] - ETA: 0s - loss: 1.0119 - acc: 0.6903"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4538/4538 [==============================] - 2s 337us/sample - loss: 1.0117 - acc: 0.6904 - val_loss: 0.3468 - val_acc: 0.8943\n",
      "Epoch 2/2\n",
      "4538/4538 [==============================] - 1s 245us/sample - loss: 0.4318 - acc: 0.8658 - val_loss: 0.3057 - val_acc: 0.9104\n",
      "test loss for 0th run:  0.30570339290534015\n",
      "test accuracy for 0th run:  0.91040623\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_33 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 4538 samples, validate on 1797 samples\n",
      "Epoch 1/2\n",
      "4464/4538 [============================>.] - ETA: 0s - loss: 1.1833 - acc: 0.6243"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4538/4538 [==============================] - 2s 342us/sample - loss: 1.1799 - acc: 0.6254 - val_loss: 0.4131 - val_acc: 0.8759\n",
      "Epoch 2/2\n",
      "4538/4538 [==============================] - 1s 246us/sample - loss: 0.4980 - acc: 0.8438 - val_loss: 0.3409 - val_acc: 0.8915\n",
      "test loss for 1th run:  0.340894561425747\n",
      "test accuracy for 1th run:  0.8914858\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 4538 samples, validate on 1797 samples\n",
      "Epoch 1/2\n",
      "4384/4538 [===========================>..] - ETA: 0s - loss: 1.1576 - acc: 0.6428"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4538/4538 [==============================] - 2s 353us/sample - loss: 1.1388 - acc: 0.6492 - val_loss: 0.3327 - val_acc: 0.8937\n",
      "Epoch 2/2\n",
      "4538/4538 [==============================] - 1s 250us/sample - loss: 0.4798 - acc: 0.8521 - val_loss: 0.2868 - val_acc: 0.9132\n",
      "test loss for 2th run:  0.286790833828694\n",
      "test accuracy for 2th run:  0.91318864\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_39 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 4538 samples, validate on 1797 samples\n",
      "Epoch 1/2\n",
      "4512/4538 [============================>.] - ETA: 0s - loss: 1.1307 - acc: 0.6543"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4538/4538 [==============================] - 2s 363us/sample - loss: 1.1272 - acc: 0.6551 - val_loss: 0.3629 - val_acc: 0.8826\n",
      "Epoch 2/2\n",
      "4538/4538 [==============================] - 1s 256us/sample - loss: 0.4721 - acc: 0.8607 - val_loss: 0.2539 - val_acc: 0.9165\n",
      "test loss for 3th run:  0.253944815818011\n",
      "test accuracy for 3th run:  0.91652757\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_42 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 4538 samples, validate on 1797 samples\n",
      "Epoch 1/2\n",
      "4480/4538 [============================>.] - ETA: 0s - loss: 1.2027 - acc: 0.6241"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4538/4538 [==============================] - 2s 370us/sample - loss: 1.1953 - acc: 0.6267 - val_loss: 0.4316 - val_acc: 0.8770\n",
      "Epoch 2/2\n",
      "4538/4538 [==============================] - 1s 260us/sample - loss: 0.4840 - acc: 0.8499 - val_loss: 0.3096 - val_acc: 0.9021\n",
      "test loss for 4th run:  0.30955893476688007\n",
      "test accuracy for 4th run:  0.90205896\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_45 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 4538 samples, validate on 1797 samples\n",
      "Epoch 1/2\n",
      "4328/4538 [===========================>..] - ETA: 0s - loss: 1.1735 - acc: 0.6467"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4538/4538 [==============================] - 2s 387us/sample - loss: 1.1457 - acc: 0.6554 - val_loss: 0.4073 - val_acc: 0.8792\n",
      "Epoch 2/2\n",
      "4538/4538 [==============================] - 1s 266us/sample - loss: 0.4538 - acc: 0.8587 - val_loss: 0.2895 - val_acc: 0.9093\n",
      "test loss for 5th run:  0.28949252350704663\n",
      "test accuracy for 5th run:  0.9092933\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_48 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 4538 samples, validate on 1797 samples\n",
      "Epoch 1/2\n",
      "4528/4538 [============================>.] - ETA: 0s - loss: 1.0770 - acc: 0.6731"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4538/4538 [==============================] - 2s 401us/sample - loss: 1.0757 - acc: 0.6736 - val_loss: 0.4313 - val_acc: 0.8737\n",
      "Epoch 2/2\n",
      "4538/4538 [==============================] - 1s 275us/sample - loss: 0.4396 - acc: 0.8709 - val_loss: 0.3100 - val_acc: 0.9004\n",
      "test loss for 6th run:  0.3099774638508044\n",
      "test accuracy for 6th run:  0.90038955\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_51 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 4538 samples, validate on 1797 samples\n",
      "Epoch 1/2\n",
      "4392/4538 [============================>.] - ETA: 0s - loss: 1.1324 - acc: 0.6473"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4538/4538 [==============================] - 2s 427us/sample - loss: 1.1157 - acc: 0.6523 - val_loss: 0.4113 - val_acc: 0.8625\n",
      "Epoch 2/2\n",
      "4538/4538 [==============================] - 2s 335us/sample - loss: 0.4845 - acc: 0.8519 - val_loss: 0.2975 - val_acc: 0.9054\n",
      "test loss for 7th run:  0.2974753447258446\n",
      "test accuracy for 7th run:  0.9053979\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_54 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 4538 samples, validate on 1797 samples\n",
      "Epoch 1/2\n",
      "4440/4538 [============================>.] - ETA: 0s - loss: 1.1136 - acc: 0.6514"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4538/4538 [==============================] - 2s 525us/sample - loss: 1.1032 - acc: 0.6547 - val_loss: 0.3951 - val_acc: 0.8831\n",
      "Epoch 2/2\n",
      "4538/4538 [==============================] - 1s 296us/sample - loss: 0.4970 - acc: 0.8471 - val_loss: 0.2499 - val_acc: 0.9193\n",
      "test loss for 8th run:  0.24990354760969238\n",
      "test accuracy for 8th run:  0.91931\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_57 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 4538 samples, validate on 1797 samples\n",
      "Epoch 1/2\n",
      "4432/4538 [============================>.] - ETA: 0s - loss: 1.1128 - acc: 0.6591"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4538/4538 [==============================] - 2s 440us/sample - loss: 1.0970 - acc: 0.6644 - val_loss: 0.4161 - val_acc: 0.8715\n",
      "Epoch 2/2\n",
      "4538/4538 [==============================] - 1s 292us/sample - loss: 0.4699 - acc: 0.8550 - val_loss: 0.3184 - val_acc: 0.8965\n",
      "test loss for 9th run:  0.3184420837563114\n",
      "test accuracy for 9th run:  0.89649415\n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "for i in range(10):\n",
    "    gen_samples, gen_labels = generate_samples()\n",
    "    gen_y = tf.keras.utils.to_categorical(gen_labels, num_classes=10, dtype='float32')\n",
    "    X = np.concatenate([train_images, gen_samples])\n",
    "    Y = np.concatenate([train_y, gen_y])\n",
    "    model_aug = build_model()\n",
    "    history_aug = model_aug.fit(X, Y, batch_size=batch_size, epochs=epochs, validation_data=(test_images, test_y))\n",
    "    \n",
    "    aug_score = model_aug.evaluate(test_images, test_y, verbose=0)\n",
    "    print('test loss for {}th run: '.format(i), aug_score[0])\n",
    "    print('test accuracy for {}th run: '.format(i), aug_score[1] )\n",
    "    \n",
    "    y_pred_aug_oh = model_aug.predict(test_images)\n",
    "    y_pred_aug = y_pred_aug_oh.argmax(axis=-1)\n",
    "    results_list.append(classification_report(y_test, y_pred_aug, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37048cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of runs: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.985277</td>\n",
       "      <td>0.964045</td>\n",
       "      <td>0.974441</td>\n",
       "      <td>178.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.918026</td>\n",
       "      <td>0.729121</td>\n",
       "      <td>0.810623</td>\n",
       "      <td>182.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.919842</td>\n",
       "      <td>0.952542</td>\n",
       "      <td>0.934878</td>\n",
       "      <td>177.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.955297</td>\n",
       "      <td>0.899454</td>\n",
       "      <td>0.926116</td>\n",
       "      <td>183.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.906845</td>\n",
       "      <td>0.967956</td>\n",
       "      <td>0.935996</td>\n",
       "      <td>181.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.882991</td>\n",
       "      <td>0.969780</td>\n",
       "      <td>0.923931</td>\n",
       "      <td>182.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.972228</td>\n",
       "      <td>0.967403</td>\n",
       "      <td>0.969522</td>\n",
       "      <td>181.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.956447</td>\n",
       "      <td>0.889944</td>\n",
       "      <td>0.921265</td>\n",
       "      <td>179.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.753923</td>\n",
       "      <td>0.844253</td>\n",
       "      <td>0.794786</td>\n",
       "      <td>174.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.857290</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.866328</td>\n",
       "      <td>180.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.906455</td>\n",
       "      <td>0.906455</td>\n",
       "      <td>0.906455</td>\n",
       "      <td>0.906455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.910817</td>\n",
       "      <td>0.906450</td>\n",
       "      <td>0.905789</td>\n",
       "      <td>1797.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.911300</td>\n",
       "      <td>0.906455</td>\n",
       "      <td>0.906026</td>\n",
       "      <td>1797.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.985277  0.964045  0.974441   178.000000\n",
       "1              0.918026  0.729121  0.810623   182.000000\n",
       "2              0.919842  0.952542  0.934878   177.000000\n",
       "3              0.955297  0.899454  0.926116   183.000000\n",
       "4              0.906845  0.967956  0.935996   181.000000\n",
       "5              0.882991  0.969780  0.923931   182.000000\n",
       "6              0.972228  0.967403  0.969522   181.000000\n",
       "7              0.956447  0.889944  0.921265   179.000000\n",
       "8              0.753923  0.844253  0.794786   174.000000\n",
       "9              0.857290  0.880000  0.866328   180.000000\n",
       "accuracy       0.906455  0.906455  0.906455     0.906455\n",
       "macro avg      0.910817  0.906450  0.905789  1797.000000\n",
       "weighted avg   0.911300  0.906455  0.906026  1797.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_process_results(results_list, 'results_csv/opt_VAE.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8669158a",
   "metadata": {},
   "source": [
    "# Random UnderSmapling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05e99e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_60 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 1000 samples, validate on 1797 samples\n",
      "Epoch 1/30\n",
      "  64/1000 [>.............................] - ETA: 3s - loss: 3.0542 - acc: 0.1406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 799us/sample - loss: 2.7017 - acc: 0.2200 - val_loss: 2.1713 - val_acc: 0.2960\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 1.9094 - acc: 0.3920 - val_loss: 2.0489 - val_acc: 0.4652\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s 77us/sample - loss: 1.3198 - acc: 0.5660 - val_loss: 1.9189 - val_acc: 0.5965\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 1.0705 - acc: 0.6570 - val_loss: 1.7817 - val_acc: 0.6706\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.8589 - acc: 0.7310 - val_loss: 1.6401 - val_acc: 0.7223\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.7472 - acc: 0.7780 - val_loss: 1.4963 - val_acc: 0.7596\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.6430 - acc: 0.8120 - val_loss: 1.3306 - val_acc: 0.8036\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.5436 - acc: 0.8560 - val_loss: 1.1777 - val_acc: 0.8331\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.4669 - acc: 0.8710 - val_loss: 1.0306 - val_acc: 0.8598\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.4394 - acc: 0.8750 - val_loss: 0.9033 - val_acc: 0.8692\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.3996 - acc: 0.8800 - val_loss: 0.7830 - val_acc: 0.8865\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.3313 - acc: 0.9040 - val_loss: 0.6672 - val_acc: 0.8971\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.3576 - acc: 0.9040 - val_loss: 0.5741 - val_acc: 0.9032\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.3206 - acc: 0.9040 - val_loss: 0.4996 - val_acc: 0.9115\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.3184 - acc: 0.9080 - val_loss: 0.4401 - val_acc: 0.9176\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.2503 - acc: 0.9310 - val_loss: 0.3850 - val_acc: 0.9260\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.2593 - acc: 0.9180 - val_loss: 0.3478 - val_acc: 0.9265\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.2528 - acc: 0.9290 - val_loss: 0.3146 - val_acc: 0.9265\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.2371 - acc: 0.9350 - val_loss: 0.2866 - val_acc: 0.9304\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.2073 - acc: 0.9470 - val_loss: 0.2687 - val_acc: 0.9327\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.2099 - acc: 0.9410 - val_loss: 0.2497 - val_acc: 0.9332\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.2065 - acc: 0.9460 - val_loss: 0.2374 - val_acc: 0.9321\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.1950 - acc: 0.9460 - val_loss: 0.2254 - val_acc: 0.9332\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.1819 - acc: 0.9510 - val_loss: 0.2204 - val_acc: 0.9371\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.1838 - acc: 0.9510 - val_loss: 0.2192 - val_acc: 0.9349\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.1769 - acc: 0.9440 - val_loss: 0.2090 - val_acc: 0.9405\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.1591 - acc: 0.9540 - val_loss: 0.2005 - val_acc: 0.9377\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 0s 77us/sample - loss: 0.1823 - acc: 0.9530 - val_loss: 0.1975 - val_acc: 0.9399\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 0s 71us/sample - loss: 0.1401 - acc: 0.9670 - val_loss: 0.2003 - val_acc: 0.9382\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 0s 77us/sample - loss: 0.1527 - acc: 0.9550 - val_loss: 0.2026 - val_acc: 0.9360\n",
      "undersampling test loss:  0.20258634860441296\n",
      "undersampling accuracy:  0.93600446\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_63 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 1000 samples, validate on 1797 samples\n",
      "Epoch 1/30\n",
      "  64/1000 [>.............................] - ETA: 3s - loss: 3.6526 - acc: 0.1094"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 845us/sample - loss: 3.0409 - acc: 0.1590 - val_loss: 2.2101 - val_acc: 0.2337\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s 77us/sample - loss: 1.9834 - acc: 0.3510 - val_loss: 2.0776 - val_acc: 0.3996\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s 78us/sample - loss: 1.4484 - acc: 0.5260 - val_loss: 1.9378 - val_acc: 0.5687\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s 76us/sample - loss: 1.1374 - acc: 0.6320 - val_loss: 1.7964 - val_acc: 0.6505\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s 78us/sample - loss: 0.8899 - acc: 0.6980 - val_loss: 1.6433 - val_acc: 0.7229\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.7846 - acc: 0.7640 - val_loss: 1.4869 - val_acc: 0.7629\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.6305 - acc: 0.8130 - val_loss: 1.3269 - val_acc: 0.8119\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s 73us/sample - loss: 0.5512 - acc: 0.8420 - val_loss: 1.1746 - val_acc: 0.8420\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.4717 - acc: 0.8650 - val_loss: 1.0444 - val_acc: 0.8509\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.4400 - acc: 0.8750 - val_loss: 0.9141 - val_acc: 0.8620\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.4070 - acc: 0.8900 - val_loss: 0.7930 - val_acc: 0.8781\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s 77us/sample - loss: 0.3734 - acc: 0.8810 - val_loss: 0.6921 - val_acc: 0.8898\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.3367 - acc: 0.9120 - val_loss: 0.6053 - val_acc: 0.8998\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 0s 78us/sample - loss: 0.3215 - acc: 0.9060 - val_loss: 0.5286 - val_acc: 0.9087\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 0s 73us/sample - loss: 0.3106 - acc: 0.9100 - val_loss: 0.4652 - val_acc: 0.9160\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 0s 76us/sample - loss: 0.2888 - acc: 0.9210 - val_loss: 0.4164 - val_acc: 0.9160\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 0s 77us/sample - loss: 0.3023 - acc: 0.9170 - val_loss: 0.3803 - val_acc: 0.9188\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.2638 - acc: 0.9290 - val_loss: 0.3440 - val_acc: 0.9232\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.2446 - acc: 0.9310 - val_loss: 0.3175 - val_acc: 0.9288\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.2181 - acc: 0.9440 - val_loss: 0.2973 - val_acc: 0.9304\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 0s 74us/sample - loss: 0.2103 - acc: 0.9410 - val_loss: 0.2795 - val_acc: 0.9327\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 0s 76us/sample - loss: 0.2187 - acc: 0.9410 - val_loss: 0.2639 - val_acc: 0.9354\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 0s 76us/sample - loss: 0.1941 - acc: 0.9520 - val_loss: 0.2518 - val_acc: 0.9354\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.1786 - acc: 0.9550 - val_loss: 0.2418 - val_acc: 0.9343\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 0s 75us/sample - loss: 0.1594 - acc: 0.9590 - val_loss: 0.2354 - val_acc: 0.9343\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 0s 76us/sample - loss: 0.1793 - acc: 0.9500 - val_loss: 0.2296 - val_acc: 0.9371\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 0s 78us/sample - loss: 0.1668 - acc: 0.9490 - val_loss: 0.2185 - val_acc: 0.9388\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 0s 78us/sample - loss: 0.1819 - acc: 0.9520 - val_loss: 0.2099 - val_acc: 0.9416\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 0s 78us/sample - loss: 0.1461 - acc: 0.9590 - val_loss: 0.2064 - val_acc: 0.9421\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 0s 74us/sample - loss: 0.1409 - acc: 0.9650 - val_loss: 0.2082 - val_acc: 0.9410\n",
      "undersampling test loss:  0.2081905354263181\n",
      "undersampling accuracy:  0.9410128\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_66 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 1000 samples, validate on 1797 samples\n",
      "Epoch 1/30\n",
      "  64/1000 [>.............................] - ETA: 3s - loss: 3.8789 - acc: 0.0781"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 830us/sample - loss: 3.2061 - acc: 0.1660 - val_loss: 2.2448 - val_acc: 0.1514\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 2.1477 - acc: 0.3300 - val_loss: 2.1369 - val_acc: 0.2170\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 1.5912 - acc: 0.4770 - val_loss: 2.0206 - val_acc: 0.3645\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 1.3024 - acc: 0.5760 - val_loss: 1.8885 - val_acc: 0.5264\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 1.0232 - acc: 0.6620 - val_loss: 1.7412 - val_acc: 0.6272\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.9164 - acc: 0.7210 - val_loss: 1.5945 - val_acc: 0.6672\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.7510 - acc: 0.7750 - val_loss: 1.4456 - val_acc: 0.7112\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s 78us/sample - loss: 0.6689 - acc: 0.7880 - val_loss: 1.2900 - val_acc: 0.7585\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.5928 - acc: 0.8150 - val_loss: 1.1401 - val_acc: 0.8013\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s 75us/sample - loss: 0.5197 - acc: 0.8470 - val_loss: 1.0024 - val_acc: 0.8297\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.4783 - acc: 0.8480 - val_loss: 0.8671 - val_acc: 0.8492\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.4262 - acc: 0.8810 - val_loss: 0.7508 - val_acc: 0.8631\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 0s 78us/sample - loss: 0.4281 - acc: 0.8770 - val_loss: 0.6604 - val_acc: 0.8804\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 0s 77us/sample - loss: 0.3822 - acc: 0.8890 - val_loss: 0.5806 - val_acc: 0.8893\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.3477 - acc: 0.9020 - val_loss: 0.5089 - val_acc: 0.8954\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.2964 - acc: 0.9220 - val_loss: 0.4510 - val_acc: 0.9004\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.3090 - acc: 0.9060 - val_loss: 0.4063 - val_acc: 0.9037\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.3170 - acc: 0.9100 - val_loss: 0.3691 - val_acc: 0.9082\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.2864 - acc: 0.9290 - val_loss: 0.3413 - val_acc: 0.9087\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.2506 - acc: 0.9290 - val_loss: 0.3134 - val_acc: 0.9137\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.2441 - acc: 0.9280 - val_loss: 0.2910 - val_acc: 0.9182\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.2156 - acc: 0.9450 - val_loss: 0.2735 - val_acc: 0.9221\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.2213 - acc: 0.9350 - val_loss: 0.2598 - val_acc: 0.9226\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.2265 - acc: 0.9340 - val_loss: 0.2530 - val_acc: 0.9210\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.1915 - acc: 0.9550 - val_loss: 0.2470 - val_acc: 0.9215\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 0s 74us/sample - loss: 0.1956 - acc: 0.9520 - val_loss: 0.2415 - val_acc: 0.9221\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.1986 - acc: 0.9410 - val_loss: 0.2324 - val_acc: 0.9249\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.1761 - acc: 0.9480 - val_loss: 0.2230 - val_acc: 0.9254\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.1570 - acc: 0.9640 - val_loss: 0.2223 - val_acc: 0.9282\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.1666 - acc: 0.9540 - val_loss: 0.2278 - val_acc: 0.9265\n",
      "undersampling test loss:  0.22781631020900603\n",
      "undersampling accuracy:  0.92654425\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_69 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 1000 samples, validate on 1797 samples\n",
      "Epoch 1/30\n",
      "  64/1000 [>.............................] - ETA: 3s - loss: 2.7246 - acc: 0.3125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 864us/sample - loss: 2.5516 - acc: 0.2630 - val_loss: 2.1888 - val_acc: 0.1848\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 1.7126 - acc: 0.4560 - val_loss: 2.0615 - val_acc: 0.4190\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 1.3299 - acc: 0.5530 - val_loss: 1.9303 - val_acc: 0.5637\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 1.0183 - acc: 0.6880 - val_loss: 1.7872 - val_acc: 0.6728\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.8782 - acc: 0.7200 - val_loss: 1.6449 - val_acc: 0.7134\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.6861 - acc: 0.7850 - val_loss: 1.4904 - val_acc: 0.7568\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.6249 - acc: 0.8020 - val_loss: 1.3501 - val_acc: 0.7702\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.5210 - acc: 0.8470 - val_loss: 1.2040 - val_acc: 0.7963\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.4981 - acc: 0.8550 - val_loss: 1.0677 - val_acc: 0.8147\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s 76us/sample - loss: 0.4502 - acc: 0.8600 - val_loss: 0.9286 - val_acc: 0.8325\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.3960 - acc: 0.8770 - val_loss: 0.8108 - val_acc: 0.8575\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.3591 - acc: 0.9040 - val_loss: 0.7039 - val_acc: 0.8692\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.3323 - acc: 0.9050 - val_loss: 0.6171 - val_acc: 0.8737\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.3130 - acc: 0.9100 - val_loss: 0.5447 - val_acc: 0.8770\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.2921 - acc: 0.9240 - val_loss: 0.4768 - val_acc: 0.8893\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.2764 - acc: 0.9210 - val_loss: 0.4181 - val_acc: 0.9015\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.2412 - acc: 0.9280 - val_loss: 0.3760 - val_acc: 0.9071\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.2279 - acc: 0.9390 - val_loss: 0.3489 - val_acc: 0.9121\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.2270 - acc: 0.9390 - val_loss: 0.3272 - val_acc: 0.9132\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.2037 - acc: 0.9430 - val_loss: 0.3040 - val_acc: 0.9193\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.2055 - acc: 0.9410 - val_loss: 0.2855 - val_acc: 0.9210\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.1780 - acc: 0.9510 - val_loss: 0.2607 - val_acc: 0.9277\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 0s 88us/sample - loss: 0.1764 - acc: 0.9440 - val_loss: 0.2524 - val_acc: 0.9288\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 0s 75us/sample - loss: 0.1756 - acc: 0.9470 - val_loss: 0.2420 - val_acc: 0.9293\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.1669 - acc: 0.9520 - val_loss: 0.2350 - val_acc: 0.9338\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.1656 - acc: 0.9530 - val_loss: 0.2293 - val_acc: 0.9332\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.1567 - acc: 0.9510 - val_loss: 0.2192 - val_acc: 0.9393\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.1456 - acc: 0.9590 - val_loss: 0.2180 - val_acc: 0.9321\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.1331 - acc: 0.9680 - val_loss: 0.2156 - val_acc: 0.9349\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.1411 - acc: 0.9550 - val_loss: 0.2113 - val_acc: 0.9366\n",
      "undersampling test loss:  0.21132343298919876\n",
      "undersampling accuracy:  0.9365609\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_72 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 1000 samples, validate on 1797 samples\n",
      "Epoch 1/30\n",
      " 640/1000 [==================>...........] - ETA: 0s - loss: 3.0304 - acc: 0.1672"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 2.9194 - acc: 0.1880 - val_loss: 2.2296 - val_acc: 0.1146\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 1.9323 - acc: 0.3720 - val_loss: 2.0900 - val_acc: 0.3317\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s 76us/sample - loss: 1.4828 - acc: 0.5040 - val_loss: 1.9477 - val_acc: 0.5442\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 1.1017 - acc: 0.6350 - val_loss: 1.8139 - val_acc: 0.6544\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.9693 - acc: 0.6870 - val_loss: 1.6815 - val_acc: 0.7117\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.7921 - acc: 0.7420 - val_loss: 1.5443 - val_acc: 0.7462\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.6773 - acc: 0.7850 - val_loss: 1.4117 - val_acc: 0.7551\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.6269 - acc: 0.7930 - val_loss: 1.2641 - val_acc: 0.7869\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.5643 - acc: 0.8280 - val_loss: 1.1340 - val_acc: 0.8119\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.5034 - acc: 0.8460 - val_loss: 0.9976 - val_acc: 0.8353\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.4318 - acc: 0.8690 - val_loss: 0.8865 - val_acc: 0.8520\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.4107 - acc: 0.8800 - val_loss: 0.7645 - val_acc: 0.8748\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.3898 - acc: 0.8870 - val_loss: 0.6592 - val_acc: 0.8915\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.3376 - acc: 0.9050 - val_loss: 0.5748 - val_acc: 0.8993\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.3177 - acc: 0.9150 - val_loss: 0.5046 - val_acc: 0.8987\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.3034 - acc: 0.9180 - val_loss: 0.4511 - val_acc: 0.9087\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.2992 - acc: 0.9240 - val_loss: 0.4084 - val_acc: 0.9115\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.2782 - acc: 0.9160 - val_loss: 0.3675 - val_acc: 0.9137\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.2521 - acc: 0.9310 - val_loss: 0.3455 - val_acc: 0.9160\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.2418 - acc: 0.9350 - val_loss: 0.3186 - val_acc: 0.9210\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.2270 - acc: 0.9410 - val_loss: 0.2944 - val_acc: 0.9260\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.2183 - acc: 0.9450 - val_loss: 0.2792 - val_acc: 0.9254\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.1941 - acc: 0.9430 - val_loss: 0.2614 - val_acc: 0.9299\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.1755 - acc: 0.9540 - val_loss: 0.2558 - val_acc: 0.9288\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.1698 - acc: 0.9570 - val_loss: 0.2473 - val_acc: 0.9299\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.1710 - acc: 0.9520 - val_loss: 0.2445 - val_acc: 0.9316\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.1604 - acc: 0.9620 - val_loss: 0.2328 - val_acc: 0.9343\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.1617 - acc: 0.9580 - val_loss: 0.2247 - val_acc: 0.9332\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.1417 - acc: 0.9610 - val_loss: 0.2197 - val_acc: 0.9360\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 0s 78us/sample - loss: 0.1711 - acc: 0.9510 - val_loss: 0.2179 - val_acc: 0.9366\n",
      "undersampling test loss:  0.21786899876697366\n",
      "undersampling accuracy:  0.9365609\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_75 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 1000 samples, validate on 1797 samples\n",
      "Epoch 1/30\n",
      "  64/1000 [>.............................] - ETA: 3s - loss: 3.3663 - acc: 0.1719"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 932us/sample - loss: 2.8432 - acc: 0.1820 - val_loss: 2.1884 - val_acc: 0.1775\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 1.8954 - acc: 0.3970 - val_loss: 2.0483 - val_acc: 0.3250\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 1.3532 - acc: 0.5320 - val_loss: 1.9111 - val_acc: 0.5303\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 1.0498 - acc: 0.6500 - val_loss: 1.7605 - val_acc: 0.6962\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.8807 - acc: 0.7200 - val_loss: 1.6118 - val_acc: 0.7457\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s 86us/sample - loss: 0.7091 - acc: 0.7760 - val_loss: 1.4575 - val_acc: 0.7869\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s 86us/sample - loss: 0.6344 - acc: 0.8180 - val_loss: 1.3046 - val_acc: 0.8075\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.5189 - acc: 0.8450 - val_loss: 1.1538 - val_acc: 0.8269\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.4628 - acc: 0.8600 - val_loss: 1.0057 - val_acc: 0.8442\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.4304 - acc: 0.8690 - val_loss: 0.8590 - val_acc: 0.8648\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.3982 - acc: 0.8770 - val_loss: 0.7365 - val_acc: 0.8765\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.3395 - acc: 0.9050 - val_loss: 0.6393 - val_acc: 0.8876\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.3604 - acc: 0.8860 - val_loss: 0.5662 - val_acc: 0.8920\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.3199 - acc: 0.9020 - val_loss: 0.5055 - val_acc: 0.8943\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.3352 - acc: 0.8890 - val_loss: 0.4595 - val_acc: 0.8954\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.2690 - acc: 0.9190 - val_loss: 0.4104 - val_acc: 0.9032\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.2708 - acc: 0.9180 - val_loss: 0.3763 - val_acc: 0.9115\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.2590 - acc: 0.9250 - val_loss: 0.3476 - val_acc: 0.9121\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.2514 - acc: 0.9360 - val_loss: 0.3218 - val_acc: 0.9188\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.2505 - acc: 0.9230 - val_loss: 0.3033 - val_acc: 0.9199\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 0.2174 - acc: 0.9300 - val_loss: 0.2891 - val_acc: 0.9249\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.2244 - acc: 0.9310 - val_loss: 0.2728 - val_acc: 0.9265\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.1848 - acc: 0.9500 - val_loss: 0.2654 - val_acc: 0.9293\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.1959 - acc: 0.9430 - val_loss: 0.2555 - val_acc: 0.9277\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.2109 - acc: 0.9400 - val_loss: 0.2486 - val_acc: 0.9293\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 0s 78us/sample - loss: 0.1798 - acc: 0.9490 - val_loss: 0.2454 - val_acc: 0.9282\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.1757 - acc: 0.9530 - val_loss: 0.2363 - val_acc: 0.9310\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.1778 - acc: 0.9450 - val_loss: 0.2290 - val_acc: 0.9338\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.1545 - acc: 0.9540 - val_loss: 0.2265 - val_acc: 0.9349\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 0s 86us/sample - loss: 0.1584 - acc: 0.9510 - val_loss: 0.2269 - val_acc: 0.9349\n",
      "undersampling test loss:  0.22691573603021714\n",
      "undersampling accuracy:  0.93489146\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_78 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_26 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 1000 samples, validate on 1797 samples\n",
      "Epoch 1/30\n",
      "  64/1000 [>.............................] - ETA: 3s - loss: 3.4331 - acc: 0.0781"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 948us/sample - loss: 2.8506 - acc: 0.1630 - val_loss: 2.1973 - val_acc: 0.1981\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 1.9742 - acc: 0.3420 - val_loss: 2.0857 - val_acc: 0.3656\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 1.4466 - acc: 0.5120 - val_loss: 1.9629 - val_acc: 0.5454\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s 86us/sample - loss: 1.2038 - acc: 0.5840 - val_loss: 1.8304 - val_acc: 0.6221\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.9446 - acc: 0.6820 - val_loss: 1.6905 - val_acc: 0.6811\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.7983 - acc: 0.7240 - val_loss: 1.5465 - val_acc: 0.7023\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.7169 - acc: 0.7780 - val_loss: 1.3986 - val_acc: 0.7362\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.6068 - acc: 0.8090 - val_loss: 1.2488 - val_acc: 0.7802\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.5379 - acc: 0.8420 - val_loss: 1.1058 - val_acc: 0.7986\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.5033 - acc: 0.8440 - val_loss: 0.9651 - val_acc: 0.8370\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.4445 - acc: 0.8710 - val_loss: 0.8432 - val_acc: 0.8559\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 0.4032 - acc: 0.8760 - val_loss: 0.7291 - val_acc: 0.8748\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.3590 - acc: 0.8950 - val_loss: 0.6309 - val_acc: 0.8887\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.3354 - acc: 0.9100 - val_loss: 0.5471 - val_acc: 0.8982\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.2942 - acc: 0.9240 - val_loss: 0.4816 - val_acc: 0.9026\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 0s 87us/sample - loss: 0.2961 - acc: 0.9160 - val_loss: 0.4315 - val_acc: 0.9048\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.2952 - acc: 0.9180 - val_loss: 0.3861 - val_acc: 0.9132\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.2593 - acc: 0.9340 - val_loss: 0.3489 - val_acc: 0.9182\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.2531 - acc: 0.9240 - val_loss: 0.3269 - val_acc: 0.9210\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.2276 - acc: 0.9320 - val_loss: 0.3004 - val_acc: 0.9249\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.2076 - acc: 0.9400 - val_loss: 0.2774 - val_acc: 0.9316\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.2080 - acc: 0.9360 - val_loss: 0.2719 - val_acc: 0.9288\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.2035 - acc: 0.9450 - val_loss: 0.2596 - val_acc: 0.9260\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 0s 86us/sample - loss: 0.2062 - acc: 0.9440 - val_loss: 0.2434 - val_acc: 0.9293\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.1814 - acc: 0.9500 - val_loss: 0.2313 - val_acc: 0.9299\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 0.1756 - acc: 0.9530 - val_loss: 0.2234 - val_acc: 0.9310\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.1699 - acc: 0.9540 - val_loss: 0.2164 - val_acc: 0.9316\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.1565 - acc: 0.9570 - val_loss: 0.2183 - val_acc: 0.9343\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.1560 - acc: 0.9550 - val_loss: 0.2096 - val_acc: 0.9343\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 0s 78us/sample - loss: 0.1367 - acc: 0.9680 - val_loss: 0.1989 - val_acc: 0.9399\n",
      "undersampling test loss:  0.19893178064687075\n",
      "undersampling accuracy:  0.93989986\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_81 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_27 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 1000 samples, validate on 1797 samples\n",
      "Epoch 1/30\n",
      "  64/1000 [>.............................] - ETA: 3s - loss: 3.7176 - acc: 0.0781"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 986us/sample - loss: 2.8983 - acc: 0.1800 - val_loss: 2.2201 - val_acc: 0.1959\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 1.9440 - acc: 0.3710 - val_loss: 2.0900 - val_acc: 0.2827\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 1.4033 - acc: 0.5270 - val_loss: 1.9593 - val_acc: 0.3901\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 1.1119 - acc: 0.6280 - val_loss: 1.8214 - val_acc: 0.5042\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 0.8883 - acc: 0.7070 - val_loss: 1.6789 - val_acc: 0.5860\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 0.8010 - acc: 0.7520 - val_loss: 1.5314 - val_acc: 0.6700\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.6872 - acc: 0.7880 - val_loss: 1.3754 - val_acc: 0.7240\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 0.5779 - acc: 0.8160 - val_loss: 1.2286 - val_acc: 0.7613\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 0.5511 - acc: 0.8170 - val_loss: 1.0823 - val_acc: 0.8075\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s 87us/sample - loss: 0.4707 - acc: 0.8590 - val_loss: 0.9362 - val_acc: 0.8425\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.4421 - acc: 0.8690 - val_loss: 0.8114 - val_acc: 0.8637\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.3962 - acc: 0.8860 - val_loss: 0.7038 - val_acc: 0.8781\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.3580 - acc: 0.8900 - val_loss: 0.6168 - val_acc: 0.8854\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.3192 - acc: 0.9150 - val_loss: 0.5425 - val_acc: 0.8926\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 0s 87us/sample - loss: 0.3141 - acc: 0.9130 - val_loss: 0.4718 - val_acc: 0.9043\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 0s 86us/sample - loss: 0.2832 - acc: 0.9200 - val_loss: 0.4161 - val_acc: 0.9110\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.2878 - acc: 0.9130 - val_loss: 0.3747 - val_acc: 0.9149\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.2602 - acc: 0.9160 - val_loss: 0.3376 - val_acc: 0.9199\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.2557 - acc: 0.9190 - val_loss: 0.3100 - val_acc: 0.9221\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.2123 - acc: 0.9460 - val_loss: 0.2872 - val_acc: 0.9232\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.2157 - acc: 0.9330 - val_loss: 0.2683 - val_acc: 0.9238\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.2087 - acc: 0.9380 - val_loss: 0.2522 - val_acc: 0.9277\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.1825 - acc: 0.9510 - val_loss: 0.2455 - val_acc: 0.9282\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.2039 - acc: 0.9510 - val_loss: 0.2352 - val_acc: 0.9304\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.1992 - acc: 0.9390 - val_loss: 0.2305 - val_acc: 0.9321\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 0s 90us/sample - loss: 0.1741 - acc: 0.9460 - val_loss: 0.2210 - val_acc: 0.9327\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.1804 - acc: 0.9410 - val_loss: 0.2172 - val_acc: 0.9299\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 0s 87us/sample - loss: 0.1634 - acc: 0.9550 - val_loss: 0.2139 - val_acc: 0.9327\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 0s 86us/sample - loss: 0.1575 - acc: 0.9570 - val_loss: 0.2096 - val_acc: 0.9377\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 0s 86us/sample - loss: 0.1363 - acc: 0.9540 - val_loss: 0.2028 - val_acc: 0.9371\n",
      "undersampling test loss:  0.20279201512672235\n",
      "undersampling accuracy:  0.9371174\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_84 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_28 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 1000 samples, validate on 1797 samples\n",
      "Epoch 1/30\n",
      "  64/1000 [>.............................] - ETA: 3s - loss: 3.9208 - acc: 0.0781"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 3.0500 - acc: 0.1310 - val_loss: 2.2518 - val_acc: 0.1914\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 2.0074 - acc: 0.3530 - val_loss: 2.1160 - val_acc: 0.2777\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s 86us/sample - loss: 1.3865 - acc: 0.5450 - val_loss: 1.9777 - val_acc: 0.4380\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s 88us/sample - loss: 1.0853 - acc: 0.6450 - val_loss: 1.8362 - val_acc: 0.5715\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s 86us/sample - loss: 0.9378 - acc: 0.7180 - val_loss: 1.6843 - val_acc: 0.6628\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.7939 - acc: 0.7590 - val_loss: 1.5314 - val_acc: 0.7373\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s 87us/sample - loss: 0.6925 - acc: 0.7880 - val_loss: 1.3772 - val_acc: 0.7858\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s 88us/sample - loss: 0.6014 - acc: 0.8120 - val_loss: 1.2230 - val_acc: 0.8191\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s 86us/sample - loss: 0.5204 - acc: 0.8370 - val_loss: 1.0806 - val_acc: 0.8331\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s 88us/sample - loss: 0.4482 - acc: 0.8620 - val_loss: 0.9407 - val_acc: 0.8687\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s 87us/sample - loss: 0.4189 - acc: 0.8780 - val_loss: 0.8173 - val_acc: 0.8804\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.4006 - acc: 0.8730 - val_loss: 0.7224 - val_acc: 0.8843\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.3894 - acc: 0.8840 - val_loss: 0.6312 - val_acc: 0.8915\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.2921 - acc: 0.9190 - val_loss: 0.5436 - val_acc: 0.9037\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.3213 - acc: 0.9090 - val_loss: 0.4798 - val_acc: 0.9098\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.2934 - acc: 0.9160 - val_loss: 0.4365 - val_acc: 0.9110\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.2602 - acc: 0.9260 - val_loss: 0.3928 - val_acc: 0.9171\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 0s 94us/sample - loss: 0.2687 - acc: 0.9190 - val_loss: 0.3616 - val_acc: 0.9188\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 0s 88us/sample - loss: 0.2373 - acc: 0.9370 - val_loss: 0.3331 - val_acc: 0.9226\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 0s 88us/sample - loss: 0.1959 - acc: 0.9550 - val_loss: 0.3032 - val_acc: 0.9260\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 0s 90us/sample - loss: 0.2059 - acc: 0.9480 - val_loss: 0.2857 - val_acc: 0.9260\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 0s 87us/sample - loss: 0.2189 - acc: 0.9380 - val_loss: 0.2715 - val_acc: 0.9299\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 0s 90us/sample - loss: 0.2010 - acc: 0.9450 - val_loss: 0.2578 - val_acc: 0.9310\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 0s 88us/sample - loss: 0.2037 - acc: 0.9370 - val_loss: 0.2417 - val_acc: 0.9371\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.1752 - acc: 0.9520 - val_loss: 0.2399 - val_acc: 0.9338\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.1722 - acc: 0.9520 - val_loss: 0.2211 - val_acc: 0.9399\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 0s 89us/sample - loss: 0.1483 - acc: 0.9650 - val_loss: 0.2069 - val_acc: 0.9444\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 0s 88us/sample - loss: 0.1428 - acc: 0.9600 - val_loss: 0.2024 - val_acc: 0.9444\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 0s 86us/sample - loss: 0.1505 - acc: 0.9550 - val_loss: 0.2020 - val_acc: 0.9460\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.1288 - acc: 0.9650 - val_loss: 0.2016 - val_acc: 0.9444\n",
      "undersampling test loss:  0.20162389026078903\n",
      "undersampling accuracy:  0.9443517\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_87 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_59 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_29 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 1000 samples, validate on 1797 samples\n",
      "Epoch 1/30\n",
      "  64/1000 [>.............................] - ETA: 3s - loss: 3.6258 - acc: 0.1250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 2.8490 - acc: 0.1780 - val_loss: 2.2311 - val_acc: 0.2204\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s 86us/sample - loss: 1.9339 - acc: 0.3730 - val_loss: 2.1057 - val_acc: 0.3812\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 1.4719 - acc: 0.5020 - val_loss: 1.9755 - val_acc: 0.5175\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 1.1197 - acc: 0.6290 - val_loss: 1.8429 - val_acc: 0.6077\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s 90us/sample - loss: 0.9037 - acc: 0.7200 - val_loss: 1.7027 - val_acc: 0.6939\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s 90us/sample - loss: 0.7753 - acc: 0.7590 - val_loss: 1.5597 - val_acc: 0.7385\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s 90us/sample - loss: 0.6727 - acc: 0.7850 - val_loss: 1.4059 - val_acc: 0.7830\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s 91us/sample - loss: 0.5737 - acc: 0.8350 - val_loss: 1.2577 - val_acc: 0.8130\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s 101us/sample - loss: 0.5508 - acc: 0.8370 - val_loss: 1.1114 - val_acc: 0.8342\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s 100us/sample - loss: 0.5115 - acc: 0.8520 - val_loss: 0.9814 - val_acc: 0.8503\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s 89us/sample - loss: 0.4560 - acc: 0.8710 - val_loss: 0.8484 - val_acc: 0.8653\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s 151us/sample - loss: 0.4142 - acc: 0.8810 - val_loss: 0.7325 - val_acc: 0.8726\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 0s 140us/sample - loss: 0.3889 - acc: 0.8910 - val_loss: 0.6333 - val_acc: 0.8904\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 0s 123us/sample - loss: 0.3483 - acc: 0.8930 - val_loss: 0.5460 - val_acc: 0.8987\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 0s 89us/sample - loss: 0.3654 - acc: 0.8950 - val_loss: 0.4789 - val_acc: 0.9060\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 0s 98us/sample - loss: 0.3198 - acc: 0.9030 - val_loss: 0.4207 - val_acc: 0.9104\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 0s 139us/sample - loss: 0.2677 - acc: 0.9200 - val_loss: 0.3735 - val_acc: 0.9165\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 0s 113us/sample - loss: 0.2648 - acc: 0.9230 - val_loss: 0.3425 - val_acc: 0.9210\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 0s 91us/sample - loss: 0.2592 - acc: 0.9250 - val_loss: 0.3142 - val_acc: 0.9238\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 0s 86us/sample - loss: 0.2224 - acc: 0.9340 - val_loss: 0.2846 - val_acc: 0.9299\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 0s 126us/sample - loss: 0.2612 - acc: 0.9160 - val_loss: 0.2643 - val_acc: 0.9338\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 0s 138us/sample - loss: 0.2215 - acc: 0.9360 - val_loss: 0.2538 - val_acc: 0.9310\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 0s 129us/sample - loss: 0.2243 - acc: 0.9400 - val_loss: 0.2419 - val_acc: 0.9343\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 0s 92us/sample - loss: 0.1855 - acc: 0.9430 - val_loss: 0.2306 - val_acc: 0.9343\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 0s 87us/sample - loss: 0.1934 - acc: 0.9460 - val_loss: 0.2214 - val_acc: 0.9366\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 0s 93us/sample - loss: 0.2144 - acc: 0.9340 - val_loss: 0.2124 - val_acc: 0.9388\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 0s 95us/sample - loss: 0.1931 - acc: 0.9460 - val_loss: 0.2077 - val_acc: 0.9377\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 0s 89us/sample - loss: 0.1796 - acc: 0.9550 - val_loss: 0.2031 - val_acc: 0.9377\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 0s 99us/sample - loss: 0.1774 - acc: 0.9530 - val_loss: 0.1956 - val_acc: 0.9416\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 0s 144us/sample - loss: 0.1657 - acc: 0.9520 - val_loss: 0.1885 - val_acc: 0.9438\n",
      "undersampling test loss:  0.1884891626051418\n",
      "undersampling accuracy:  0.9437952\n"
     ]
    }
   ],
   "source": [
    "undersampling_list =[]\n",
    "for i in range(10):\n",
    "    bl_model = build_model()\n",
    "    batch_size=64\n",
    "    epochs=30\n",
    "    \n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_train, y_train1 = rus.fit_resample(train_images, train_y)\n",
    "    \n",
    "    bl_history = bl_model.fit(X_train, y_train1, batch_size=batch_size,\n",
    "                        epochs=epochs, validation_data=(test_images, test_y))\n",
    "\n",
    "    bl_score = bl_model.evaluate(test_images, test_y, verbose=0)\n",
    "    print('undersampling test loss: ', bl_score[0])\n",
    "    print('undersampling accuracy: ', bl_score[1] )\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    y_pred_oh = bl_model.predict(test_images)\n",
    "    y_pred_aug = y_pred_oh.argmax(axis=-1)\n",
    "    undersampling_list.append(classification_report(y_test, y_pred_aug, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a585c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of runs: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.988242</td>\n",
       "      <td>0.984831</td>\n",
       "      <td>0.986499</td>\n",
       "      <td>178.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.899379</td>\n",
       "      <td>0.913187</td>\n",
       "      <td>0.905686</td>\n",
       "      <td>182.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.971281</td>\n",
       "      <td>0.967232</td>\n",
       "      <td>0.969116</td>\n",
       "      <td>177.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.974604</td>\n",
       "      <td>0.915847</td>\n",
       "      <td>0.944138</td>\n",
       "      <td>183.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.934698</td>\n",
       "      <td>0.970718</td>\n",
       "      <td>0.952162</td>\n",
       "      <td>181.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.920423</td>\n",
       "      <td>0.974725</td>\n",
       "      <td>0.946710</td>\n",
       "      <td>182.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.983315</td>\n",
       "      <td>0.970718</td>\n",
       "      <td>0.976935</td>\n",
       "      <td>181.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.983503</td>\n",
       "      <td>0.894413</td>\n",
       "      <td>0.936705</td>\n",
       "      <td>179.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.873491</td>\n",
       "      <td>0.871839</td>\n",
       "      <td>0.871879</td>\n",
       "      <td>174.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.864534</td>\n",
       "      <td>0.911667</td>\n",
       "      <td>0.887053</td>\n",
       "      <td>180.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.937674</td>\n",
       "      <td>0.937674</td>\n",
       "      <td>0.937674</td>\n",
       "      <td>0.937674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.939347</td>\n",
       "      <td>0.937518</td>\n",
       "      <td>0.937688</td>\n",
       "      <td>1797.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.939450</td>\n",
       "      <td>0.937674</td>\n",
       "      <td>0.937817</td>\n",
       "      <td>1797.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.988242  0.984831  0.986499   178.000000\n",
       "1              0.899379  0.913187  0.905686   182.000000\n",
       "2              0.971281  0.967232  0.969116   177.000000\n",
       "3              0.974604  0.915847  0.944138   183.000000\n",
       "4              0.934698  0.970718  0.952162   181.000000\n",
       "5              0.920423  0.974725  0.946710   182.000000\n",
       "6              0.983315  0.970718  0.976935   181.000000\n",
       "7              0.983503  0.894413  0.936705   179.000000\n",
       "8              0.873491  0.871839  0.871879   174.000000\n",
       "9              0.864534  0.911667  0.887053   180.000000\n",
       "accuracy       0.937674  0.937674  0.937674     0.937674\n",
       "macro avg      0.939347  0.937518  0.937688  1797.000000\n",
       "weighted avg   0.939450  0.937674  0.937817  1797.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_process_results(undersampling_list, 'results_csv/opt_undersampling.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d417e0",
   "metadata": {},
   "source": [
    "# Random OverSampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d8e3796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_120 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_80 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_81 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_40 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 3890 samples, validate on 1797 samples\n",
      "Epoch 1/30\n",
      "3136/3890 [=======================>......] - ETA: 0s - loss: 2.1528 - acc: 0.3555"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3890/3890 [==============================] - 2s 407us/sample - loss: 1.9802 - acc: 0.3995 - val_loss: 1.8444 - val_acc: 0.6984\n",
      "Epoch 2/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.8688 - acc: 0.7257 - val_loss: 1.2889 - val_acc: 0.8381\n",
      "Epoch 3/30\n",
      "3890/3890 [==============================] - 0s 64us/sample - loss: 0.5559 - acc: 0.8288 - val_loss: 0.7834 - val_acc: 0.8820\n",
      "Epoch 4/30\n",
      "3890/3890 [==============================] - 0s 65us/sample - loss: 0.4228 - acc: 0.8758 - val_loss: 0.4462 - val_acc: 0.9165\n",
      "Epoch 5/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.3209 - acc: 0.9046 - val_loss: 0.3128 - val_acc: 0.9265\n",
      "Epoch 6/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.2773 - acc: 0.9177 - val_loss: 0.2473 - val_acc: 0.9332\n",
      "Epoch 7/30\n",
      "3890/3890 [==============================] - 0s 64us/sample - loss: 0.2582 - acc: 0.9267 - val_loss: 0.2165 - val_acc: 0.9371\n",
      "Epoch 8/30\n",
      "3890/3890 [==============================] - 0s 64us/sample - loss: 0.2157 - acc: 0.9365 - val_loss: 0.1982 - val_acc: 0.9388\n",
      "Epoch 9/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.1903 - acc: 0.9429 - val_loss: 0.1867 - val_acc: 0.9421\n",
      "Epoch 10/30\n",
      "3890/3890 [==============================] - 0s 65us/sample - loss: 0.1631 - acc: 0.9524 - val_loss: 0.1843 - val_acc: 0.9421\n",
      "Epoch 11/30\n",
      "3890/3890 [==============================] - 0s 64us/sample - loss: 0.1666 - acc: 0.9506 - val_loss: 0.1710 - val_acc: 0.9482\n",
      "Epoch 12/30\n",
      "3890/3890 [==============================] - 0s 65us/sample - loss: 0.1369 - acc: 0.9568 - val_loss: 0.1635 - val_acc: 0.9471\n",
      "Epoch 13/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.1307 - acc: 0.9596 - val_loss: 0.1603 - val_acc: 0.9471\n",
      "Epoch 14/30\n",
      "3890/3890 [==============================] - 0s 65us/sample - loss: 0.1324 - acc: 0.9612 - val_loss: 0.1656 - val_acc: 0.9471\n",
      "Epoch 15/30\n",
      "3890/3890 [==============================] - 0s 64us/sample - loss: 0.1221 - acc: 0.9638 - val_loss: 0.1589 - val_acc: 0.9549\n",
      "Epoch 16/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.1121 - acc: 0.9625 - val_loss: 0.1551 - val_acc: 0.9516\n",
      "Epoch 17/30\n",
      "3890/3890 [==============================] - 0s 65us/sample - loss: 0.1096 - acc: 0.9712 - val_loss: 0.1601 - val_acc: 0.9538\n",
      "Epoch 18/30\n",
      "3890/3890 [==============================] - 0s 65us/sample - loss: 0.0968 - acc: 0.9722 - val_loss: 0.1548 - val_acc: 0.9538\n",
      "Epoch 19/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.0905 - acc: 0.9738 - val_loss: 0.1466 - val_acc: 0.9566\n",
      "Epoch 20/30\n",
      "3890/3890 [==============================] - 0s 65us/sample - loss: 0.0883 - acc: 0.9715 - val_loss: 0.1491 - val_acc: 0.9538\n",
      "Epoch 21/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.0769 - acc: 0.9794 - val_loss: 0.1560 - val_acc: 0.9560\n",
      "Epoch 22/30\n",
      "3890/3890 [==============================] - 0s 65us/sample - loss: 0.0730 - acc: 0.9787 - val_loss: 0.1584 - val_acc: 0.9549\n",
      "Epoch 23/30\n",
      "3890/3890 [==============================] - 0s 65us/sample - loss: 0.0805 - acc: 0.9743 - val_loss: 0.1557 - val_acc: 0.9544\n",
      "Epoch 24/30\n",
      "3890/3890 [==============================] - 0s 64us/sample - loss: 0.0754 - acc: 0.9776 - val_loss: 0.1577 - val_acc: 0.9527\n",
      "Epoch 25/30\n",
      "3890/3890 [==============================] - 0s 65us/sample - loss: 0.0714 - acc: 0.9797 - val_loss: 0.1531 - val_acc: 0.9594\n",
      "Epoch 26/30\n",
      "3890/3890 [==============================] - 0s 65us/sample - loss: 0.0637 - acc: 0.9807 - val_loss: 0.1548 - val_acc: 0.9605\n",
      "Epoch 27/30\n",
      "3890/3890 [==============================] - 0s 71us/sample - loss: 0.0616 - acc: 0.9833 - val_loss: 0.1422 - val_acc: 0.9583\n",
      "Epoch 28/30\n",
      "3890/3890 [==============================] - 0s 64us/sample - loss: 0.0583 - acc: 0.9828 - val_loss: 0.1671 - val_acc: 0.9577\n",
      "Epoch 29/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.0536 - acc: 0.9846 - val_loss: 0.1494 - val_acc: 0.9572\n",
      "Epoch 30/30\n",
      "3890/3890 [==============================] - 0s 64us/sample - loss: 0.0487 - acc: 0.9874 - val_loss: 0.1452 - val_acc: 0.9588\n",
      "oversampling test loss:  0.14524579887876748\n",
      "oversampling accuracy:  0.9588203\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_123 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_82 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_83 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_41 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 3890 samples, validate on 1797 samples\n",
      "Epoch 1/30\n",
      "3072/3890 [======================>.......] - ETA: 0s - loss: 2.2202 - acc: 0.3421"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3890/3890 [==============================] - 2s 408us/sample - loss: 2.0044 - acc: 0.3938 - val_loss: 1.8297 - val_acc: 0.5593\n",
      "Epoch 2/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.8637 - acc: 0.7213 - val_loss: 1.2734 - val_acc: 0.7613\n",
      "Epoch 3/30\n",
      "3890/3890 [==============================] - 0s 64us/sample - loss: 0.5413 - acc: 0.8362 - val_loss: 0.7942 - val_acc: 0.8397\n",
      "Epoch 4/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.4131 - acc: 0.8815 - val_loss: 0.4851 - val_acc: 0.8937\n",
      "Epoch 5/30\n",
      "3890/3890 [==============================] - 0s 63us/sample - loss: 0.3358 - acc: 0.9031 - val_loss: 0.3370 - val_acc: 0.9093\n",
      "Epoch 6/30\n",
      "3890/3890 [==============================] - 0s 65us/sample - loss: 0.2660 - acc: 0.9280 - val_loss: 0.2470 - val_acc: 0.9338\n",
      "Epoch 7/30\n",
      "3890/3890 [==============================] - ETA: 0s - loss: 0.2386 - acc: 0.933 - 0s 65us/sample - loss: 0.2438 - acc: 0.9314 - val_loss: 0.2177 - val_acc: 0.9382\n",
      "Epoch 8/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.2108 - acc: 0.9386 - val_loss: 0.1958 - val_acc: 0.9438\n",
      "Epoch 9/30\n",
      "3890/3890 [==============================] - 0s 64us/sample - loss: 0.1753 - acc: 0.9512 - val_loss: 0.1791 - val_acc: 0.9449\n",
      "Epoch 10/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.1680 - acc: 0.9527 - val_loss: 0.1729 - val_acc: 0.9427\n",
      "Epoch 11/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.1507 - acc: 0.9555 - val_loss: 0.1652 - val_acc: 0.9499\n",
      "Epoch 12/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.1359 - acc: 0.9622 - val_loss: 0.1635 - val_acc: 0.9499\n",
      "Epoch 13/30\n",
      "3890/3890 [==============================] - 0s 64us/sample - loss: 0.1245 - acc: 0.9630 - val_loss: 0.1498 - val_acc: 0.9510\n",
      "Epoch 14/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.1072 - acc: 0.9681 - val_loss: 0.1527 - val_acc: 0.9527\n",
      "Epoch 15/30\n",
      "3890/3890 [==============================] - 0s 65us/sample - loss: 0.1164 - acc: 0.9676 - val_loss: 0.1449 - val_acc: 0.9527\n",
      "Epoch 16/30\n",
      "3890/3890 [==============================] - 0s 69us/sample - loss: 0.1033 - acc: 0.9697 - val_loss: 0.1468 - val_acc: 0.9521\n",
      "Epoch 17/30\n",
      "3890/3890 [==============================] - 0s 63us/sample - loss: 0.0938 - acc: 0.9710 - val_loss: 0.1364 - val_acc: 0.9533\n",
      "Epoch 18/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.0885 - acc: 0.9746 - val_loss: 0.1326 - val_acc: 0.9560\n",
      "Epoch 19/30\n",
      "3890/3890 [==============================] - 0s 65us/sample - loss: 0.0880 - acc: 0.9740 - val_loss: 0.1291 - val_acc: 0.9577\n",
      "Epoch 20/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.0805 - acc: 0.9787 - val_loss: 0.1460 - val_acc: 0.9533\n",
      "Epoch 21/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.0788 - acc: 0.9771 - val_loss: 0.1314 - val_acc: 0.9599\n",
      "Epoch 22/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.0731 - acc: 0.9769 - val_loss: 0.1414 - val_acc: 0.9549\n",
      "Epoch 23/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.0690 - acc: 0.9794 - val_loss: 0.1379 - val_acc: 0.9566\n",
      "Epoch 24/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.0579 - acc: 0.9810 - val_loss: 0.1316 - val_acc: 0.9577\n",
      "Epoch 25/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.0573 - acc: 0.9825 - val_loss: 0.1310 - val_acc: 0.9560\n",
      "Epoch 26/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.0572 - acc: 0.9856 - val_loss: 0.1337 - val_acc: 0.9577\n",
      "Epoch 27/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.0541 - acc: 0.9828 - val_loss: 0.1422 - val_acc: 0.9566\n",
      "Epoch 28/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.0535 - acc: 0.9828 - val_loss: 0.1505 - val_acc: 0.9533\n",
      "Epoch 29/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.0569 - acc: 0.9859 - val_loss: 0.1268 - val_acc: 0.9577\n",
      "Epoch 30/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.0474 - acc: 0.9856 - val_loss: 0.1504 - val_acc: 0.9555\n",
      "oversampling test loss:  0.15035024542965536\n",
      "oversampling accuracy:  0.95548135\n",
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_126 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_84 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_127 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_85 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_42 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_128 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 3890 samples, validate on 1797 samples\n",
      "Epoch 1/30\n",
      "3072/3890 [======================>.......] - ETA: 0s - loss: 2.0894 - acc: 0.3685"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3890/3890 [==============================] - 2s 416us/sample - loss: 1.9090 - acc: 0.4157 - val_loss: 1.8294 - val_acc: 0.5031\n",
      "Epoch 2/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.8243 - acc: 0.7463 - val_loss: 1.2646 - val_acc: 0.8230\n",
      "Epoch 3/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.5600 - acc: 0.8290 - val_loss: 0.7581 - val_acc: 0.8826\n",
      "Epoch 4/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.4204 - acc: 0.8774 - val_loss: 0.4633 - val_acc: 0.8998\n",
      "Epoch 5/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.3510 - acc: 0.8997 - val_loss: 0.3174 - val_acc: 0.9204\n",
      "Epoch 6/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.2846 - acc: 0.9165 - val_loss: 0.2606 - val_acc: 0.9249\n",
      "Epoch 7/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.2476 - acc: 0.9316 - val_loss: 0.2224 - val_acc: 0.9299\n",
      "Epoch 8/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.2247 - acc: 0.9368 - val_loss: 0.2024 - val_acc: 0.9388\n",
      "Epoch 9/30\n",
      "3890/3890 [==============================] - 0s 64us/sample - loss: 0.1948 - acc: 0.9440 - val_loss: 0.1881 - val_acc: 0.9388\n",
      "Epoch 10/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.1793 - acc: 0.9506 - val_loss: 0.1847 - val_acc: 0.9393\n",
      "Epoch 11/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.1696 - acc: 0.9491 - val_loss: 0.1732 - val_acc: 0.9410\n",
      "Epoch 12/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.1544 - acc: 0.9571 - val_loss: 0.1672 - val_acc: 0.9432\n",
      "Epoch 13/30\n",
      "3890/3890 [==============================] - 0s 65us/sample - loss: 0.1623 - acc: 0.9509 - val_loss: 0.1726 - val_acc: 0.9460\n",
      "Epoch 14/30\n",
      "3890/3890 [==============================] - 0s 70us/sample - loss: 0.1378 - acc: 0.9591 - val_loss: 0.1619 - val_acc: 0.9482\n",
      "Epoch 15/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.1221 - acc: 0.9686 - val_loss: 0.1593 - val_acc: 0.9466\n",
      "Epoch 16/30\n",
      "3890/3890 [==============================] - 0s 69us/sample - loss: 0.1208 - acc: 0.9617 - val_loss: 0.1562 - val_acc: 0.9516\n",
      "Epoch 17/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.1244 - acc: 0.9632 - val_loss: 0.1675 - val_acc: 0.9460\n",
      "Epoch 18/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.1183 - acc: 0.9653 - val_loss: 0.1635 - val_acc: 0.9482\n",
      "Epoch 19/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.1106 - acc: 0.9689 - val_loss: 0.1682 - val_acc: 0.9471\n",
      "Epoch 20/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.1056 - acc: 0.9668 - val_loss: 0.1387 - val_acc: 0.9577\n",
      "Epoch 21/30\n",
      "3890/3890 [==============================] - 0s 65us/sample - loss: 0.0998 - acc: 0.9681 - val_loss: 0.1491 - val_acc: 0.9521\n",
      "Epoch 22/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.0884 - acc: 0.9746 - val_loss: 0.1517 - val_acc: 0.9499\n",
      "Epoch 23/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.0842 - acc: 0.9774 - val_loss: 0.1519 - val_acc: 0.9510\n",
      "Epoch 24/30\n",
      "3890/3890 [==============================] - 0s 71us/sample - loss: 0.0839 - acc: 0.9771 - val_loss: 0.1520 - val_acc: 0.9488\n",
      "Epoch 25/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.0888 - acc: 0.9720 - val_loss: 0.1509 - val_acc: 0.9516\n",
      "Epoch 26/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.0803 - acc: 0.9758 - val_loss: 0.1346 - val_acc: 0.9533\n",
      "Epoch 27/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.0779 - acc: 0.9781 - val_loss: 0.1416 - val_acc: 0.9549\n",
      "Epoch 28/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.0679 - acc: 0.9792 - val_loss: 0.1490 - val_acc: 0.9527\n",
      "Epoch 29/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.0688 - acc: 0.9781 - val_loss: 0.1527 - val_acc: 0.9477\n",
      "Epoch 30/30\n",
      "3890/3890 [==============================] - 0s 71us/sample - loss: 0.0671 - acc: 0.9789 - val_loss: 0.1669 - val_acc: 0.9477\n",
      "oversampling test loss:  0.1668560371131589\n",
      "oversampling accuracy:  0.9476906\n",
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_129 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_86 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_130 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_87 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_43 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_131 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 3890 samples, validate on 1797 samples\n",
      "Epoch 1/30\n",
      "3136/3890 [=======================>......] - ETA: 0s - loss: 2.0882 - acc: 0.3680"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3890/3890 [==============================] - 2s 435us/sample - loss: 1.9004 - acc: 0.4180 - val_loss: 1.8095 - val_acc: 0.5821\n",
      "Epoch 2/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.7648 - acc: 0.7674 - val_loss: 1.2391 - val_acc: 0.8191\n",
      "Epoch 3/30\n",
      "3890/3890 [==============================] - 0s 69us/sample - loss: 0.5146 - acc: 0.8483 - val_loss: 0.7605 - val_acc: 0.8826\n",
      "Epoch 4/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.3801 - acc: 0.8841 - val_loss: 0.4453 - val_acc: 0.9149\n",
      "Epoch 5/30\n",
      "3890/3890 [==============================] - 0s 69us/sample - loss: 0.3248 - acc: 0.9031 - val_loss: 0.3085 - val_acc: 0.9271\n",
      "Epoch 6/30\n",
      "3890/3890 [==============================] - 0s 65us/sample - loss: 0.2712 - acc: 0.9221 - val_loss: 0.2360 - val_acc: 0.9382\n",
      "Epoch 7/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.2240 - acc: 0.9380 - val_loss: 0.2021 - val_acc: 0.9444\n",
      "Epoch 8/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.2061 - acc: 0.9409 - val_loss: 0.1764 - val_acc: 0.9449\n",
      "Epoch 9/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.2001 - acc: 0.9368 - val_loss: 0.1639 - val_acc: 0.9544\n",
      "Epoch 10/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.1651 - acc: 0.9540 - val_loss: 0.1592 - val_acc: 0.9499\n",
      "Epoch 11/30\n",
      "3890/3890 [==============================] - 0s 71us/sample - loss: 0.1633 - acc: 0.9524 - val_loss: 0.1556 - val_acc: 0.9505\n",
      "Epoch 12/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.1434 - acc: 0.9576 - val_loss: 0.1457 - val_acc: 0.9549\n",
      "Epoch 13/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.1250 - acc: 0.9650 - val_loss: 0.1362 - val_acc: 0.9566\n",
      "Epoch 14/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.1188 - acc: 0.9671 - val_loss: 0.1387 - val_acc: 0.9572\n",
      "Epoch 15/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.1056 - acc: 0.9697 - val_loss: 0.1350 - val_acc: 0.9555\n",
      "Epoch 16/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.0938 - acc: 0.9722 - val_loss: 0.1306 - val_acc: 0.9599\n",
      "Epoch 17/30\n",
      "3890/3890 [==============================] - 0s 69us/sample - loss: 0.0947 - acc: 0.9738 - val_loss: 0.1304 - val_acc: 0.9577\n",
      "Epoch 18/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.0868 - acc: 0.9761 - val_loss: 0.1213 - val_acc: 0.9583\n",
      "Epoch 19/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.0855 - acc: 0.9746 - val_loss: 0.1170 - val_acc: 0.9610\n",
      "Epoch 20/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.0675 - acc: 0.9792 - val_loss: 0.1228 - val_acc: 0.9622\n",
      "Epoch 21/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.0723 - acc: 0.9771 - val_loss: 0.1126 - val_acc: 0.9610\n",
      "Epoch 22/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.0713 - acc: 0.9776 - val_loss: 0.1220 - val_acc: 0.9583\n",
      "Epoch 23/30\n",
      "3890/3890 [==============================] - 0s 70us/sample - loss: 0.0706 - acc: 0.9781 - val_loss: 0.1303 - val_acc: 0.9583\n",
      "Epoch 24/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.0658 - acc: 0.9805 - val_loss: 0.1222 - val_acc: 0.9610\n",
      "Epoch 25/30\n",
      "3890/3890 [==============================] - 0s 69us/sample - loss: 0.0595 - acc: 0.9802 - val_loss: 0.1196 - val_acc: 0.9599\n",
      "Epoch 26/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.0576 - acc: 0.9823 - val_loss: 0.1221 - val_acc: 0.9633\n",
      "Epoch 27/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.0558 - acc: 0.9810 - val_loss: 0.1215 - val_acc: 0.9633\n",
      "Epoch 28/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.0485 - acc: 0.9838 - val_loss: 0.1364 - val_acc: 0.9583\n",
      "Epoch 29/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.0493 - acc: 0.9851 - val_loss: 0.1338 - val_acc: 0.9599\n",
      "Epoch 30/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.0513 - acc: 0.9841 - val_loss: 0.1210 - val_acc: 0.9583\n",
      "oversampling test loss:  0.12099861749980724\n",
      "oversampling accuracy:  0.95826375\n",
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_132 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_88 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_89 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_44 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 3890 samples, validate on 1797 samples\n",
      "Epoch 1/30\n",
      "3008/3890 [======================>.......] - ETA: 0s - loss: 2.3531 - acc: 0.2789"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3890/3890 [==============================] - 2s 433us/sample - loss: 2.1471 - acc: 0.3332 - val_loss: 1.9502 - val_acc: 0.4602\n",
      "Epoch 2/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.9840 - acc: 0.6735 - val_loss: 1.4350 - val_acc: 0.7685\n",
      "Epoch 3/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.6344 - acc: 0.7974 - val_loss: 0.9197 - val_acc: 0.8520\n",
      "Epoch 4/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.4465 - acc: 0.8617 - val_loss: 0.5282 - val_acc: 0.9009\n",
      "Epoch 5/30\n",
      "3890/3890 [==============================] - 0s 69us/sample - loss: 0.3646 - acc: 0.8923 - val_loss: 0.3435 - val_acc: 0.9199\n",
      "Epoch 6/30\n",
      "3890/3890 [==============================] - 0s 71us/sample - loss: 0.2932 - acc: 0.9134 - val_loss: 0.2460 - val_acc: 0.9304\n",
      "Epoch 7/30\n",
      "3890/3890 [==============================] - 0s 71us/sample - loss: 0.2604 - acc: 0.9244 - val_loss: 0.2058 - val_acc: 0.9410\n",
      "Epoch 8/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.2193 - acc: 0.9352 - val_loss: 0.1819 - val_acc: 0.9460\n",
      "Epoch 9/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.1842 - acc: 0.9488 - val_loss: 0.1641 - val_acc: 0.9505\n",
      "Epoch 10/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.1653 - acc: 0.9514 - val_loss: 0.1579 - val_acc: 0.9521\n",
      "Epoch 11/30\n",
      "3890/3890 [==============================] - 0s 71us/sample - loss: 0.1512 - acc: 0.9591 - val_loss: 0.1525 - val_acc: 0.9488\n",
      "Epoch 12/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.1444 - acc: 0.9519 - val_loss: 0.1474 - val_acc: 0.9499\n",
      "Epoch 13/30\n",
      "3890/3890 [==============================] - 0s 69us/sample - loss: 0.1364 - acc: 0.9578 - val_loss: 0.1472 - val_acc: 0.9544\n",
      "Epoch 14/30\n",
      "3890/3890 [==============================] - 0s 71us/sample - loss: 0.1290 - acc: 0.9632 - val_loss: 0.1363 - val_acc: 0.9549\n",
      "Epoch 15/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.1161 - acc: 0.9630 - val_loss: 0.1322 - val_acc: 0.9577\n",
      "Epoch 16/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.1133 - acc: 0.9645 - val_loss: 0.1321 - val_acc: 0.9555\n",
      "Epoch 17/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.1087 - acc: 0.9650 - val_loss: 0.1333 - val_acc: 0.9560\n",
      "Epoch 18/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.1036 - acc: 0.9715 - val_loss: 0.1259 - val_acc: 0.9572\n",
      "Epoch 19/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.0948 - acc: 0.9692 - val_loss: 0.1215 - val_acc: 0.9599\n",
      "Epoch 20/30\n",
      "3890/3890 [==============================] - 0s 70us/sample - loss: 0.0850 - acc: 0.9733 - val_loss: 0.1269 - val_acc: 0.9566\n",
      "Epoch 21/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.0871 - acc: 0.9730 - val_loss: 0.1202 - val_acc: 0.9610\n",
      "Epoch 22/30\n",
      "3890/3890 [==============================] - 0s 70us/sample - loss: 0.0866 - acc: 0.9720 - val_loss: 0.1280 - val_acc: 0.9549\n",
      "Epoch 23/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.0777 - acc: 0.9776 - val_loss: 0.1143 - val_acc: 0.9622\n",
      "Epoch 24/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.0758 - acc: 0.9761 - val_loss: 0.1229 - val_acc: 0.9566\n",
      "Epoch 25/30\n",
      "3890/3890 [==============================] - 0s 69us/sample - loss: 0.0685 - acc: 0.9799 - val_loss: 0.1175 - val_acc: 0.9577\n",
      "Epoch 26/30\n",
      "3890/3890 [==============================] - 0s 69us/sample - loss: 0.0605 - acc: 0.9799 - val_loss: 0.1232 - val_acc: 0.9616\n",
      "Epoch 27/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.0602 - acc: 0.9820 - val_loss: 0.1214 - val_acc: 0.9633\n",
      "Epoch 28/30\n",
      "3890/3890 [==============================] - 0s 71us/sample - loss: 0.0690 - acc: 0.9756 - val_loss: 0.1187 - val_acc: 0.9583\n",
      "Epoch 29/30\n",
      "3890/3890 [==============================] - 0s 66us/sample - loss: 0.0604 - acc: 0.9830 - val_loss: 0.1151 - val_acc: 0.9633\n",
      "Epoch 30/30\n",
      "3890/3890 [==============================] - 0s 69us/sample - loss: 0.0589 - acc: 0.9794 - val_loss: 0.1212 - val_acc: 0.9610\n",
      "oversampling test loss:  0.12122149555539012\n",
      "oversampling accuracy:  0.96104616\n",
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_135 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_90 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_91 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_45 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 3890 samples, validate on 1797 samples\n",
      "Epoch 1/30\n",
      "3776/3890 [============================>.] - ETA: 0s - loss: 2.0643 - acc: 0.3731"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3890/3890 [==============================] - 2s 452us/sample - loss: 2.0381 - acc: 0.3781 - val_loss: 1.9094 - val_acc: 0.6533\n",
      "Epoch 2/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.9241 - acc: 0.7175 - val_loss: 1.3930 - val_acc: 0.7880\n",
      "Epoch 3/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.5667 - acc: 0.8342 - val_loss: 0.8712 - val_acc: 0.8581\n",
      "Epoch 4/30\n",
      "3890/3890 [==============================] - 0s 71us/sample - loss: 0.4288 - acc: 0.8771 - val_loss: 0.5024 - val_acc: 0.9082\n",
      "Epoch 5/30\n",
      "3890/3890 [==============================] - 0s 69us/sample - loss: 0.3335 - acc: 0.9023 - val_loss: 0.3316 - val_acc: 0.9210\n",
      "Epoch 6/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.2909 - acc: 0.9103 - val_loss: 0.2461 - val_acc: 0.9371\n",
      "Epoch 7/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.2489 - acc: 0.9242 - val_loss: 0.2133 - val_acc: 0.9393\n",
      "Epoch 8/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.2126 - acc: 0.9406 - val_loss: 0.1888 - val_acc: 0.9466\n",
      "Epoch 9/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.1977 - acc: 0.9422 - val_loss: 0.1773 - val_acc: 0.9438\n",
      "Epoch 10/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.1853 - acc: 0.9460 - val_loss: 0.1723 - val_acc: 0.9499\n",
      "Epoch 11/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.1674 - acc: 0.9512 - val_loss: 0.1572 - val_acc: 0.9488\n",
      "Epoch 12/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.1457 - acc: 0.9540 - val_loss: 0.1566 - val_acc: 0.9521\n",
      "Epoch 13/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.1413 - acc: 0.9571 - val_loss: 0.1558 - val_acc: 0.9516\n",
      "Epoch 14/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.1368 - acc: 0.9563 - val_loss: 0.1393 - val_acc: 0.9549\n",
      "Epoch 15/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.1173 - acc: 0.9661 - val_loss: 0.1546 - val_acc: 0.9510\n",
      "Epoch 16/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.1162 - acc: 0.9635 - val_loss: 0.1549 - val_acc: 0.9494\n",
      "Epoch 17/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.1125 - acc: 0.9686 - val_loss: 0.1443 - val_acc: 0.9555\n",
      "Epoch 18/30\n",
      "3890/3890 [==============================] - 0s 69us/sample - loss: 0.0925 - acc: 0.9722 - val_loss: 0.1425 - val_acc: 0.9527\n",
      "Epoch 19/30\n",
      "3890/3890 [==============================] - 0s 69us/sample - loss: 0.0911 - acc: 0.9730 - val_loss: 0.1346 - val_acc: 0.9544\n",
      "Epoch 20/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.0935 - acc: 0.9704 - val_loss: 0.1425 - val_acc: 0.9516\n",
      "Epoch 21/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.0851 - acc: 0.9735 - val_loss: 0.1389 - val_acc: 0.9572\n",
      "Epoch 22/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.0868 - acc: 0.9712 - val_loss: 0.1298 - val_acc: 0.9572\n",
      "Epoch 23/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.0763 - acc: 0.9776 - val_loss: 0.1325 - val_acc: 0.9594\n",
      "Epoch 24/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.0744 - acc: 0.9769 - val_loss: 0.1360 - val_acc: 0.9544\n",
      "Epoch 25/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.0691 - acc: 0.9784 - val_loss: 0.1341 - val_acc: 0.9538\n",
      "Epoch 26/30\n",
      "3890/3890 [==============================] - 0s 71us/sample - loss: 0.0629 - acc: 0.9805 - val_loss: 0.1497 - val_acc: 0.9499\n",
      "Epoch 27/30\n",
      "3890/3890 [==============================] - 0s 69us/sample - loss: 0.0672 - acc: 0.9802 - val_loss: 0.1234 - val_acc: 0.9610\n",
      "Epoch 28/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.0581 - acc: 0.9851 - val_loss: 0.1363 - val_acc: 0.9577\n",
      "Epoch 29/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.0593 - acc: 0.9817 - val_loss: 0.1415 - val_acc: 0.9533\n",
      "Epoch 30/30\n",
      "3890/3890 [==============================] - 0s 70us/sample - loss: 0.0542 - acc: 0.9835 - val_loss: 0.1490 - val_acc: 0.9566\n",
      "oversampling test loss:  0.14900282707042217\n",
      "oversampling accuracy:  0.95659435\n",
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_138 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_92 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_93 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_46 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_140 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 3890 samples, validate on 1797 samples\n",
      "Epoch 1/30\n",
      "3890/3890 [==============================] - ETA: 0s - loss: 2.1669 - acc: 0.3447"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3890/3890 [==============================] - 2s 453us/sample - loss: 2.1669 - acc: 0.3447 - val_loss: 1.8918 - val_acc: 0.5531\n",
      "Epoch 2/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.8711 - acc: 0.7211 - val_loss: 1.3333 - val_acc: 0.8013\n",
      "Epoch 3/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.5563 - acc: 0.8429 - val_loss: 0.7969 - val_acc: 0.8859\n",
      "Epoch 4/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.4191 - acc: 0.8769 - val_loss: 0.4561 - val_acc: 0.9076\n",
      "Epoch 5/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.3257 - acc: 0.9057 - val_loss: 0.3061 - val_acc: 0.9215\n",
      "Epoch 6/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.2633 - acc: 0.9198 - val_loss: 0.2330 - val_acc: 0.9321\n",
      "Epoch 7/30\n",
      "3890/3890 [==============================] - 0s 69us/sample - loss: 0.2412 - acc: 0.9280 - val_loss: 0.2050 - val_acc: 0.9371\n",
      "Epoch 8/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.2074 - acc: 0.9422 - val_loss: 0.1791 - val_acc: 0.9488\n",
      "Epoch 9/30\n",
      "3890/3890 [==============================] - 0s 71us/sample - loss: 0.1945 - acc: 0.9434 - val_loss: 0.1670 - val_acc: 0.9466\n",
      "Epoch 10/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.1776 - acc: 0.9465 - val_loss: 0.1547 - val_acc: 0.9482\n",
      "Epoch 11/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.1582 - acc: 0.9519 - val_loss: 0.1485 - val_acc: 0.9499\n",
      "Epoch 12/30\n",
      "3890/3890 [==============================] - 0s 71us/sample - loss: 0.1420 - acc: 0.9589 - val_loss: 0.1545 - val_acc: 0.9494\n",
      "Epoch 13/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.1343 - acc: 0.9602 - val_loss: 0.1442 - val_acc: 0.9521\n",
      "Epoch 14/30\n",
      "3890/3890 [==============================] - 0s 69us/sample - loss: 0.1205 - acc: 0.9650 - val_loss: 0.1397 - val_acc: 0.9560\n",
      "Epoch 15/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.1202 - acc: 0.9653 - val_loss: 0.1380 - val_acc: 0.9544\n",
      "Epoch 16/30\n",
      "3890/3890 [==============================] - 0s 69us/sample - loss: 0.1031 - acc: 0.9717 - val_loss: 0.1284 - val_acc: 0.9549\n",
      "Epoch 17/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.1086 - acc: 0.9671 - val_loss: 0.1300 - val_acc: 0.9583\n",
      "Epoch 18/30\n",
      "3890/3890 [==============================] - 0s 71us/sample - loss: 0.0915 - acc: 0.9733 - val_loss: 0.1210 - val_acc: 0.9599\n",
      "Epoch 19/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.0969 - acc: 0.9717 - val_loss: 0.1146 - val_acc: 0.9622\n",
      "Epoch 20/30\n",
      "3890/3890 [==============================] - 0s 68us/sample - loss: 0.0782 - acc: 0.9779 - val_loss: 0.1194 - val_acc: 0.9577\n",
      "Epoch 21/30\n",
      "3890/3890 [==============================] - 0s 70us/sample - loss: 0.0813 - acc: 0.9730 - val_loss: 0.1194 - val_acc: 0.9594\n",
      "Epoch 22/30\n",
      "3890/3890 [==============================] - 0s 67us/sample - loss: 0.0757 - acc: 0.9763 - val_loss: 0.1127 - val_acc: 0.9594\n",
      "Epoch 23/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.0774 - acc: 0.9756 - val_loss: 0.1125 - val_acc: 0.9599\n",
      "Epoch 24/30\n",
      "3890/3890 [==============================] - 0s 70us/sample - loss: 0.0705 - acc: 0.9769 - val_loss: 0.1269 - val_acc: 0.9555\n",
      "Epoch 25/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.0681 - acc: 0.9797 - val_loss: 0.1171 - val_acc: 0.9605\n",
      "Epoch 26/30\n",
      "3890/3890 [==============================] - 0s 71us/sample - loss: 0.0654 - acc: 0.9805 - val_loss: 0.1164 - val_acc: 0.9610\n",
      "Epoch 27/30\n",
      "3890/3890 [==============================] - 0s 69us/sample - loss: 0.0598 - acc: 0.9807 - val_loss: 0.1237 - val_acc: 0.9577\n",
      "Epoch 28/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.0546 - acc: 0.9835 - val_loss: 0.1305 - val_acc: 0.9588\n",
      "Epoch 29/30\n",
      "3890/3890 [==============================] - 0s 71us/sample - loss: 0.0579 - acc: 0.9810 - val_loss: 0.1217 - val_acc: 0.9566\n",
      "Epoch 30/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.0585 - acc: 0.9817 - val_loss: 0.1169 - val_acc: 0.9616\n",
      "oversampling test loss:  0.1168944491266381\n",
      "oversampling accuracy:  0.9616027\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_141 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_94 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_47 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 3890 samples, validate on 1797 samples\n",
      "Epoch 1/30\n",
      "3890/3890 [==============================] - ETA: 0s - loss: 2.2474 - acc: 0.3254"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3890/3890 [==============================] - 2s 492us/sample - loss: 2.2474 - acc: 0.3254 - val_loss: 1.9565 - val_acc: 0.5771\n",
      "Epoch 2/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.9795 - acc: 0.6848 - val_loss: 1.4681 - val_acc: 0.7334\n",
      "Epoch 3/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.6198 - acc: 0.8087 - val_loss: 0.9104 - val_acc: 0.8370\n",
      "Epoch 4/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.4333 - acc: 0.8743 - val_loss: 0.5393 - val_acc: 0.8870\n",
      "Epoch 5/30\n",
      "3890/3890 [==============================] - 0s 69us/sample - loss: 0.3359 - acc: 0.9039 - val_loss: 0.3456 - val_acc: 0.9221\n",
      "Epoch 6/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.2965 - acc: 0.9139 - val_loss: 0.2550 - val_acc: 0.9316\n",
      "Epoch 7/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.2503 - acc: 0.9278 - val_loss: 0.2110 - val_acc: 0.9382\n",
      "Epoch 8/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.2222 - acc: 0.9368 - val_loss: 0.1903 - val_acc: 0.9449\n",
      "Epoch 9/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.1971 - acc: 0.9419 - val_loss: 0.1842 - val_acc: 0.9466\n",
      "Epoch 10/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.1760 - acc: 0.9501 - val_loss: 0.1705 - val_acc: 0.9460\n",
      "Epoch 11/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.1657 - acc: 0.9532 - val_loss: 0.1543 - val_acc: 0.9538\n",
      "Epoch 12/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.1424 - acc: 0.9584 - val_loss: 0.1560 - val_acc: 0.9516\n",
      "Epoch 13/30\n",
      "3890/3890 [==============================] - 0s 75us/sample - loss: 0.1476 - acc: 0.9573 - val_loss: 0.1482 - val_acc: 0.9533\n",
      "Epoch 14/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.1161 - acc: 0.9650 - val_loss: 0.1481 - val_acc: 0.9538\n",
      "Epoch 15/30\n",
      "3890/3890 [==============================] - 0s 75us/sample - loss: 0.1219 - acc: 0.9681 - val_loss: 0.1468 - val_acc: 0.9544\n",
      "Epoch 16/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.1252 - acc: 0.9635 - val_loss: 0.1407 - val_acc: 0.9566\n",
      "Epoch 17/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.1061 - acc: 0.9702 - val_loss: 0.1404 - val_acc: 0.9544\n",
      "Epoch 18/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.1019 - acc: 0.9684 - val_loss: 0.1373 - val_acc: 0.9583\n",
      "Epoch 19/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.0974 - acc: 0.9712 - val_loss: 0.1387 - val_acc: 0.9577\n",
      "Epoch 20/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.0962 - acc: 0.9722 - val_loss: 0.1381 - val_acc: 0.9549\n",
      "Epoch 21/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.0938 - acc: 0.9738 - val_loss: 0.1334 - val_acc: 0.9599\n",
      "Epoch 22/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.0920 - acc: 0.9715 - val_loss: 0.1304 - val_acc: 0.9583\n",
      "Epoch 23/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.0835 - acc: 0.9722 - val_loss: 0.1258 - val_acc: 0.9605\n",
      "Epoch 24/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.0773 - acc: 0.9769 - val_loss: 0.1291 - val_acc: 0.9644\n",
      "Epoch 25/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.0748 - acc: 0.9784 - val_loss: 0.1468 - val_acc: 0.9555\n",
      "Epoch 26/30\n",
      "3890/3890 [==============================] - 0s 75us/sample - loss: 0.0718 - acc: 0.9771 - val_loss: 0.1261 - val_acc: 0.9616\n",
      "Epoch 27/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.0671 - acc: 0.9805 - val_loss: 0.1335 - val_acc: 0.9599\n",
      "Epoch 28/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.0647 - acc: 0.9784 - val_loss: 0.1265 - val_acc: 0.9583\n",
      "Epoch 29/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.0648 - acc: 0.9807 - val_loss: 0.1171 - val_acc: 0.9633\n",
      "Epoch 30/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.0584 - acc: 0.9828 - val_loss: 0.1284 - val_acc: 0.9622\n",
      "oversampling test loss:  0.1283574877260823\n",
      "oversampling accuracy:  0.96215916\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_144 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_96 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_145 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_97 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_48 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_146 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 3890 samples, validate on 1797 samples\n",
      "Epoch 1/30\n",
      "3890/3890 [==============================] - ETA: 0s - loss: 2.1806 - acc: 0.3237"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3890/3890 [==============================] - 2s 495us/sample - loss: 2.1806 - acc: 0.3237 - val_loss: 1.9097 - val_acc: 0.5409\n",
      "Epoch 2/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 1.0073 - acc: 0.6704 - val_loss: 1.3730 - val_acc: 0.8086\n",
      "Epoch 3/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.6353 - acc: 0.8075 - val_loss: 0.8425 - val_acc: 0.8765\n",
      "Epoch 4/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.4736 - acc: 0.8602 - val_loss: 0.5037 - val_acc: 0.8982\n",
      "Epoch 5/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.3681 - acc: 0.8959 - val_loss: 0.3316 - val_acc: 0.9210\n",
      "Epoch 6/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.3083 - acc: 0.9105 - val_loss: 0.2730 - val_acc: 0.9149\n",
      "Epoch 7/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.2650 - acc: 0.9234 - val_loss: 0.2319 - val_acc: 0.9238\n",
      "Epoch 8/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.2354 - acc: 0.9383 - val_loss: 0.2109 - val_acc: 0.9371\n",
      "Epoch 9/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.2170 - acc: 0.9416 - val_loss: 0.1986 - val_acc: 0.9399\n",
      "Epoch 10/30\n",
      "3890/3890 [==============================] - 0s 71us/sample - loss: 0.1822 - acc: 0.9476 - val_loss: 0.1920 - val_acc: 0.9366\n",
      "Epoch 11/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.1758 - acc: 0.9499 - val_loss: 0.1780 - val_acc: 0.9449\n",
      "Epoch 12/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.1775 - acc: 0.9486 - val_loss: 0.1752 - val_acc: 0.9455\n",
      "Epoch 13/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.1480 - acc: 0.9596 - val_loss: 0.1726 - val_acc: 0.9477\n",
      "Epoch 14/30\n",
      "3890/3890 [==============================] - 0s 69us/sample - loss: 0.1472 - acc: 0.9599 - val_loss: 0.1645 - val_acc: 0.9466\n",
      "Epoch 15/30\n",
      "3890/3890 [==============================] - 0s 75us/sample - loss: 0.1306 - acc: 0.9630 - val_loss: 0.1612 - val_acc: 0.9488\n",
      "Epoch 16/30\n",
      "3890/3890 [==============================] - 0s 75us/sample - loss: 0.1312 - acc: 0.9599 - val_loss: 0.1544 - val_acc: 0.9549\n",
      "Epoch 17/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.1226 - acc: 0.9658 - val_loss: 0.1556 - val_acc: 0.9488\n",
      "Epoch 18/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.1083 - acc: 0.9692 - val_loss: 0.1596 - val_acc: 0.9527\n",
      "Epoch 19/30\n",
      "3890/3890 [==============================] - 0s 75us/sample - loss: 0.1048 - acc: 0.9697 - val_loss: 0.1548 - val_acc: 0.9527\n",
      "Epoch 20/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.1061 - acc: 0.9686 - val_loss: 0.1541 - val_acc: 0.9533\n",
      "Epoch 21/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.1153 - acc: 0.9643 - val_loss: 0.1586 - val_acc: 0.9510\n",
      "Epoch 22/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.0964 - acc: 0.9710 - val_loss: 0.1557 - val_acc: 0.9566\n",
      "Epoch 23/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.0906 - acc: 0.9761 - val_loss: 0.1574 - val_acc: 0.9544\n",
      "Epoch 24/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.0834 - acc: 0.9769 - val_loss: 0.1608 - val_acc: 0.9566\n",
      "Epoch 25/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.0801 - acc: 0.9763 - val_loss: 0.1567 - val_acc: 0.9521\n",
      "Epoch 26/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.0846 - acc: 0.9758 - val_loss: 0.1572 - val_acc: 0.9555\n",
      "Epoch 27/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.0772 - acc: 0.9746 - val_loss: 0.1644 - val_acc: 0.9510\n",
      "Epoch 28/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.0762 - acc: 0.9748 - val_loss: 0.1466 - val_acc: 0.9555\n",
      "Epoch 29/30\n",
      "3890/3890 [==============================] - 0s 76us/sample - loss: 0.0625 - acc: 0.9805 - val_loss: 0.1512 - val_acc: 0.9521\n",
      "Epoch 30/30\n",
      "3890/3890 [==============================] - 0s 76us/sample - loss: 0.0685 - acc: 0.9799 - val_loss: 0.1565 - val_acc: 0.9549\n",
      "oversampling test loss:  0.15650569880951318\n",
      "oversampling accuracy:  0.9549249\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_147 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_98 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_148 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_99 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_49 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 5,226\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 3890 samples, validate on 1797 samples\n",
      "Epoch 1/30\n",
      "3712/3890 [===========================>..] - ETA: 0s - loss: 1.8713 - acc: 0.4181"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3890/3890 [==============================] - 2s 490us/sample - loss: 1.8380 - acc: 0.4285 - val_loss: 1.7591 - val_acc: 0.6272\n",
      "Epoch 2/30\n",
      "3890/3890 [==============================] - 0s 75us/sample - loss: 0.7830 - acc: 0.7568 - val_loss: 1.1477 - val_acc: 0.8141\n",
      "Epoch 3/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.5013 - acc: 0.8494 - val_loss: 0.6593 - val_acc: 0.8887\n",
      "Epoch 4/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.3757 - acc: 0.8918 - val_loss: 0.3943 - val_acc: 0.9121\n",
      "Epoch 5/30\n",
      "3890/3890 [==============================] - 0s 76us/sample - loss: 0.3071 - acc: 0.9105 - val_loss: 0.2833 - val_acc: 0.9232\n",
      "Epoch 6/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.2593 - acc: 0.9270 - val_loss: 0.2256 - val_acc: 0.9321\n",
      "Epoch 7/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.2282 - acc: 0.9334 - val_loss: 0.1966 - val_acc: 0.9371\n",
      "Epoch 8/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.1978 - acc: 0.9452 - val_loss: 0.1830 - val_acc: 0.9410\n",
      "Epoch 9/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.1746 - acc: 0.9468 - val_loss: 0.1728 - val_acc: 0.9427\n",
      "Epoch 10/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.1606 - acc: 0.9527 - val_loss: 0.1712 - val_acc: 0.9455\n",
      "Epoch 11/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.1460 - acc: 0.9563 - val_loss: 0.1569 - val_acc: 0.9477\n",
      "Epoch 12/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.1332 - acc: 0.9586 - val_loss: 0.1597 - val_acc: 0.9494\n",
      "Epoch 13/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.1304 - acc: 0.9627 - val_loss: 0.1453 - val_acc: 0.9555\n",
      "Epoch 14/30\n",
      "3890/3890 [==============================] - 0s 75us/sample - loss: 0.1139 - acc: 0.9679 - val_loss: 0.1431 - val_acc: 0.9555\n",
      "Epoch 15/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.1049 - acc: 0.9692 - val_loss: 0.1466 - val_acc: 0.9527\n",
      "Epoch 16/30\n",
      "3890/3890 [==============================] - 0s 75us/sample - loss: 0.1040 - acc: 0.9692 - val_loss: 0.1422 - val_acc: 0.9538\n",
      "Epoch 17/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.1065 - acc: 0.9702 - val_loss: 0.1423 - val_acc: 0.9533\n",
      "Epoch 18/30\n",
      "3890/3890 [==============================] - 0s 75us/sample - loss: 0.0919 - acc: 0.9722 - val_loss: 0.1439 - val_acc: 0.9555\n",
      "Epoch 19/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.0875 - acc: 0.9720 - val_loss: 0.1337 - val_acc: 0.9577\n",
      "Epoch 20/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.0798 - acc: 0.9753 - val_loss: 0.1414 - val_acc: 0.9572\n",
      "Epoch 21/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.0729 - acc: 0.9776 - val_loss: 0.1431 - val_acc: 0.9555\n",
      "Epoch 22/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.0720 - acc: 0.9802 - val_loss: 0.1355 - val_acc: 0.9599\n",
      "Epoch 23/30\n",
      "3890/3890 [==============================] - 0s 75us/sample - loss: 0.0762 - acc: 0.9812 - val_loss: 0.1472 - val_acc: 0.9555\n",
      "Epoch 24/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.0708 - acc: 0.9776 - val_loss: 0.1370 - val_acc: 0.9622\n",
      "Epoch 25/30\n",
      "3890/3890 [==============================] - 0s 72us/sample - loss: 0.0683 - acc: 0.9771 - val_loss: 0.1318 - val_acc: 0.9572\n",
      "Epoch 26/30\n",
      "3890/3890 [==============================] - 0s 75us/sample - loss: 0.0729 - acc: 0.9771 - val_loss: 0.1388 - val_acc: 0.9588\n",
      "Epoch 27/30\n",
      "3890/3890 [==============================] - 0s 75us/sample - loss: 0.0607 - acc: 0.9820 - val_loss: 0.1270 - val_acc: 0.9633\n",
      "Epoch 28/30\n",
      "3890/3890 [==============================] - 0s 74us/sample - loss: 0.0634 - acc: 0.9799 - val_loss: 0.1241 - val_acc: 0.9616\n",
      "Epoch 29/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.0538 - acc: 0.9833 - val_loss: 0.1260 - val_acc: 0.9633\n",
      "Epoch 30/30\n",
      "3890/3890 [==============================] - 0s 73us/sample - loss: 0.0516 - acc: 0.9864 - val_loss: 0.1235 - val_acc: 0.9605\n",
      "oversampling test loss:  0.12350480428825064\n",
      "oversampling accuracy:  0.9604897\n"
     ]
    }
   ],
   "source": [
    "oversampling_list =[]\n",
    "for i in range(10):\n",
    "    bl_model = build_model()\n",
    "    batch_size=64\n",
    "    epochs=30\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_train, y_train1 = ros.fit_resample(train_images, train_y)\n",
    "    \n",
    "    bl_history = bl_model.fit(X_train, y_train1, batch_size=batch_size,\n",
    "                        epochs=epochs, validation_data=(test_images, test_y))\n",
    "\n",
    "    bl_score = bl_model.evaluate(test_images, test_y, verbose=0)\n",
    "    print('oversampling test loss: ', bl_score[0])\n",
    "    print('oversampling accuracy: ', bl_score[1] )\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    y_pred_oh = bl_model.predict(test_images)\n",
    "    y_pred_aug = y_pred_oh.argmax(axis=-1)\n",
    "    oversampling_list.append(classification_report(y_test, y_pred_aug, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09ee49fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of runs: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.993775</td>\n",
       "      <td>0.983146</td>\n",
       "      <td>0.988415</td>\n",
       "      <td>178.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.959529</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.950441</td>\n",
       "      <td>182.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.972196</td>\n",
       "      <td>0.977401</td>\n",
       "      <td>0.974657</td>\n",
       "      <td>177.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.983297</td>\n",
       "      <td>0.927869</td>\n",
       "      <td>0.954661</td>\n",
       "      <td>183.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.966792</td>\n",
       "      <td>0.980663</td>\n",
       "      <td>0.973658</td>\n",
       "      <td>181.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.923556</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.952466</td>\n",
       "      <td>182.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.986776</td>\n",
       "      <td>0.986740</td>\n",
       "      <td>0.986745</td>\n",
       "      <td>181.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.988734</td>\n",
       "      <td>0.923464</td>\n",
       "      <td>0.954921</td>\n",
       "      <td>179.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.918654</td>\n",
       "      <td>0.912069</td>\n",
       "      <td>0.914913</td>\n",
       "      <td>174.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.896027</td>\n",
       "      <td>0.958889</td>\n",
       "      <td>0.926268</td>\n",
       "      <td>180.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.957707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.958933</td>\n",
       "      <td>0.957607</td>\n",
       "      <td>0.957714</td>\n",
       "      <td>1797.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.959012</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.957802</td>\n",
       "      <td>1797.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.993775  0.983146  0.988415   178.000000\n",
       "1              0.959529  0.942308  0.950441   182.000000\n",
       "2              0.972196  0.977401  0.974657   177.000000\n",
       "3              0.983297  0.927869  0.954661   183.000000\n",
       "4              0.966792  0.980663  0.973658   181.000000\n",
       "5              0.923556  0.983516  0.952466   182.000000\n",
       "6              0.986776  0.986740  0.986745   181.000000\n",
       "7              0.988734  0.923464  0.954921   179.000000\n",
       "8              0.918654  0.912069  0.914913   174.000000\n",
       "9              0.896027  0.958889  0.926268   180.000000\n",
       "accuracy       0.957707  0.957707  0.957707     0.957707\n",
       "macro avg      0.958933  0.957607  0.957714  1797.000000\n",
       "weighted avg   0.959012  0.957707  0.957802  1797.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_process_results(oversampling_list, 'results_csv/opt_oversampling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd40437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4726aa8c3a011139ee9eae324612f94ded5d9a6c6a3363e2331a9b86c3055c02"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
