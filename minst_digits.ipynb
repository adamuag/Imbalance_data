{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f719fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83617bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images,test_labels) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "402b3004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = [np.reshape(x, -1) for x in train_images]\n",
    "test_images= [np.reshape(x, -1) for x in test_images]\n",
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0643ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def group_all_labels(data, num=100, minor=[]):\n",
    "    # this function is to limit the number of labels that are used\n",
    "    # it returns the indexes according the labels\n",
    "    # data is an array of labels\n",
    "    '''\n",
    "\n",
    "    :param data: array of labels\n",
    "    :param num: number required\n",
    "    :param minor: list of minority indexes\n",
    "    :return: array of labels indexes\n",
    "    '''\n",
    "\n",
    "    labels = np.unique(data)\n",
    "    co_l = []\n",
    "    min_col =[]\n",
    "    if not minor:\n",
    "        for l in labels:\n",
    "            el_l = np.where(np.array(data) == l)\n",
    "            co_l.append(el_l[0])\n",
    "\n",
    "    else:\n",
    "        for l in labels:\n",
    "            if l in minor:\n",
    "                el_l = np.where(np.array(data) == l)\n",
    "                co_l.append((el_l[0])[:num])\n",
    "                min_col.append((el_l[0])[:num])\n",
    "            else:\n",
    "                el_l = np.where(np.array(data) == l)\n",
    "                co_l.append(el_l[0])\n",
    "    return co_l, min_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e12951a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0 : number of samples : 250\n",
      "class 1 : number of samples : 250\n",
      "class 2 : number of samples : 5958\n",
      "class 3 : number of samples : 6131\n",
      "class 4 : number of samples : 5842\n",
      "class 5 : number of samples : 5421\n",
      "class 6 : number of samples : 5918\n",
      "class 7 : number of samples : 6265\n",
      "class 8 : number of samples : 5851\n",
      "class 9 : number of samples : 5949\n"
     ]
    }
   ],
   "source": [
    "grouped_labels, min_label = group_all_labels(train_labels, 250, [0, 1])\n",
    "gr_data = []\n",
    "gr_labels = [] \n",
    "for index, q in enumerate(grouped_labels):\n",
    "    print('class {} : number of samples : {}'.format(index,len(q)))\n",
    "    for r in q:\n",
    "        gr_data.append(train_images[r])\n",
    "        gr_labels.append(train_labels[r])\n",
    "\n",
    "gr_min_data = []\n",
    "gr_min_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5861d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority data only\n",
      "class 0 : number of samples : 250\n",
      "class 1 : number of samples : 250\n"
     ]
    }
   ],
   "source": [
    "print('minority data only')\n",
    "gr_min_data = []\n",
    "gr_min_labels = []\n",
    "for index, q in enumerate(min_label):\n",
    "    print('class {} : number of samples : {}'.format(index,len(q)))\n",
    "    for r in q:\n",
    "        gr_min_data.append(train_images[r])\n",
    "        gr_min_labels.append(train_labels[r])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45fad581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f41c73fb4f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_x = np.array(gr_data)\n",
    "train_x = (train_x.astype(np.float32) / 255.0) \n",
    "test_x =  np.array(test_images)\n",
    "test_images = (test_x.astype(np.float32)/255.0)\n",
    "\n",
    "train_min_x = np.array(gr_min_data)\n",
    "train_min_x = (train_min_x.astype(np.float32) / 255.0) \n",
    "\n",
    "train_y = tf.keras.utils.to_categorical(gr_labels, num_classes=10, dtype='float32')\n",
    "test_y = tf.keras.utils.to_categorical(test_labels, num_classes=10, dtype='float32')\n",
    "train_min_y = tf.keras.utils.to_categorical(gr_min_labels, num_classes=2, dtype='float32')\n",
    "\n",
    "print(test_y[0])\n",
    "plt.imshow(np.reshape(test_x[0],(28,28)), cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5635c9da",
   "metadata": {},
   "source": [
    "#  VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1039ac98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-02 12:07:13.012427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 12:07:13.018447: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2021-11-02 12:07:13.019177: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-11-02 12:07:13.019747: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1  as tf\n",
    "tf.disable_v2_behavior()\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "mb_size = 64\n",
    "z_dim = 100\n",
    "X_dim = 784\n",
    "y_dim = 10\n",
    "h_dim = 128\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "def plot(samples, sz, shape):\n",
    "    fig = plt.figure(figsize=(sz, sz))\n",
    "    gs = gridspec.GridSpec(sz, sz)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(shape, shape), cmap='Greys_r')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random.normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "\n",
    "# Q(z|X) \n",
    "\n",
    "X = tf.keras.Input(shape=(X_dim,))\n",
    "c = tf.keras.Input(shape=(y_dim,))\n",
    "z = tf.keras.Input(shape=(z_dim,))\n",
    "\n",
    "Q_W1 = tf.Variable(xavier_init([X_dim + y_dim, h_dim]))\n",
    "Q_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "Q_W2_mu = tf.Variable(xavier_init([h_dim, z_dim]))\n",
    "Q_b2_mu = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "Q_W2_sigma = tf.Variable(xavier_init([h_dim, z_dim]))\n",
    "Q_b2_sigma = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "\n",
    "def Q(X, c):\n",
    "    inputs = tf.concat(axis=1, values=[X, c])\n",
    "    h = tf.nn.relu(tf.matmul(inputs, Q_W1) + Q_b1)\n",
    "    z_mu = tf.matmul(h, Q_W2_mu) + Q_b2_mu\n",
    "    z_logvar = tf.matmul(h, Q_W2_sigma) + Q_b2_sigma\n",
    "    return z_mu, z_logvar\n",
    "\n",
    "\n",
    "def sample_z(mu, log_var):\n",
    "    eps = tf.random.normal(shape=tf.shape(mu))\n",
    "    return mu + tf.exp(log_var / 2) * eps\n",
    "\n",
    "\n",
    "# P(X|z)\n",
    "\n",
    "P_W1 = tf.Variable(xavier_init([z_dim + y_dim, h_dim]))\n",
    "P_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "P_W2 = tf.Variable(xavier_init([h_dim, X_dim]))\n",
    "P_b2 = tf.Variable(tf.zeros(shape=[X_dim]))\n",
    "\n",
    "\n",
    "def P(z, c):\n",
    "    inputs = tf.concat(axis=1, values=[z, c])\n",
    "    h = tf.nn.relu(tf.matmul(inputs, P_W1) + P_b1)\n",
    "    logits = tf.matmul(h, P_W2) + P_b2\n",
    "    prob = tf.nn.sigmoid(logits)\n",
    "    return prob, logits\n",
    "\n",
    "z_mu, z_logvar = Q(X, c)\n",
    "z_sample = sample_z(z_mu, z_logvar)\n",
    "_, logits = P(z_sample, c)\n",
    "\n",
    "# Sampling from random z\n",
    "X_samples, _ = P(z, c)\n",
    "\n",
    "# E[log P(X|z)]\n",
    "recon_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=X), 1)\n",
    "kl_loss = 0.5 * tf.reduce_sum(tf.exp(z_logvar) + z_mu**2 - 1. - z_logvar, 1)\n",
    "\n",
    "# VAE loss\n",
    "vae_loss = tf.reduce_mean(recon_loss + kl_loss)\n",
    "\n",
    "# gradient step\n",
    "solver = tf.compat.v1.train.AdamOptimizer().minimize(vae_loss)\n",
    "sess = tf.compat.v1.Session ()\n",
    "sess.run(\n",
    "tf.compat.v1.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b6f9797",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for it in range(100000):\n",
    "    ind = np.random.choice(train_x.shape[0], mb_size)\n",
    "    X_mb = np.array(train_x[ind])\n",
    "    y_mb = np.array(train_y[ind])\n",
    "    \n",
    "    _, loss = sess.run([solver, vae_loss], feed_dict={X: X_mb, c: y_mb})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b35943d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples():\n",
    "    samples = []\n",
    "    gen_labels =[]\n",
    "    for r in range(62):\n",
    "        for index in range(2):\n",
    "            gen_labels = gen_labels + [index]*mb_size\n",
    "            y = np.zeros([mb_size, y_dim])\n",
    "            y[range(mb_size), index] = 1\n",
    "            samples.extend(sess.run(X_samples,\n",
    "                                   feed_dict={z: np.random.randn(mb_size, z_dim), c: y}))\n",
    "\n",
    "    gen_samples = np.array(samples)\n",
    "    gen_labels = np.array(gen_labels)\n",
    "    print(gen_samples.shape)\n",
    "    print(gen_labels.shape)\n",
    "    print(gen_labels[0])\n",
    "    \n",
    "    return gen_samples, gen_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dcfc5e",
   "metadata": {},
   "source": [
    "# Visualize generated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90a9ed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f41c4eca7f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "aug_data, aug_labels = generate_samples()\n",
    "plt.imshow(np.reshape(aug_data[0],(28,28)), cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe354d",
   "metadata": {},
   "source": [
    "# MLP Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6c6de50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Input, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "def build_model(input_shape=(784,), num_classes=10):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_shape: shape of input_data\n",
    "    :param num_classes: number of classes\n",
    "    :return: keras.model.sequential compiled with categorical cross-entropy loss\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6037773",
   "metadata": {},
   "source": [
    "# Base line classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "968d0e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_results(b_list, filename='default.csv'):\n",
    "    total_df = pd.DataFrame(b_list[0]).transpose()\n",
    "    print('number of runs: {}'.format(len(b_list)))\n",
    "    for r_dict in b_list[1:]:\n",
    "        temp = pd.DataFrame(r_dict).transpose()\n",
    "        total_df = total_df.add(temp)\n",
    "        \n",
    "    average_pd = total_df/10.0\n",
    "    average_pd.to_csv(filename, sep=',')\n",
    "    \n",
    "    return average_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4588a026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 47835 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "47744/47835 [============================>.] - ETA: 0s - loss: 0.5943 - acc: 0.8198"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47835/47835 [==============================] - 2s 48us/sample - loss: 0.5939 - acc: 0.8199 - val_loss: 0.4925 - val_acc: 0.8215\n",
      "Epoch 2/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.2949 - acc: 0.9115 - val_loss: 0.2884 - val_acc: 0.9131\n",
      "Epoch 3/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.2326 - acc: 0.9314 - val_loss: 0.2380 - val_acc: 0.9296\n",
      "Epoch 4/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.2025 - acc: 0.9402 - val_loss: 0.2376 - val_acc: 0.9333\n",
      "Epoch 5/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.1773 - acc: 0.9484 - val_loss: 0.1909 - val_acc: 0.9421\n",
      "Epoch 6/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.1639 - acc: 0.9517 - val_loss: 0.1762 - val_acc: 0.9499\n",
      "Epoch 7/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.1516 - acc: 0.9558 - val_loss: 0.1941 - val_acc: 0.9434\n",
      "Epoch 8/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.1425 - acc: 0.9573 - val_loss: 0.1716 - val_acc: 0.9494\n",
      "Epoch 9/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.1366 - acc: 0.9604 - val_loss: 0.1952 - val_acc: 0.9444\n",
      "Epoch 10/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.1254 - acc: 0.9620 - val_loss: 0.1850 - val_acc: 0.9450\n",
      "Epoch 11/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.1169 - acc: 0.9645 - val_loss: 0.1878 - val_acc: 0.9461\n",
      "Epoch 12/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.1132 - acc: 0.9654 - val_loss: 0.1689 - val_acc: 0.9533\n",
      "Epoch 13/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.1139 - acc: 0.9656 - val_loss: 0.1524 - val_acc: 0.9578\n",
      "Epoch 14/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.1058 - acc: 0.9678 - val_loss: 0.1919 - val_acc: 0.9441\n",
      "Epoch 15/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.1046 - acc: 0.9680 - val_loss: 0.1648 - val_acc: 0.9504\n",
      "Epoch 16/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.0996 - acc: 0.9692 - val_loss: 0.1588 - val_acc: 0.9557\n",
      "Epoch 17/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0985 - acc: 0.9689 - val_loss: 0.1686 - val_acc: 0.9531\n",
      "Epoch 18/30\n",
      "47835/47835 [==============================] - 2s 45us/sample - loss: 0.0965 - acc: 0.9697 - val_loss: 0.1608 - val_acc: 0.9549\n",
      "Epoch 19/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0923 - acc: 0.9713 - val_loss: 0.1638 - val_acc: 0.9535\n",
      "Epoch 20/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.0890 - acc: 0.9719 - val_loss: 0.1623 - val_acc: 0.9572\n",
      "Epoch 21/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0866 - acc: 0.9734 - val_loss: 0.1730 - val_acc: 0.9491\n",
      "Epoch 22/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.0871 - acc: 0.9738 - val_loss: 0.1888 - val_acc: 0.9495\n",
      "Epoch 23/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0850 - acc: 0.9738 - val_loss: 0.1548 - val_acc: 0.9582\n",
      "Epoch 24/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0845 - acc: 0.9732 - val_loss: 0.1955 - val_acc: 0.9488\n",
      "Epoch 25/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0835 - acc: 0.9742 - val_loss: 0.1809 - val_acc: 0.9517\n",
      "Epoch 26/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0794 - acc: 0.9757 - val_loss: 0.1737 - val_acc: 0.9540\n",
      "Epoch 27/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.0762 - acc: 0.9760 - val_loss: 0.1832 - val_acc: 0.9546\n",
      "Epoch 28/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.0802 - acc: 0.9748 - val_loss: 0.1644 - val_acc: 0.9594\n",
      "Epoch 29/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.0763 - acc: 0.9762 - val_loss: 0.1830 - val_acc: 0.9529\n",
      "Epoch 30/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.0730 - acc: 0.9768 - val_loss: 0.1787 - val_acc: 0.9560\n",
      "baseline test loss:  0.17866745745991355\n",
      "baseline test accuracy:  0.956\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_33 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 47835 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "46720/47835 [============================>.] - ETA: 0s - loss: 0.5943 - acc: 0.8166"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47835/47835 [==============================] - 2s 46us/sample - loss: 0.5886 - acc: 0.8185 - val_loss: 0.5257 - val_acc: 0.8175\n",
      "Epoch 2/30\n",
      "47835/47835 [==============================] - 2s 36us/sample - loss: 0.2875 - acc: 0.9154 - val_loss: 0.3038 - val_acc: 0.9073\n",
      "Epoch 3/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.2349 - acc: 0.9314 - val_loss: 0.2420 - val_acc: 0.9266\n",
      "Epoch 4/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.2033 - acc: 0.9391 - val_loss: 0.2170 - val_acc: 0.9341\n",
      "Epoch 5/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.1787 - acc: 0.9461 - val_loss: 0.2338 - val_acc: 0.9259\n",
      "Epoch 6/30\n",
      "47835/47835 [==============================] - 2s 36us/sample - loss: 0.1625 - acc: 0.9519 - val_loss: 0.1888 - val_acc: 0.9441\n",
      "Epoch 7/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.1533 - acc: 0.9542 - val_loss: 0.1667 - val_acc: 0.9477\n",
      "Epoch 8/30\n",
      "47835/47835 [==============================] - 2s 36us/sample - loss: 0.1417 - acc: 0.9582 - val_loss: 0.1848 - val_acc: 0.9442\n",
      "Epoch 9/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.1325 - acc: 0.9602 - val_loss: 0.1980 - val_acc: 0.9393\n",
      "Epoch 10/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.1269 - acc: 0.9614 - val_loss: 0.1655 - val_acc: 0.9524\n",
      "Epoch 11/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.1204 - acc: 0.9636 - val_loss: 0.1615 - val_acc: 0.9527\n",
      "Epoch 12/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.1146 - acc: 0.9660 - val_loss: 0.1632 - val_acc: 0.9511\n",
      "Epoch 13/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.1104 - acc: 0.9664 - val_loss: 0.2062 - val_acc: 0.9372\n",
      "Epoch 14/30\n",
      "47835/47835 [==============================] - 2s 36us/sample - loss: 0.1078 - acc: 0.9658 - val_loss: 0.1809 - val_acc: 0.9471\n",
      "Epoch 15/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.1052 - acc: 0.9680 - val_loss: 0.1770 - val_acc: 0.9477\n",
      "Epoch 16/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.0996 - acc: 0.9691 - val_loss: 0.1738 - val_acc: 0.9485\n",
      "Epoch 17/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0973 - acc: 0.9705 - val_loss: 0.1844 - val_acc: 0.9492\n",
      "Epoch 18/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0964 - acc: 0.9701 - val_loss: 0.1588 - val_acc: 0.9541\n",
      "Epoch 19/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.0929 - acc: 0.9712 - val_loss: 0.1464 - val_acc: 0.9596\n",
      "Epoch 20/30\n",
      "47835/47835 [==============================] - 2s 44us/sample - loss: 0.0911 - acc: 0.9716 - val_loss: 0.1451 - val_acc: 0.9624\n",
      "Epoch 21/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.0868 - acc: 0.9732 - val_loss: 0.1687 - val_acc: 0.9527\n",
      "Epoch 22/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0841 - acc: 0.9727 - val_loss: 0.1715 - val_acc: 0.9518\n",
      "Epoch 23/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0868 - acc: 0.9731 - val_loss: 0.1480 - val_acc: 0.9582\n",
      "Epoch 24/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0858 - acc: 0.9734 - val_loss: 0.1912 - val_acc: 0.9482\n",
      "Epoch 25/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0813 - acc: 0.9751 - val_loss: 0.1622 - val_acc: 0.9527\n",
      "Epoch 26/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0776 - acc: 0.9756 - val_loss: 0.1618 - val_acc: 0.9570\n",
      "Epoch 27/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0786 - acc: 0.9750 - val_loss: 0.1841 - val_acc: 0.9491\n",
      "Epoch 28/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0808 - acc: 0.9744 - val_loss: 0.1640 - val_acc: 0.9533\n",
      "Epoch 29/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0772 - acc: 0.9753 - val_loss: 0.1795 - val_acc: 0.9519\n",
      "Epoch 30/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0722 - acc: 0.9774 - val_loss: 0.1674 - val_acc: 0.9538\n",
      "baseline test loss:  0.16743913610689343\n",
      "baseline test accuracy:  0.9538\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 47835 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "47360/47835 [============================>.] - ETA: 0s - loss: 0.5775 - acc: 0.8253"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47835/47835 [==============================] - 2s 49us/sample - loss: 0.5749 - acc: 0.8262 - val_loss: 0.4702 - val_acc: 0.8395\n",
      "Epoch 2/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.2765 - acc: 0.9202 - val_loss: 0.3063 - val_acc: 0.9082\n",
      "Epoch 3/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.2176 - acc: 0.9362 - val_loss: 0.2602 - val_acc: 0.9219\n",
      "Epoch 4/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.1857 - acc: 0.9461 - val_loss: 0.2826 - val_acc: 0.9075\n",
      "Epoch 5/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.1669 - acc: 0.9503 - val_loss: 0.2068 - val_acc: 0.9393\n",
      "Epoch 6/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.1520 - acc: 0.9554 - val_loss: 0.1974 - val_acc: 0.9399\n",
      "Epoch 7/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.1412 - acc: 0.9586 - val_loss: 0.1676 - val_acc: 0.9502\n",
      "Epoch 8/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.1316 - acc: 0.9609 - val_loss: 0.1864 - val_acc: 0.9449\n",
      "Epoch 9/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.1282 - acc: 0.9617 - val_loss: 0.1604 - val_acc: 0.9528\n",
      "Epoch 10/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.1242 - acc: 0.9622 - val_loss: 0.1658 - val_acc: 0.9499\n",
      "Epoch 11/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.1121 - acc: 0.9668 - val_loss: 0.1798 - val_acc: 0.9491\n",
      "Epoch 12/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.1104 - acc: 0.9666 - val_loss: 0.1720 - val_acc: 0.9498\n",
      "Epoch 13/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.1060 - acc: 0.9680 - val_loss: 0.1635 - val_acc: 0.9516\n",
      "Epoch 14/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.1044 - acc: 0.9685 - val_loss: 0.1431 - val_acc: 0.9568\n",
      "Epoch 15/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0975 - acc: 0.9708 - val_loss: 0.1761 - val_acc: 0.9487\n",
      "Epoch 16/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0991 - acc: 0.9692 - val_loss: 0.1622 - val_acc: 0.9538\n",
      "Epoch 17/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0929 - acc: 0.9709 - val_loss: 0.1836 - val_acc: 0.9472\n",
      "Epoch 18/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0938 - acc: 0.9705 - val_loss: 0.1535 - val_acc: 0.9553\n",
      "Epoch 19/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0912 - acc: 0.9721 - val_loss: 0.1602 - val_acc: 0.9541\n",
      "Epoch 20/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0875 - acc: 0.9729 - val_loss: 0.1748 - val_acc: 0.9533\n",
      "Epoch 21/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0842 - acc: 0.9740 - val_loss: 0.1511 - val_acc: 0.9574\n",
      "Epoch 22/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0841 - acc: 0.9745 - val_loss: 0.1563 - val_acc: 0.9571\n",
      "Epoch 23/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0843 - acc: 0.9737 - val_loss: 0.1881 - val_acc: 0.9480\n",
      "Epoch 24/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0827 - acc: 0.9738 - val_loss: 0.1570 - val_acc: 0.9559\n",
      "Epoch 25/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0767 - acc: 0.9758 - val_loss: 0.1800 - val_acc: 0.9500\n",
      "Epoch 26/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0782 - acc: 0.9756 - val_loss: 0.1622 - val_acc: 0.9548\n",
      "Epoch 27/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0760 - acc: 0.9762 - val_loss: 0.1622 - val_acc: 0.9543\n",
      "Epoch 28/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0761 - acc: 0.9764 - val_loss: 0.1674 - val_acc: 0.9556\n",
      "Epoch 29/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0735 - acc: 0.9765 - val_loss: 0.1908 - val_acc: 0.9512\n",
      "Epoch 30/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0692 - acc: 0.9774 - val_loss: 0.1682 - val_acc: 0.9553\n",
      "baseline test loss:  0.16818593181800098\n",
      "baseline test accuracy:  0.9553\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_39 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 47835 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "46720/47835 [============================>.] - ETA: 0s - loss: 0.6163 - acc: 0.8124"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47835/47835 [==============================] - 2s 49us/sample - loss: 0.6110 - acc: 0.8141 - val_loss: 0.4612 - val_acc: 0.8432\n",
      "Epoch 2/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.2890 - acc: 0.9159 - val_loss: 0.3464 - val_acc: 0.8910\n",
      "Epoch 3/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.2254 - acc: 0.9337 - val_loss: 0.2884 - val_acc: 0.9125\n",
      "Epoch 4/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.1961 - acc: 0.9422 - val_loss: 0.2594 - val_acc: 0.9237\n",
      "Epoch 5/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.1729 - acc: 0.9481 - val_loss: 0.2274 - val_acc: 0.9280\n",
      "Epoch 6/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.1539 - acc: 0.9544 - val_loss: 0.1877 - val_acc: 0.9432\n",
      "Epoch 7/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.1455 - acc: 0.9558 - val_loss: 0.1872 - val_acc: 0.9437\n",
      "Epoch 8/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.1366 - acc: 0.9601 - val_loss: 0.1707 - val_acc: 0.9498\n",
      "Epoch 9/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.1281 - acc: 0.9630 - val_loss: 0.1880 - val_acc: 0.9446\n",
      "Epoch 10/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.1195 - acc: 0.9642 - val_loss: 0.1477 - val_acc: 0.9572\n",
      "Epoch 11/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.1121 - acc: 0.9660 - val_loss: 0.1826 - val_acc: 0.9429\n",
      "Epoch 12/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.1080 - acc: 0.9670 - val_loss: 0.1882 - val_acc: 0.9456\n",
      "Epoch 13/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.1054 - acc: 0.9680 - val_loss: 0.1692 - val_acc: 0.9494\n",
      "Epoch 14/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.1019 - acc: 0.9694 - val_loss: 0.1554 - val_acc: 0.9558\n",
      "Epoch 15/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0962 - acc: 0.9705 - val_loss: 0.1553 - val_acc: 0.9578\n",
      "Epoch 16/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0956 - acc: 0.9707 - val_loss: 0.1667 - val_acc: 0.9529\n",
      "Epoch 17/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0932 - acc: 0.9712 - val_loss: 0.1846 - val_acc: 0.9480\n",
      "Epoch 18/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0870 - acc: 0.9730 - val_loss: 0.1716 - val_acc: 0.9535\n",
      "Epoch 19/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0869 - acc: 0.9733 - val_loss: 0.1701 - val_acc: 0.9552\n",
      "Epoch 20/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0857 - acc: 0.9732 - val_loss: 0.1557 - val_acc: 0.9570\n",
      "Epoch 21/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0841 - acc: 0.9742 - val_loss: 0.1795 - val_acc: 0.9498\n",
      "Epoch 22/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0776 - acc: 0.9761 - val_loss: 0.1560 - val_acc: 0.9574\n",
      "Epoch 23/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0788 - acc: 0.9753 - val_loss: 0.1424 - val_acc: 0.9632\n",
      "Epoch 24/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0771 - acc: 0.9756 - val_loss: 0.1646 - val_acc: 0.9583\n",
      "Epoch 25/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0771 - acc: 0.9751 - val_loss: 0.1517 - val_acc: 0.9578\n",
      "Epoch 26/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.0743 - acc: 0.9765 - val_loss: 0.1650 - val_acc: 0.9561\n",
      "Epoch 27/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0719 - acc: 0.9773 - val_loss: 0.1953 - val_acc: 0.9486\n",
      "Epoch 28/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0716 - acc: 0.9770 - val_loss: 0.1600 - val_acc: 0.9565\n",
      "Epoch 29/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0687 - acc: 0.9778 - val_loss: 0.1844 - val_acc: 0.9490\n",
      "Epoch 30/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0708 - acc: 0.9787 - val_loss: 0.1773 - val_acc: 0.9533\n",
      "baseline test loss:  0.1773486872026697\n",
      "baseline test accuracy:  0.9533\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_42 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 47835 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "46784/47835 [============================>.] - ETA: 0s - loss: 0.5789 - acc: 0.8230"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47835/47835 [==============================] - 3s 56us/sample - loss: 0.5734 - acc: 0.8247 - val_loss: 0.5097 - val_acc: 0.8112\n",
      "Epoch 2/30\n",
      "47835/47835 [==============================] - 2s 45us/sample - loss: 0.2861 - acc: 0.9159 - val_loss: 0.2814 - val_acc: 0.9114\n",
      "Epoch 3/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.2205 - acc: 0.9336 - val_loss: 0.2452 - val_acc: 0.9265\n",
      "Epoch 4/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.1950 - acc: 0.9414 - val_loss: 0.2532 - val_acc: 0.9225\n",
      "Epoch 5/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.1721 - acc: 0.9484 - val_loss: 0.2093 - val_acc: 0.9405\n",
      "Epoch 6/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.1572 - acc: 0.9538 - val_loss: 0.1944 - val_acc: 0.9419\n",
      "Epoch 7/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.1459 - acc: 0.9562 - val_loss: 0.1789 - val_acc: 0.9475\n",
      "Epoch 8/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.1388 - acc: 0.9588 - val_loss: 0.1968 - val_acc: 0.9418\n",
      "Epoch 9/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.1285 - acc: 0.9615 - val_loss: 0.1851 - val_acc: 0.9446\n",
      "Epoch 10/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.1225 - acc: 0.9632 - val_loss: 0.2069 - val_acc: 0.9398\n",
      "Epoch 11/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.1186 - acc: 0.9639 - val_loss: 0.1703 - val_acc: 0.9523\n",
      "Epoch 12/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.1122 - acc: 0.9656 - val_loss: 0.2010 - val_acc: 0.9433\n",
      "Epoch 13/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.1084 - acc: 0.9680 - val_loss: 0.1638 - val_acc: 0.9528\n",
      "Epoch 14/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.1032 - acc: 0.9684 - val_loss: 0.2044 - val_acc: 0.9406\n",
      "Epoch 15/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0999 - acc: 0.9703 - val_loss: 0.1937 - val_acc: 0.9445\n",
      "Epoch 16/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0980 - acc: 0.9703 - val_loss: 0.1883 - val_acc: 0.9476\n",
      "Epoch 17/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0935 - acc: 0.9722 - val_loss: 0.1726 - val_acc: 0.9501\n",
      "Epoch 18/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0934 - acc: 0.9713 - val_loss: 0.2151 - val_acc: 0.9398\n",
      "Epoch 19/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0904 - acc: 0.9728 - val_loss: 0.1598 - val_acc: 0.9567\n",
      "Epoch 20/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0903 - acc: 0.9729 - val_loss: 0.1714 - val_acc: 0.9520\n",
      "Epoch 21/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0866 - acc: 0.9732 - val_loss: 0.1562 - val_acc: 0.9585\n",
      "Epoch 22/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0837 - acc: 0.9731 - val_loss: 0.1660 - val_acc: 0.9559\n",
      "Epoch 23/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0815 - acc: 0.9747 - val_loss: 0.1824 - val_acc: 0.9510\n",
      "Epoch 24/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0801 - acc: 0.9749 - val_loss: 0.1801 - val_acc: 0.9528\n",
      "Epoch 25/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0804 - acc: 0.9750 - val_loss: 0.1746 - val_acc: 0.9532\n",
      "Epoch 26/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0762 - acc: 0.9754 - val_loss: 0.1610 - val_acc: 0.9560\n",
      "Epoch 27/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0775 - acc: 0.9758 - val_loss: 0.1728 - val_acc: 0.9539\n",
      "Epoch 28/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.0750 - acc: 0.9766 - val_loss: 0.1924 - val_acc: 0.9490\n",
      "Epoch 29/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0731 - acc: 0.9769 - val_loss: 0.1522 - val_acc: 0.9603\n",
      "Epoch 30/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0724 - acc: 0.9772 - val_loss: 0.1843 - val_acc: 0.9505\n",
      "baseline test loss:  0.1843178832948208\n",
      "baseline test accuracy:  0.9505\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_45 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 47835 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "47680/47835 [============================>.] - ETA: 0s - loss: 0.6065 - acc: 0.8143"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47835/47835 [==============================] - 2s 52us/sample - loss: 0.6056 - acc: 0.8146 - val_loss: 0.4856 - val_acc: 0.8291\n",
      "Epoch 2/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.3047 - acc: 0.9090 - val_loss: 0.3886 - val_acc: 0.8645\n",
      "Epoch 3/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.2388 - acc: 0.9287 - val_loss: 0.2409 - val_acc: 0.9249\n",
      "Epoch 4/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.1984 - acc: 0.9416 - val_loss: 0.2444 - val_acc: 0.9256\n",
      "Epoch 5/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.1750 - acc: 0.9479 - val_loss: 0.2210 - val_acc: 0.9324\n",
      "Epoch 6/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.1605 - acc: 0.9520 - val_loss: 0.1645 - val_acc: 0.9472\n",
      "Epoch 7/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.1468 - acc: 0.9556 - val_loss: 0.1871 - val_acc: 0.9432\n",
      "Epoch 8/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.1390 - acc: 0.9580 - val_loss: 0.2075 - val_acc: 0.9333\n",
      "Epoch 9/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.1330 - acc: 0.9591 - val_loss: 0.1822 - val_acc: 0.9417\n",
      "Epoch 10/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.1257 - acc: 0.9607 - val_loss: 0.2297 - val_acc: 0.9256\n",
      "Epoch 11/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.1257 - acc: 0.9616 - val_loss: 0.1776 - val_acc: 0.9466\n",
      "Epoch 12/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.1179 - acc: 0.9640 - val_loss: 0.1912 - val_acc: 0.9439\n",
      "Epoch 13/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.1129 - acc: 0.9651 - val_loss: 0.1600 - val_acc: 0.9517\n",
      "Epoch 14/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.1074 - acc: 0.9669 - val_loss: 0.1594 - val_acc: 0.9529\n",
      "Epoch 15/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.1045 - acc: 0.9675 - val_loss: 0.1744 - val_acc: 0.9508\n",
      "Epoch 16/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.1037 - acc: 0.9675 - val_loss: 0.1699 - val_acc: 0.9509\n",
      "Epoch 17/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0986 - acc: 0.9685 - val_loss: 0.1700 - val_acc: 0.9493\n",
      "Epoch 18/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0961 - acc: 0.9693 - val_loss: 0.1664 - val_acc: 0.9529\n",
      "Epoch 19/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0946 - acc: 0.9704 - val_loss: 0.1610 - val_acc: 0.9522\n",
      "Epoch 20/30\n",
      "47835/47835 [==============================] - 2s 44us/sample - loss: 0.0938 - acc: 0.9703 - val_loss: 0.1651 - val_acc: 0.9527\n",
      "Epoch 21/30\n",
      "47835/47835 [==============================] - 2s 46us/sample - loss: 0.0885 - acc: 0.9716 - val_loss: 0.1930 - val_acc: 0.9441\n",
      "Epoch 22/30\n",
      "47835/47835 [==============================] - 2s 39us/sample - loss: 0.0832 - acc: 0.9731 - val_loss: 0.1647 - val_acc: 0.9512\n",
      "Epoch 23/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.0871 - acc: 0.9718 - val_loss: 0.1643 - val_acc: 0.9519\n",
      "Epoch 24/30\n",
      "47835/47835 [==============================] - 2s 38us/sample - loss: 0.0851 - acc: 0.9733 - val_loss: 0.1614 - val_acc: 0.9534\n",
      "Epoch 25/30\n",
      "47835/47835 [==============================] - 2s 36us/sample - loss: 0.0828 - acc: 0.9739 - val_loss: 0.1758 - val_acc: 0.9482\n",
      "Epoch 26/30\n",
      "47835/47835 [==============================] - 2s 36us/sample - loss: 0.0826 - acc: 0.9738 - val_loss: 0.1395 - val_acc: 0.9585\n",
      "Epoch 27/30\n",
      "47835/47835 [==============================] - 2s 36us/sample - loss: 0.0804 - acc: 0.9740 - val_loss: 0.1627 - val_acc: 0.9535\n",
      "Epoch 28/30\n",
      "47835/47835 [==============================] - 2s 36us/sample - loss: 0.0808 - acc: 0.9741 - val_loss: 0.1614 - val_acc: 0.9558\n",
      "Epoch 29/30\n",
      "47835/47835 [==============================] - 2s 37us/sample - loss: 0.0794 - acc: 0.9742 - val_loss: 0.1534 - val_acc: 0.9563\n",
      "Epoch 30/30\n",
      "47835/47835 [==============================] - 2s 36us/sample - loss: 0.0742 - acc: 0.9766 - val_loss: 0.1938 - val_acc: 0.9475\n",
      "baseline test loss:  0.19383578218054026\n",
      "baseline test accuracy:  0.9475\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_48 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 47835 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "46720/47835 [============================>.] - ETA: 0s - loss: 0.5936 - acc: 0.8176"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47835/47835 [==============================] - 2s 48us/sample - loss: 0.5878 - acc: 0.8193 - val_loss: 0.5040 - val_acc: 0.8088\n",
      "Epoch 2/30\n",
      "47835/47835 [==============================] - 2s 46us/sample - loss: 0.2896 - acc: 0.9146 - val_loss: 0.3086 - val_acc: 0.8981\n",
      "Epoch 3/30\n",
      "47835/47835 [==============================] - 2s 49us/sample - loss: 0.2352 - acc: 0.9304 - val_loss: 0.2732 - val_acc: 0.9125\n",
      "Epoch 4/30\n",
      "47835/47835 [==============================] - 2s 45us/sample - loss: 0.1982 - acc: 0.9414 - val_loss: 0.2346 - val_acc: 0.9293\n",
      "Epoch 5/30\n",
      "47835/47835 [==============================] - 2s 45us/sample - loss: 0.1747 - acc: 0.9488 - val_loss: 0.2096 - val_acc: 0.9363\n",
      "Epoch 6/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.1614 - acc: 0.9525 - val_loss: 0.1966 - val_acc: 0.9402\n",
      "Epoch 7/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.1485 - acc: 0.9549 - val_loss: 0.2123 - val_acc: 0.9367\n",
      "Epoch 8/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.1369 - acc: 0.9588 - val_loss: 0.2008 - val_acc: 0.9425\n",
      "Epoch 9/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.1289 - acc: 0.9618 - val_loss: 0.2136 - val_acc: 0.9342\n",
      "Epoch 10/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.1204 - acc: 0.9636 - val_loss: 0.1624 - val_acc: 0.9522\n",
      "Epoch 11/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.1135 - acc: 0.9657 - val_loss: 0.1730 - val_acc: 0.9499\n",
      "Epoch 12/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.1110 - acc: 0.9665 - val_loss: 0.1786 - val_acc: 0.9486\n",
      "Epoch 13/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.1056 - acc: 0.9686 - val_loss: 0.1644 - val_acc: 0.9526\n",
      "Epoch 14/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.1030 - acc: 0.9681 - val_loss: 0.1666 - val_acc: 0.9524\n",
      "Epoch 15/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0987 - acc: 0.9699 - val_loss: 0.1790 - val_acc: 0.9492\n",
      "Epoch 16/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0971 - acc: 0.9708 - val_loss: 0.1507 - val_acc: 0.9569\n",
      "Epoch 17/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0934 - acc: 0.9709 - val_loss: 0.1721 - val_acc: 0.9526\n",
      "Epoch 18/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0913 - acc: 0.9719 - val_loss: 0.1624 - val_acc: 0.9552\n",
      "Epoch 19/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0868 - acc: 0.9728 - val_loss: 0.1525 - val_acc: 0.9562\n",
      "Epoch 20/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0860 - acc: 0.9733 - val_loss: 0.1487 - val_acc: 0.9595\n",
      "Epoch 21/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0845 - acc: 0.9738 - val_loss: 0.1469 - val_acc: 0.9596\n",
      "Epoch 22/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0837 - acc: 0.9742 - val_loss: 0.1642 - val_acc: 0.9536\n",
      "Epoch 23/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0795 - acc: 0.9758 - val_loss: 0.1566 - val_acc: 0.9569\n",
      "Epoch 24/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0816 - acc: 0.9745 - val_loss: 0.1611 - val_acc: 0.9529\n",
      "Epoch 25/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0791 - acc: 0.9752 - val_loss: 0.1578 - val_acc: 0.9570\n",
      "Epoch 26/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0749 - acc: 0.9762 - val_loss: 0.1666 - val_acc: 0.9530\n",
      "Epoch 27/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0744 - acc: 0.9760 - val_loss: 0.1770 - val_acc: 0.9525\n",
      "Epoch 28/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0726 - acc: 0.9774 - val_loss: 0.1514 - val_acc: 0.9604\n",
      "Epoch 29/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0718 - acc: 0.9778 - val_loss: 0.1466 - val_acc: 0.9615\n",
      "Epoch 30/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0718 - acc: 0.9776 - val_loss: 0.1640 - val_acc: 0.9571\n",
      "baseline test loss:  0.16398745581433177\n",
      "baseline test accuracy:  0.9571\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_51 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 47835 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "47104/47835 [============================>.] - ETA: 0s - loss: 0.5964 - acc: 0.8213"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47835/47835 [==============================] - 3s 55us/sample - loss: 0.5920 - acc: 0.8226 - val_loss: 0.4770 - val_acc: 0.8426\n",
      "Epoch 2/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.2878 - acc: 0.9164 - val_loss: 0.2830 - val_acc: 0.9163\n",
      "Epoch 3/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.2219 - acc: 0.9348 - val_loss: 0.2462 - val_acc: 0.9253\n",
      "Epoch 4/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.1893 - acc: 0.9453 - val_loss: 0.2468 - val_acc: 0.9221\n",
      "Epoch 5/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.1704 - acc: 0.9495 - val_loss: 0.2286 - val_acc: 0.9298\n",
      "Epoch 6/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.1576 - acc: 0.9523 - val_loss: 0.1973 - val_acc: 0.9394\n",
      "Epoch 7/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.1378 - acc: 0.9589 - val_loss: 0.2016 - val_acc: 0.9408\n",
      "Epoch 8/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.1285 - acc: 0.9612 - val_loss: 0.1781 - val_acc: 0.9464\n",
      "Epoch 9/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.1284 - acc: 0.9613 - val_loss: 0.1649 - val_acc: 0.9517\n",
      "Epoch 10/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.1185 - acc: 0.9643 - val_loss: 0.1859 - val_acc: 0.9448\n",
      "Epoch 11/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.1127 - acc: 0.9657 - val_loss: 0.1780 - val_acc: 0.9498\n",
      "Epoch 12/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.1043 - acc: 0.9684 - val_loss: 0.1607 - val_acc: 0.9533\n",
      "Epoch 13/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.1079 - acc: 0.9667 - val_loss: 0.1494 - val_acc: 0.9579\n",
      "Epoch 14/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.1021 - acc: 0.9686 - val_loss: 0.1560 - val_acc: 0.9556\n",
      "Epoch 15/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0969 - acc: 0.9701 - val_loss: 0.1874 - val_acc: 0.9454\n",
      "Epoch 16/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0956 - acc: 0.9703 - val_loss: 0.1828 - val_acc: 0.9477\n",
      "Epoch 17/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0922 - acc: 0.9712 - val_loss: 0.1755 - val_acc: 0.9518\n",
      "Epoch 18/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0887 - acc: 0.9726 - val_loss: 0.1519 - val_acc: 0.9592\n",
      "Epoch 19/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0849 - acc: 0.9730 - val_loss: 0.1727 - val_acc: 0.9514\n",
      "Epoch 20/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0859 - acc: 0.9730 - val_loss: 0.1428 - val_acc: 0.9605\n",
      "Epoch 21/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0821 - acc: 0.9741 - val_loss: 0.1512 - val_acc: 0.9575\n",
      "Epoch 22/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0814 - acc: 0.9753 - val_loss: 0.1558 - val_acc: 0.9588\n",
      "Epoch 23/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0787 - acc: 0.9755 - val_loss: 0.1454 - val_acc: 0.9611\n",
      "Epoch 24/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0735 - acc: 0.9771 - val_loss: 0.1744 - val_acc: 0.9540\n",
      "Epoch 25/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0801 - acc: 0.9752 - val_loss: 0.1378 - val_acc: 0.9633\n",
      "Epoch 26/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0778 - acc: 0.9751 - val_loss: 0.1742 - val_acc: 0.9525\n",
      "Epoch 27/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0744 - acc: 0.9767 - val_loss: 0.1386 - val_acc: 0.9611\n",
      "Epoch 28/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.0727 - acc: 0.9770 - val_loss: 0.1508 - val_acc: 0.9572\n",
      "Epoch 29/30\n",
      "47835/47835 [==============================] - 2s 41us/sample - loss: 0.0700 - acc: 0.9779 - val_loss: 0.1718 - val_acc: 0.9555\n",
      "Epoch 30/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0662 - acc: 0.9795 - val_loss: 0.1593 - val_acc: 0.9564\n",
      "baseline test loss:  0.1593267345287837\n",
      "baseline test accuracy:  0.9564\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_54 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 47835 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "47616/47835 [============================>.] - ETA: 0s - loss: 0.5782 - acc: 0.8265"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47835/47835 [==============================] - 3s 56us/sample - loss: 0.5764 - acc: 0.8271 - val_loss: 0.4757 - val_acc: 0.8320\n",
      "Epoch 2/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.2731 - acc: 0.9206 - val_loss: 0.3232 - val_acc: 0.8928\n",
      "Epoch 3/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.2239 - acc: 0.9327 - val_loss: 0.2191 - val_acc: 0.9327\n",
      "Epoch 4/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.1873 - acc: 0.9441 - val_loss: 0.2613 - val_acc: 0.9189\n",
      "Epoch 5/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.1687 - acc: 0.9505 - val_loss: 0.1949 - val_acc: 0.9392\n",
      "Epoch 6/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.1561 - acc: 0.9528 - val_loss: 0.2131 - val_acc: 0.9321\n",
      "Epoch 7/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.1447 - acc: 0.9562 - val_loss: 0.2206 - val_acc: 0.9307\n",
      "Epoch 8/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.1351 - acc: 0.9588 - val_loss: 0.1845 - val_acc: 0.9426\n",
      "Epoch 9/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.1262 - acc: 0.9622 - val_loss: 0.1655 - val_acc: 0.9498\n",
      "Epoch 10/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.1205 - acc: 0.9632 - val_loss: 0.1988 - val_acc: 0.9404\n",
      "Epoch 11/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.1150 - acc: 0.9643 - val_loss: 0.1873 - val_acc: 0.9447\n",
      "Epoch 12/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.1142 - acc: 0.9657 - val_loss: 0.1739 - val_acc: 0.9479\n",
      "Epoch 13/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.1075 - acc: 0.9675 - val_loss: 0.1545 - val_acc: 0.9527\n",
      "Epoch 14/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.1029 - acc: 0.9684 - val_loss: 0.1775 - val_acc: 0.9455\n",
      "Epoch 15/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.1004 - acc: 0.9697 - val_loss: 0.1693 - val_acc: 0.9494\n",
      "Epoch 16/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0970 - acc: 0.9700 - val_loss: 0.1683 - val_acc: 0.9525\n",
      "Epoch 17/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0993 - acc: 0.9698 - val_loss: 0.1651 - val_acc: 0.9487\n",
      "Epoch 18/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0938 - acc: 0.9706 - val_loss: 0.1748 - val_acc: 0.9487\n",
      "Epoch 19/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.0932 - acc: 0.9706 - val_loss: 0.1673 - val_acc: 0.9513\n",
      "Epoch 20/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0896 - acc: 0.9720 - val_loss: 0.1813 - val_acc: 0.9472\n",
      "Epoch 21/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0863 - acc: 0.9732 - val_loss: 0.1542 - val_acc: 0.9553\n",
      "Epoch 22/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0847 - acc: 0.9726 - val_loss: 0.1568 - val_acc: 0.9565\n",
      "Epoch 23/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0848 - acc: 0.9737 - val_loss: 0.1648 - val_acc: 0.9527\n",
      "Epoch 24/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0796 - acc: 0.9748 - val_loss: 0.1481 - val_acc: 0.9573\n",
      "Epoch 25/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.0803 - acc: 0.9749 - val_loss: 0.1526 - val_acc: 0.9588\n",
      "Epoch 26/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0794 - acc: 0.9748 - val_loss: 0.1543 - val_acc: 0.9554\n",
      "Epoch 27/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0780 - acc: 0.9750 - val_loss: 0.1549 - val_acc: 0.9564\n",
      "Epoch 28/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0771 - acc: 0.9754 - val_loss: 0.1727 - val_acc: 0.9542\n",
      "Epoch 29/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.0746 - acc: 0.9759 - val_loss: 0.1448 - val_acc: 0.9611\n",
      "Epoch 30/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.0765 - acc: 0.9757 - val_loss: 0.1636 - val_acc: 0.9547\n",
      "baseline test loss:  0.16361745212078094\n",
      "baseline test accuracy:  0.9547\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_57 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 47835 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "46848/47835 [============================>.] - ETA: 0s - loss: 0.6360 - acc: 0.8071"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47835/47835 [==============================] - 3s 57us/sample - loss: 0.6321 - acc: 0.8083 - val_loss: 0.5421 - val_acc: 0.8118\n",
      "Epoch 2/30\n",
      "47835/47835 [==============================] - 2s 44us/sample - loss: 0.2965 - acc: 0.9121 - val_loss: 0.3740 - val_acc: 0.8757\n",
      "Epoch 3/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.2325 - acc: 0.9304 - val_loss: 0.2685 - val_acc: 0.9169\n",
      "Epoch 4/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.2019 - acc: 0.9392 - val_loss: 0.2324 - val_acc: 0.9283\n",
      "Epoch 5/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.1783 - acc: 0.9459 - val_loss: 0.2359 - val_acc: 0.9288\n",
      "Epoch 6/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.1598 - acc: 0.9510 - val_loss: 0.2280 - val_acc: 0.9293\n",
      "Epoch 7/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.1489 - acc: 0.9550 - val_loss: 0.1780 - val_acc: 0.9468\n",
      "Epoch 8/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.1401 - acc: 0.9570 - val_loss: 0.1701 - val_acc: 0.9476\n",
      "Epoch 9/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.1319 - acc: 0.9589 - val_loss: 0.1767 - val_acc: 0.9487\n",
      "Epoch 10/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.1238 - acc: 0.9624 - val_loss: 0.2235 - val_acc: 0.9345\n",
      "Epoch 11/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.1177 - acc: 0.9636 - val_loss: 0.1758 - val_acc: 0.9482\n",
      "Epoch 12/30\n",
      "47835/47835 [==============================] - 2s 42us/sample - loss: 0.1134 - acc: 0.9645 - val_loss: 0.1843 - val_acc: 0.9469\n",
      "Epoch 13/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.1102 - acc: 0.9665 - val_loss: 0.1871 - val_acc: 0.9467\n",
      "Epoch 14/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.1078 - acc: 0.9660 - val_loss: 0.1980 - val_acc: 0.9427\n",
      "Epoch 15/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.1065 - acc: 0.9667 - val_loss: 0.2029 - val_acc: 0.9405\n",
      "Epoch 16/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.0978 - acc: 0.9694 - val_loss: 0.1939 - val_acc: 0.9439\n",
      "Epoch 17/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.0955 - acc: 0.9696 - val_loss: 0.1900 - val_acc: 0.9470\n",
      "Epoch 18/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.0985 - acc: 0.9694 - val_loss: 0.1788 - val_acc: 0.9496\n",
      "Epoch 19/30\n",
      "47835/47835 [==============================] - ETA: 0s - loss: 0.0920 - acc: 0.970 - 2s 43us/sample - loss: 0.0920 - acc: 0.9709 - val_loss: 0.1898 - val_acc: 0.9465\n",
      "Epoch 20/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.0928 - acc: 0.9710 - val_loss: 0.1892 - val_acc: 0.9483\n",
      "Epoch 21/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.0894 - acc: 0.9721 - val_loss: 0.1805 - val_acc: 0.9491\n",
      "Epoch 22/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.0875 - acc: 0.9720 - val_loss: 0.2138 - val_acc: 0.9418\n",
      "Epoch 23/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.0878 - acc: 0.9712 - val_loss: 0.1949 - val_acc: 0.9487\n",
      "Epoch 24/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.0807 - acc: 0.9747 - val_loss: 0.1875 - val_acc: 0.9496\n",
      "Epoch 25/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.0838 - acc: 0.9738 - val_loss: 0.1583 - val_acc: 0.9576\n",
      "Epoch 26/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.0809 - acc: 0.9743 - val_loss: 0.1572 - val_acc: 0.9580\n",
      "Epoch 27/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.0814 - acc: 0.9741 - val_loss: 0.1733 - val_acc: 0.9526\n",
      "Epoch 28/30\n",
      "47835/47835 [==============================] - 2s 43us/sample - loss: 0.0750 - acc: 0.9765 - val_loss: 0.1851 - val_acc: 0.9497\n",
      "Epoch 29/30\n",
      "47835/47835 [==============================] - 2s 44us/sample - loss: 0.0746 - acc: 0.9755 - val_loss: 0.2007 - val_acc: 0.9460\n",
      "Epoch 30/30\n",
      "47835/47835 [==============================] - 2s 44us/sample - loss: 0.0747 - acc: 0.9758 - val_loss: 0.1676 - val_acc: 0.9572\n",
      "baseline test loss:  0.16759816247008275\n",
      "baseline test accuracy:  0.9572\n"
     ]
    }
   ],
   "source": [
    "baseline_list =[]\n",
    "for i in range(10):\n",
    "    bl_model = build_model()\n",
    "    batch_size=64\n",
    "    epochs=30\n",
    "    bl_history = bl_model.fit(train_x, train_y, batch_size=batch_size,\n",
    "                        epochs=epochs, validation_data=(test_images, test_y))\n",
    "\n",
    "    bl_score = bl_model.evaluate(test_images, test_y, verbose=0)\n",
    "    print('baseline test loss: ', bl_score[0])\n",
    "    print('baseline test accuracy: ', bl_score[1] )\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    y_pred_oh = bl_model.predict(test_images)\n",
    "    y_pred_baseline = y_pred_oh.argmax(axis=-1)\n",
    "    baseline_list.append(classification_report(test_labels, y_pred_baseline, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "585c68af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of runs: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.993995</td>\n",
       "      <td>0.876327</td>\n",
       "      <td>0.931332</td>\n",
       "      <td>980.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.997444</td>\n",
       "      <td>0.910308</td>\n",
       "      <td>0.951594</td>\n",
       "      <td>1135.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.948296</td>\n",
       "      <td>0.973547</td>\n",
       "      <td>0.960569</td>\n",
       "      <td>1032.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.951598</td>\n",
       "      <td>0.976139</td>\n",
       "      <td>0.963661</td>\n",
       "      <td>1010.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.963865</td>\n",
       "      <td>0.972301</td>\n",
       "      <td>0.968026</td>\n",
       "      <td>982.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.950770</td>\n",
       "      <td>0.963117</td>\n",
       "      <td>0.956859</td>\n",
       "      <td>892.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.937388</td>\n",
       "      <td>0.979228</td>\n",
       "      <td>0.957680</td>\n",
       "      <td>958.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.955625</td>\n",
       "      <td>0.969844</td>\n",
       "      <td>0.962584</td>\n",
       "      <td>1028.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.894859</td>\n",
       "      <td>0.966016</td>\n",
       "      <td>0.928800</td>\n",
       "      <td>974.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.958031</td>\n",
       "      <td>0.960654</td>\n",
       "      <td>0.959322</td>\n",
       "      <td>1009.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.954180</td>\n",
       "      <td>0.954180</td>\n",
       "      <td>0.954180</td>\n",
       "      <td>0.95418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.955187</td>\n",
       "      <td>0.954748</td>\n",
       "      <td>0.954043</td>\n",
       "      <td>10000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.955922</td>\n",
       "      <td>0.954180</td>\n",
       "      <td>0.954109</td>\n",
       "      <td>10000.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.993995  0.876327  0.931332    980.00000\n",
       "1              0.997444  0.910308  0.951594   1135.00000\n",
       "2              0.948296  0.973547  0.960569   1032.00000\n",
       "3              0.951598  0.976139  0.963661   1010.00000\n",
       "4              0.963865  0.972301  0.968026    982.00000\n",
       "5              0.950770  0.963117  0.956859    892.00000\n",
       "6              0.937388  0.979228  0.957680    958.00000\n",
       "7              0.955625  0.969844  0.962584   1028.00000\n",
       "8              0.894859  0.966016  0.928800    974.00000\n",
       "9              0.958031  0.960654  0.959322   1009.00000\n",
       "accuracy       0.954180  0.954180  0.954180      0.95418\n",
       "macro avg      0.955187  0.954748  0.954043  10000.00000\n",
       "weighted avg   0.955922  0.954180  0.954109  10000.00000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_process_results(baseline_list, 'results_csv/mnist_baseline_cnn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7165beb",
   "metadata": {},
   "source": [
    "#  Augmentation experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7d5b322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_60 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 55771 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "55616/55771 [============================>.] - ETA: 0s - loss: 0.5963 - acc: 0.8168"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55771/55771 [==============================] - 3s 61us/sample - loss: 0.5958 - acc: 0.8170 - val_loss: 0.2452 - val_acc: 0.9214\n",
      "Epoch 2/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.2919 - acc: 0.9140 - val_loss: 0.1886 - val_acc: 0.9433\n",
      "Epoch 3/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.2327 - acc: 0.9320 - val_loss: 0.1912 - val_acc: 0.9410\n",
      "Epoch 4/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.2039 - acc: 0.9408 - val_loss: 0.1855 - val_acc: 0.9459\n",
      "Epoch 5/30\n",
      "55771/55771 [==============================] - 2s 43us/sample - loss: 0.1866 - acc: 0.9444 - val_loss: 0.1629 - val_acc: 0.9505\n",
      "Epoch 6/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.1712 - acc: 0.9500 - val_loss: 0.1647 - val_acc: 0.9488\n",
      "Epoch 7/30\n",
      "55771/55771 [==============================] - 2s 43us/sample - loss: 0.1569 - acc: 0.9530 - val_loss: 0.1681 - val_acc: 0.9505\n",
      "Epoch 8/30\n",
      "55771/55771 [==============================] - 2s 43us/sample - loss: 0.1496 - acc: 0.9547 - val_loss: 0.1553 - val_acc: 0.9552\n",
      "Epoch 9/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.1450 - acc: 0.9570 - val_loss: 0.1711 - val_acc: 0.9528\n",
      "Epoch 10/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.1358 - acc: 0.9590 - val_loss: 0.1438 - val_acc: 0.9581\n",
      "Epoch 11/30\n",
      "55771/55771 [==============================] - 2s 43us/sample - loss: 0.1321 - acc: 0.9602 - val_loss: 0.1702 - val_acc: 0.9512\n",
      "Epoch 12/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.1244 - acc: 0.9620 - val_loss: 0.1598 - val_acc: 0.9545\n",
      "Epoch 13/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.1234 - acc: 0.9622 - val_loss: 0.1457 - val_acc: 0.9562\n",
      "Epoch 14/30\n",
      "55771/55771 [==============================] - 2s 43us/sample - loss: 0.1172 - acc: 0.9635 - val_loss: 0.1450 - val_acc: 0.9573\n",
      "Epoch 15/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.1126 - acc: 0.9652 - val_loss: 0.1586 - val_acc: 0.9572\n",
      "Epoch 16/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.1084 - acc: 0.9671 - val_loss: 0.1582 - val_acc: 0.9558\n",
      "Epoch 17/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.1079 - acc: 0.9667 - val_loss: 0.1943 - val_acc: 0.9465\n",
      "Epoch 18/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.1044 - acc: 0.9683 - val_loss: 0.1540 - val_acc: 0.9581\n",
      "Epoch 19/30\n",
      "55771/55771 [==============================] - 2s 43us/sample - loss: 0.1033 - acc: 0.9683 - val_loss: 0.1504 - val_acc: 0.9583\n",
      "Epoch 20/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.0992 - acc: 0.9691 - val_loss: 0.1471 - val_acc: 0.9601\n",
      "Epoch 21/30\n",
      "55771/55771 [==============================] - 2s 43us/sample - loss: 0.1018 - acc: 0.9695 - val_loss: 0.1502 - val_acc: 0.9571\n",
      "Epoch 22/30\n",
      "55771/55771 [==============================] - 2s 43us/sample - loss: 0.0971 - acc: 0.9688 - val_loss: 0.1474 - val_acc: 0.9594\n",
      "Epoch 23/30\n",
      "55771/55771 [==============================] - 3s 52us/sample - loss: 0.0979 - acc: 0.9697 - val_loss: 0.1640 - val_acc: 0.9538\n",
      "Epoch 24/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.0942 - acc: 0.9700 - val_loss: 0.1597 - val_acc: 0.9575\n",
      "Epoch 25/30\n",
      "55771/55771 [==============================] - 3s 45us/sample - loss: 0.0899 - acc: 0.9724 - val_loss: 0.1671 - val_acc: 0.9550\n",
      "Epoch 26/30\n",
      "55771/55771 [==============================] - 3s 51us/sample - loss: 0.0891 - acc: 0.9717 - val_loss: 0.1555 - val_acc: 0.9572\n",
      "Epoch 27/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.0891 - acc: 0.9722 - val_loss: 0.1722 - val_acc: 0.9539\n",
      "Epoch 28/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.0884 - acc: 0.9729 - val_loss: 0.1657 - val_acc: 0.9556\n",
      "Epoch 29/30\n",
      "55771/55771 [==============================] - 2s 43us/sample - loss: 0.0867 - acc: 0.9724 - val_loss: 0.1739 - val_acc: 0.9533\n",
      "Epoch 30/30\n",
      "55771/55771 [==============================] - 2s 43us/sample - loss: 0.0837 - acc: 0.9735 - val_loss: 0.1558 - val_acc: 0.9592\n",
      "test loss for 0th run:  0.155777185073914\n",
      "test accuracy for 0th run:  0.9592\n",
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_63 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 55771 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "55680/55771 [============================>.] - ETA: 0s - loss: 0.5939 - acc: 0.8199"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55771/55771 [==============================] - 3s 57us/sample - loss: 0.5937 - acc: 0.8200 - val_loss: 0.2526 - val_acc: 0.9239\n",
      "Epoch 2/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.3096 - acc: 0.9092 - val_loss: 0.2088 - val_acc: 0.9354\n",
      "Epoch 3/30\n",
      "55771/55771 [==============================] - 2s 43us/sample - loss: 0.2453 - acc: 0.9271 - val_loss: 0.2177 - val_acc: 0.9369\n",
      "Epoch 4/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.2158 - acc: 0.9368 - val_loss: 0.1844 - val_acc: 0.9455\n",
      "Epoch 5/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.1958 - acc: 0.9421 - val_loss: 0.1791 - val_acc: 0.9477\n",
      "Epoch 6/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.1763 - acc: 0.9471 - val_loss: 0.1646 - val_acc: 0.9520\n",
      "Epoch 7/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.1613 - acc: 0.9515 - val_loss: 0.1810 - val_acc: 0.9487\n",
      "Epoch 8/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.1541 - acc: 0.9532 - val_loss: 0.1554 - val_acc: 0.9542\n",
      "Epoch 9/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.1461 - acc: 0.9565 - val_loss: 0.1750 - val_acc: 0.9515\n",
      "Epoch 10/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.1395 - acc: 0.9591 - val_loss: 0.1724 - val_acc: 0.9505\n",
      "Epoch 11/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.1367 - acc: 0.9581 - val_loss: 0.1623 - val_acc: 0.9538\n",
      "Epoch 12/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.1283 - acc: 0.9616 - val_loss: 0.1508 - val_acc: 0.9570\n",
      "Epoch 13/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.1233 - acc: 0.9624 - val_loss: 0.1563 - val_acc: 0.9569\n",
      "Epoch 14/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.1205 - acc: 0.9633 - val_loss: 0.1523 - val_acc: 0.9568\n",
      "Epoch 15/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.1219 - acc: 0.9642 - val_loss: 0.1445 - val_acc: 0.9600\n",
      "Epoch 16/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.1139 - acc: 0.9643 - val_loss: 0.1658 - val_acc: 0.9555\n",
      "Epoch 17/30\n",
      "55771/55771 [==============================] - 3s 49us/sample - loss: 0.1136 - acc: 0.9649 - val_loss: 0.1695 - val_acc: 0.9550\n",
      "Epoch 18/30\n",
      "55771/55771 [==============================] - 2s 45us/sample - loss: 0.1075 - acc: 0.9670 - val_loss: 0.1549 - val_acc: 0.9587\n",
      "Epoch 19/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1038 - acc: 0.9680 - val_loss: 0.1422 - val_acc: 0.9622\n",
      "Epoch 20/30\n",
      "55771/55771 [==============================] - 3s 51us/sample - loss: 0.1064 - acc: 0.9673 - val_loss: 0.1609 - val_acc: 0.9569\n",
      "Epoch 21/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1011 - acc: 0.9686 - val_loss: 0.1614 - val_acc: 0.9570\n",
      "Epoch 22/30\n",
      "55771/55771 [==============================] - 3s 49us/sample - loss: 0.0992 - acc: 0.9693 - val_loss: 0.1693 - val_acc: 0.9570\n",
      "Epoch 23/30\n",
      "55771/55771 [==============================] - 3s 51us/sample - loss: 0.0963 - acc: 0.9703 - val_loss: 0.1534 - val_acc: 0.9602\n",
      "Epoch 24/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.0952 - acc: 0.9700 - val_loss: 0.1576 - val_acc: 0.9580\n",
      "Epoch 25/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.0961 - acc: 0.9702 - val_loss: 0.1511 - val_acc: 0.9606\n",
      "Epoch 26/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.0943 - acc: 0.9705 - val_loss: 0.1723 - val_acc: 0.9537\n",
      "Epoch 27/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.0901 - acc: 0.9714 - val_loss: 0.1462 - val_acc: 0.9624\n",
      "Epoch 28/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.0873 - acc: 0.9724 - val_loss: 0.1495 - val_acc: 0.9593\n",
      "Epoch 29/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.0862 - acc: 0.9739 - val_loss: 0.1498 - val_acc: 0.9600\n",
      "Epoch 30/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.0881 - acc: 0.9716 - val_loss: 0.1659 - val_acc: 0.9586\n",
      "test loss for 1th run:  0.16591190911401063\n",
      "test accuracy for 1th run:  0.9586\n",
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_66 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 55771 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "55296/55771 [============================>.] - ETA: 0s - loss: 0.6126 - acc: 0.8115"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55771/55771 [==============================] - 3s 59us/sample - loss: 0.6101 - acc: 0.8123 - val_loss: 0.2576 - val_acc: 0.9235\n",
      "Epoch 2/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.3104 - acc: 0.9084 - val_loss: 0.2109 - val_acc: 0.9367\n",
      "Epoch 3/30\n",
      "55771/55771 [==============================] - 3s 45us/sample - loss: 0.2497 - acc: 0.9256 - val_loss: 0.1976 - val_acc: 0.9405\n",
      "Epoch 4/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.2161 - acc: 0.9362 - val_loss: 0.1822 - val_acc: 0.9462\n",
      "Epoch 5/30\n",
      "55771/55771 [==============================] - 2s 45us/sample - loss: 0.1923 - acc: 0.9423 - val_loss: 0.1745 - val_acc: 0.9466\n",
      "Epoch 6/30\n",
      "55771/55771 [==============================] - 2s 45us/sample - loss: 0.1764 - acc: 0.9472 - val_loss: 0.1743 - val_acc: 0.9453\n",
      "Epoch 7/30\n",
      "55771/55771 [==============================] - 3s 45us/sample - loss: 0.1642 - acc: 0.9523 - val_loss: 0.1598 - val_acc: 0.9510\n",
      "Epoch 8/30\n",
      "55771/55771 [==============================] - 3s 45us/sample - loss: 0.1526 - acc: 0.9540 - val_loss: 0.1686 - val_acc: 0.9508\n",
      "Epoch 9/30\n",
      "55771/55771 [==============================] - 3s 45us/sample - loss: 0.1451 - acc: 0.9556 - val_loss: 0.1652 - val_acc: 0.9527\n",
      "Epoch 10/30\n",
      "55771/55771 [==============================] - 3s 45us/sample - loss: 0.1393 - acc: 0.9584 - val_loss: 0.1506 - val_acc: 0.9556\n",
      "Epoch 11/30\n",
      "55771/55771 [==============================] - 3s 45us/sample - loss: 0.1345 - acc: 0.9595 - val_loss: 0.1558 - val_acc: 0.9541\n",
      "Epoch 12/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.1348 - acc: 0.9582 - val_loss: 0.1477 - val_acc: 0.9562\n",
      "Epoch 13/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.1263 - acc: 0.9618 - val_loss: 0.1491 - val_acc: 0.9573\n",
      "Epoch 14/30\n",
      "55771/55771 [==============================] - 3s 45us/sample - loss: 0.1213 - acc: 0.9637 - val_loss: 0.1496 - val_acc: 0.9583\n",
      "Epoch 15/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.1179 - acc: 0.9633 - val_loss: 0.1475 - val_acc: 0.9573\n",
      "Epoch 16/30\n",
      "55771/55771 [==============================] - 3s 45us/sample - loss: 0.1156 - acc: 0.9647 - val_loss: 0.1481 - val_acc: 0.9594\n",
      "Epoch 17/30\n",
      "55771/55771 [==============================] - 3s 45us/sample - loss: 0.1113 - acc: 0.9661 - val_loss: 0.1426 - val_acc: 0.9599\n",
      "Epoch 18/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.1086 - acc: 0.9670 - val_loss: 0.1579 - val_acc: 0.9560\n",
      "Epoch 19/30\n",
      "55771/55771 [==============================] - 2s 45us/sample - loss: 0.1055 - acc: 0.9665 - val_loss: 0.1519 - val_acc: 0.9564\n",
      "Epoch 20/30\n",
      "55771/55771 [==============================] - 2s 45us/sample - loss: 0.1046 - acc: 0.9678 - val_loss: 0.1574 - val_acc: 0.9568\n",
      "Epoch 21/30\n",
      "55771/55771 [==============================] - 2s 45us/sample - loss: 0.1019 - acc: 0.9684 - val_loss: 0.1601 - val_acc: 0.9583\n",
      "Epoch 22/30\n",
      "55771/55771 [==============================] - 3s 45us/sample - loss: 0.0999 - acc: 0.9694 - val_loss: 0.1513 - val_acc: 0.9605\n",
      "Epoch 23/30\n",
      "55771/55771 [==============================] - 2s 45us/sample - loss: 0.0982 - acc: 0.9699 - val_loss: 0.1605 - val_acc: 0.9550\n",
      "Epoch 24/30\n",
      "55771/55771 [==============================] - 3s 45us/sample - loss: 0.0954 - acc: 0.9701 - val_loss: 0.1620 - val_acc: 0.9567\n",
      "Epoch 25/30\n",
      "55771/55771 [==============================] - 2s 45us/sample - loss: 0.0941 - acc: 0.9700 - val_loss: 0.1566 - val_acc: 0.9575\n",
      "Epoch 26/30\n",
      "55771/55771 [==============================] - 2s 45us/sample - loss: 0.0933 - acc: 0.9710 - val_loss: 0.1610 - val_acc: 0.9559\n",
      "Epoch 27/30\n",
      "55771/55771 [==============================] - 3s 45us/sample - loss: 0.0880 - acc: 0.9720 - val_loss: 0.1679 - val_acc: 0.9539\n",
      "Epoch 28/30\n",
      "55771/55771 [==============================] - 3s 45us/sample - loss: 0.0878 - acc: 0.9736 - val_loss: 0.1628 - val_acc: 0.9535\n",
      "Epoch 29/30\n",
      "55771/55771 [==============================] - 2s 45us/sample - loss: 0.0869 - acc: 0.9732 - val_loss: 0.1634 - val_acc: 0.9572\n",
      "Epoch 30/30\n",
      "55771/55771 [==============================] - 2s 45us/sample - loss: 0.0872 - acc: 0.9721 - val_loss: 0.1599 - val_acc: 0.9573\n",
      "test loss for 2th run:  0.1599312620388344\n",
      "test accuracy for 2th run:  0.9573\n",
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_69 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 55771 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "54720/55771 [============================>.] - ETA: 0s - loss: 0.6204 - acc: 0.8120"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55771/55771 [==============================] - 3s 60us/sample - loss: 0.6163 - acc: 0.8135 - val_loss: 0.2770 - val_acc: 0.9177\n",
      "Epoch 2/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.3033 - acc: 0.9134 - val_loss: 0.2135 - val_acc: 0.9322\n",
      "Epoch 3/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.2405 - acc: 0.9305 - val_loss: 0.1883 - val_acc: 0.9422\n",
      "Epoch 4/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.2069 - acc: 0.9392 - val_loss: 0.2117 - val_acc: 0.9357\n",
      "Epoch 5/30\n",
      "55771/55771 [==============================] - 3s 45us/sample - loss: 0.1905 - acc: 0.9442 - val_loss: 0.1725 - val_acc: 0.9488\n",
      "Epoch 6/30\n",
      "55771/55771 [==============================] - 3s 45us/sample - loss: 0.1721 - acc: 0.9497 - val_loss: 0.1740 - val_acc: 0.9469\n",
      "Epoch 7/30\n",
      "55771/55771 [==============================] - 3s 45us/sample - loss: 0.1604 - acc: 0.9535 - val_loss: 0.1592 - val_acc: 0.9542\n",
      "Epoch 8/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.1496 - acc: 0.9564 - val_loss: 0.1537 - val_acc: 0.9535\n",
      "Epoch 9/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.1416 - acc: 0.9586 - val_loss: 0.1759 - val_acc: 0.9476\n",
      "Epoch 10/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.1351 - acc: 0.9602 - val_loss: 0.1757 - val_acc: 0.9491\n",
      "Epoch 11/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.1274 - acc: 0.9616 - val_loss: 0.1405 - val_acc: 0.9599\n",
      "Epoch 12/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.1246 - acc: 0.9632 - val_loss: 0.1462 - val_acc: 0.9565\n",
      "Epoch 13/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.1217 - acc: 0.9631 - val_loss: 0.1492 - val_acc: 0.9595\n",
      "Epoch 14/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.1155 - acc: 0.9650 - val_loss: 0.1409 - val_acc: 0.9594\n",
      "Epoch 15/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.1115 - acc: 0.9665 - val_loss: 0.1715 - val_acc: 0.9499\n",
      "Epoch 16/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.1107 - acc: 0.9671 - val_loss: 0.1497 - val_acc: 0.9569\n",
      "Epoch 17/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.1087 - acc: 0.9676 - val_loss: 0.1522 - val_acc: 0.9561\n",
      "Epoch 18/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.1039 - acc: 0.9687 - val_loss: 0.1897 - val_acc: 0.9473\n",
      "Epoch 19/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.1018 - acc: 0.9693 - val_loss: 0.1449 - val_acc: 0.9590\n",
      "Epoch 20/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1002 - acc: 0.9702 - val_loss: 0.1588 - val_acc: 0.9554\n",
      "Epoch 21/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.0982 - acc: 0.9702 - val_loss: 0.1657 - val_acc: 0.9532\n",
      "Epoch 22/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.0968 - acc: 0.9706 - val_loss: 0.1711 - val_acc: 0.9526\n",
      "Epoch 23/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.0928 - acc: 0.9715 - val_loss: 0.1463 - val_acc: 0.9596\n",
      "Epoch 24/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.0897 - acc: 0.9726 - val_loss: 0.1766 - val_acc: 0.9514\n",
      "Epoch 25/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.0916 - acc: 0.9715 - val_loss: 0.1408 - val_acc: 0.9609\n",
      "Epoch 26/30\n",
      "55771/55771 [==============================] - 3s 45us/sample - loss: 0.0892 - acc: 0.9726 - val_loss: 0.1422 - val_acc: 0.9618\n",
      "Epoch 27/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.0889 - acc: 0.9722 - val_loss: 0.1550 - val_acc: 0.9587\n",
      "Epoch 28/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.0866 - acc: 0.9728 - val_loss: 0.1597 - val_acc: 0.9573\n",
      "Epoch 29/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.0857 - acc: 0.9736 - val_loss: 0.1690 - val_acc: 0.9549\n",
      "Epoch 30/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.0893 - acc: 0.9728 - val_loss: 0.1570 - val_acc: 0.9582\n",
      "test loss for 3th run:  0.1570164343860466\n",
      "test accuracy for 3th run:  0.9582\n",
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_72 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 55771 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "55488/55771 [============================>.] - ETA: 0s - loss: 0.5767 - acc: 0.8238"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55771/55771 [==============================] - 4s 63us/sample - loss: 0.5755 - acc: 0.8242 - val_loss: 0.2467 - val_acc: 0.9220\n",
      "Epoch 2/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.2968 - acc: 0.9119 - val_loss: 0.1961 - val_acc: 0.9394\n",
      "Epoch 3/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.2357 - acc: 0.9292 - val_loss: 0.1758 - val_acc: 0.9461\n",
      "Epoch 4/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.2006 - acc: 0.9401 - val_loss: 0.1667 - val_acc: 0.9492\n",
      "Epoch 5/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1794 - acc: 0.9465 - val_loss: 0.1676 - val_acc: 0.9507\n",
      "Epoch 6/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1669 - acc: 0.9499 - val_loss: 0.1541 - val_acc: 0.9537\n",
      "Epoch 7/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1531 - acc: 0.9541 - val_loss: 0.1609 - val_acc: 0.9533\n",
      "Epoch 8/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1491 - acc: 0.9559 - val_loss: 0.1429 - val_acc: 0.9606\n",
      "Epoch 9/30\n",
      "55771/55771 [==============================] - 3s 52us/sample - loss: 0.1374 - acc: 0.9583 - val_loss: 0.1469 - val_acc: 0.9569\n",
      "Epoch 10/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1283 - acc: 0.9611 - val_loss: 0.1571 - val_acc: 0.9573\n",
      "Epoch 11/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1240 - acc: 0.9634 - val_loss: 0.1485 - val_acc: 0.9582\n",
      "Epoch 12/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1200 - acc: 0.9635 - val_loss: 0.1453 - val_acc: 0.9615\n",
      "Epoch 13/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1168 - acc: 0.9640 - val_loss: 0.1470 - val_acc: 0.9589\n",
      "Epoch 14/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1134 - acc: 0.9651 - val_loss: 0.1465 - val_acc: 0.9589\n",
      "Epoch 15/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1091 - acc: 0.9672 - val_loss: 0.1467 - val_acc: 0.9598\n",
      "Epoch 16/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1082 - acc: 0.9660 - val_loss: 0.1464 - val_acc: 0.9602\n",
      "Epoch 17/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1062 - acc: 0.9672 - val_loss: 0.1533 - val_acc: 0.9584\n",
      "Epoch 18/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.0995 - acc: 0.9691 - val_loss: 0.1591 - val_acc: 0.9573\n",
      "Epoch 19/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.0975 - acc: 0.9692 - val_loss: 0.1589 - val_acc: 0.9581\n",
      "Epoch 20/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.0986 - acc: 0.9699 - val_loss: 0.1392 - val_acc: 0.9631\n",
      "Epoch 21/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.0931 - acc: 0.9708 - val_loss: 0.1561 - val_acc: 0.9606\n",
      "Epoch 22/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.0958 - acc: 0.9698 - val_loss: 0.1428 - val_acc: 0.9621\n",
      "Epoch 23/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.0907 - acc: 0.9719 - val_loss: 0.1478 - val_acc: 0.9600\n",
      "Epoch 24/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.0914 - acc: 0.9712 - val_loss: 0.1613 - val_acc: 0.9578\n",
      "Epoch 25/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.0917 - acc: 0.9713 - val_loss: 0.1536 - val_acc: 0.9599\n",
      "Epoch 26/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.0875 - acc: 0.9728 - val_loss: 0.1537 - val_acc: 0.9602\n",
      "Epoch 27/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.0874 - acc: 0.9722 - val_loss: 0.1722 - val_acc: 0.9552\n",
      "Epoch 28/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.0869 - acc: 0.9723 - val_loss: 0.1440 - val_acc: 0.9639\n",
      "Epoch 29/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.0845 - acc: 0.9741 - val_loss: 0.1599 - val_acc: 0.9593\n",
      "Epoch 30/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.0820 - acc: 0.9744 - val_loss: 0.1505 - val_acc: 0.9609\n",
      "test loss for 4th run:  0.1504634059737902\n",
      "test accuracy for 4th run:  0.9609\n",
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_75 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 55771 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "55771/55771 [==============================] - ETA: 0s - loss: 0.5826 - acc: 0.8210"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55771/55771 [==============================] - 3s 63us/sample - loss: 0.5826 - acc: 0.8210 - val_loss: 0.2400 - val_acc: 0.9276\n",
      "Epoch 2/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.2839 - acc: 0.9154 - val_loss: 0.1890 - val_acc: 0.9433\n",
      "Epoch 3/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.2298 - acc: 0.9314 - val_loss: 0.1776 - val_acc: 0.9463\n",
      "Epoch 4/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1982 - acc: 0.9407 - val_loss: 0.1732 - val_acc: 0.9463\n",
      "Epoch 5/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1793 - acc: 0.9468 - val_loss: 0.1629 - val_acc: 0.9529\n",
      "Epoch 6/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1690 - acc: 0.9499 - val_loss: 0.1625 - val_acc: 0.9508\n",
      "Epoch 7/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1549 - acc: 0.9537 - val_loss: 0.1604 - val_acc: 0.9539\n",
      "Epoch 8/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1447 - acc: 0.9571 - val_loss: 0.1446 - val_acc: 0.9587\n",
      "Epoch 9/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1354 - acc: 0.9593 - val_loss: 0.1534 - val_acc: 0.9564\n",
      "Epoch 10/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1308 - acc: 0.9603 - val_loss: 0.1460 - val_acc: 0.9594\n",
      "Epoch 11/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1271 - acc: 0.9616 - val_loss: 0.1587 - val_acc: 0.9553\n",
      "Epoch 12/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1220 - acc: 0.9627 - val_loss: 0.1516 - val_acc: 0.9573\n",
      "Epoch 13/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1166 - acc: 0.9634 - val_loss: 0.1519 - val_acc: 0.9578\n",
      "Epoch 14/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1122 - acc: 0.9658 - val_loss: 0.1529 - val_acc: 0.9593\n",
      "Epoch 15/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1092 - acc: 0.9670 - val_loss: 0.1402 - val_acc: 0.9587\n",
      "Epoch 16/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1098 - acc: 0.9655 - val_loss: 0.1391 - val_acc: 0.9618\n",
      "Epoch 17/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1071 - acc: 0.9673 - val_loss: 0.1333 - val_acc: 0.9620\n",
      "Epoch 18/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1021 - acc: 0.9684 - val_loss: 0.1493 - val_acc: 0.9588\n",
      "Epoch 19/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1013 - acc: 0.9676 - val_loss: 0.1658 - val_acc: 0.9552\n",
      "Epoch 20/30\n",
      "55771/55771 [==============================] - 3s 49us/sample - loss: 0.0960 - acc: 0.9699 - val_loss: 0.1511 - val_acc: 0.9598\n",
      "Epoch 21/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.0959 - acc: 0.9703 - val_loss: 0.1466 - val_acc: 0.9606\n",
      "Epoch 22/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.0943 - acc: 0.9706 - val_loss: 0.1329 - val_acc: 0.9638\n",
      "Epoch 23/30\n",
      "55771/55771 [==============================] - 3s 49us/sample - loss: 0.0921 - acc: 0.9716 - val_loss: 0.1535 - val_acc: 0.9589\n",
      "Epoch 24/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.0893 - acc: 0.9720 - val_loss: 0.1437 - val_acc: 0.9621\n",
      "Epoch 25/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.0865 - acc: 0.9720 - val_loss: 0.1594 - val_acc: 0.9586\n",
      "Epoch 26/30\n",
      "55771/55771 [==============================] - 3s 51us/sample - loss: 0.0891 - acc: 0.9719 - val_loss: 0.1595 - val_acc: 0.9599\n",
      "Epoch 27/30\n",
      "55771/55771 [==============================] - 3s 49us/sample - loss: 0.0830 - acc: 0.9740 - val_loss: 0.1610 - val_acc: 0.9591\n",
      "Epoch 28/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.0852 - acc: 0.9731 - val_loss: 0.1531 - val_acc: 0.9624\n",
      "Epoch 29/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.0855 - acc: 0.9727 - val_loss: 0.1485 - val_acc: 0.9625\n",
      "Epoch 30/30\n",
      "55771/55771 [==============================] - 3s 60us/sample - loss: 0.0824 - acc: 0.9740 - val_loss: 0.1719 - val_acc: 0.9559\n",
      "test loss for 5th run:  0.17192207254073583\n",
      "test accuracy for 5th run:  0.9559\n",
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_78 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_26 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 55771 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "55616/55771 [============================>.] - ETA: 0s - loss: 0.5947 - acc: 0.8201"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55771/55771 [==============================] - 4s 74us/sample - loss: 0.5942 - acc: 0.8202 - val_loss: 0.2539 - val_acc: 0.9201\n",
      "Epoch 2/30\n",
      "55771/55771 [==============================] - 3s 49us/sample - loss: 0.2941 - acc: 0.9136 - val_loss: 0.2076 - val_acc: 0.9362\n",
      "Epoch 3/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.2326 - acc: 0.9313 - val_loss: 0.1878 - val_acc: 0.9442\n",
      "Epoch 4/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.2039 - acc: 0.9412 - val_loss: 0.1582 - val_acc: 0.9521\n",
      "Epoch 5/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1844 - acc: 0.9457 - val_loss: 0.1596 - val_acc: 0.9491\n",
      "Epoch 6/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1737 - acc: 0.9487 - val_loss: 0.1514 - val_acc: 0.9545\n",
      "Epoch 7/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.1654 - acc: 0.9509 - val_loss: 0.1668 - val_acc: 0.9518\n",
      "Epoch 8/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1522 - acc: 0.9545 - val_loss: 0.1594 - val_acc: 0.9528\n",
      "Epoch 9/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1472 - acc: 0.9562 - val_loss: 0.1510 - val_acc: 0.9547\n",
      "Epoch 10/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1415 - acc: 0.9583 - val_loss: 0.1433 - val_acc: 0.9593\n",
      "Epoch 11/30\n",
      "55771/55771 [==============================] - 3s 49us/sample - loss: 0.1387 - acc: 0.9581 - val_loss: 0.1651 - val_acc: 0.9522\n",
      "Epoch 12/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1299 - acc: 0.9610 - val_loss: 0.1509 - val_acc: 0.9573\n",
      "Epoch 13/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1254 - acc: 0.9616 - val_loss: 0.1605 - val_acc: 0.9553\n",
      "Epoch 14/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.1219 - acc: 0.9630 - val_loss: 0.1665 - val_acc: 0.9535\n",
      "Epoch 15/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1201 - acc: 0.9630 - val_loss: 0.1447 - val_acc: 0.9576\n",
      "Epoch 16/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1158 - acc: 0.9648 - val_loss: 0.1569 - val_acc: 0.9557\n",
      "Epoch 17/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1116 - acc: 0.9660 - val_loss: 0.1699 - val_acc: 0.9524\n",
      "Epoch 18/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.1101 - acc: 0.9661 - val_loss: 0.1554 - val_acc: 0.9549\n",
      "Epoch 19/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.1122 - acc: 0.9662 - val_loss: 0.1492 - val_acc: 0.9581\n",
      "Epoch 20/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1051 - acc: 0.9680 - val_loss: 0.1472 - val_acc: 0.9606\n",
      "Epoch 21/30\n",
      "55771/55771 [==============================] - 3s 49us/sample - loss: 0.1051 - acc: 0.9681 - val_loss: 0.1492 - val_acc: 0.9588\n",
      "Epoch 22/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1020 - acc: 0.9684 - val_loss: 0.1455 - val_acc: 0.9592\n",
      "Epoch 23/30\n",
      "55771/55771 [==============================] - 3s 55us/sample - loss: 0.0999 - acc: 0.9691 - val_loss: 0.1479 - val_acc: 0.9603\n",
      "Epoch 24/30\n",
      "55771/55771 [==============================] - 3s 56us/sample - loss: 0.0997 - acc: 0.9692 - val_loss: 0.1790 - val_acc: 0.9505\n",
      "Epoch 25/30\n",
      "55771/55771 [==============================] - 3s 56us/sample - loss: 0.0954 - acc: 0.9699 - val_loss: 0.1495 - val_acc: 0.9580\n",
      "Epoch 26/30\n",
      "55771/55771 [==============================] - 3s 56us/sample - loss: 0.0953 - acc: 0.9705 - val_loss: 0.1644 - val_acc: 0.9571\n",
      "Epoch 27/30\n",
      "55771/55771 [==============================] - 3s 55us/sample - loss: 0.0927 - acc: 0.9709 - val_loss: 0.1514 - val_acc: 0.9592\n",
      "Epoch 28/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.0938 - acc: 0.9709 - val_loss: 0.1528 - val_acc: 0.9604\n",
      "Epoch 29/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.0885 - acc: 0.9725 - val_loss: 0.1609 - val_acc: 0.9587\n",
      "Epoch 30/30\n",
      "55771/55771 [==============================] - 3s 59us/sample - loss: 0.0902 - acc: 0.9714 - val_loss: 0.1434 - val_acc: 0.9619\n",
      "test loss for 6th run:  0.14342344884108751\n",
      "test accuracy for 6th run:  0.9619\n",
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_81 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_27 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 55771 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "55771/55771 [==============================] - ETA: 0s - loss: 0.6163 - acc: 0.8110"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55771/55771 [==============================] - 4s 78us/sample - loss: 0.6163 - acc: 0.8110 - val_loss: 0.2658 - val_acc: 0.9187\n",
      "Epoch 2/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.2986 - acc: 0.9119 - val_loss: 0.2163 - val_acc: 0.9339\n",
      "Epoch 3/30\n",
      "55771/55771 [==============================] - 3s 58us/sample - loss: 0.2474 - acc: 0.9270 - val_loss: 0.2147 - val_acc: 0.9341\n",
      "Epoch 4/30\n",
      "55771/55771 [==============================] - 3s 52us/sample - loss: 0.2140 - acc: 0.9366 - val_loss: 0.1950 - val_acc: 0.9398\n",
      "Epoch 5/30\n",
      "55771/55771 [==============================] - 3s 49us/sample - loss: 0.1928 - acc: 0.9426 - val_loss: 0.1861 - val_acc: 0.9443\n",
      "Epoch 6/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.1749 - acc: 0.9478 - val_loss: 0.1638 - val_acc: 0.9487\n",
      "Epoch 7/30\n",
      "55771/55771 [==============================] - 3s 49us/sample - loss: 0.1624 - acc: 0.9511 - val_loss: 0.1558 - val_acc: 0.9523\n",
      "Epoch 8/30\n",
      "55771/55771 [==============================] - 3s 56us/sample - loss: 0.1521 - acc: 0.9553 - val_loss: 0.1568 - val_acc: 0.9522\n",
      "Epoch 9/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1465 - acc: 0.9564 - val_loss: 0.1677 - val_acc: 0.9481\n",
      "Epoch 10/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1369 - acc: 0.9587 - val_loss: 0.1662 - val_acc: 0.9493\n",
      "Epoch 11/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1356 - acc: 0.9589 - val_loss: 0.1502 - val_acc: 0.9539\n",
      "Epoch 12/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1277 - acc: 0.9612 - val_loss: 0.1547 - val_acc: 0.9523\n",
      "Epoch 13/30\n",
      "55771/55771 [==============================] - 3s 54us/sample - loss: 0.1278 - acc: 0.9613 - val_loss: 0.1588 - val_acc: 0.9527\n",
      "Epoch 14/30\n",
      "55771/55771 [==============================] - 3s 51us/sample - loss: 0.1203 - acc: 0.9637 - val_loss: 0.1566 - val_acc: 0.9533\n",
      "Epoch 15/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.1163 - acc: 0.9642 - val_loss: 0.1429 - val_acc: 0.9586\n",
      "Epoch 16/30\n",
      "55771/55771 [==============================] - 3s 51us/sample - loss: 0.1171 - acc: 0.9636 - val_loss: 0.1589 - val_acc: 0.9553\n",
      "Epoch 17/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.1117 - acc: 0.9652 - val_loss: 0.1438 - val_acc: 0.9608\n",
      "Epoch 18/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.1104 - acc: 0.9663 - val_loss: 0.1442 - val_acc: 0.9571\n",
      "Epoch 19/30\n",
      "55771/55771 [==============================] - 3s 49us/sample - loss: 0.1071 - acc: 0.9668 - val_loss: 0.1646 - val_acc: 0.9536\n",
      "Epoch 20/30\n",
      "55771/55771 [==============================] - 3s 52us/sample - loss: 0.1059 - acc: 0.9677 - val_loss: 0.1433 - val_acc: 0.9585\n",
      "Epoch 21/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.1021 - acc: 0.9683 - val_loss: 0.1570 - val_acc: 0.9550\n",
      "Epoch 22/30\n",
      "55771/55771 [==============================] - 3s 57us/sample - loss: 0.1005 - acc: 0.9692 - val_loss: 0.1432 - val_acc: 0.9592\n",
      "Epoch 23/30\n",
      "55771/55771 [==============================] - 3s 51us/sample - loss: 0.0990 - acc: 0.9690 - val_loss: 0.1665 - val_acc: 0.9523\n",
      "Epoch 24/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.0935 - acc: 0.9710 - val_loss: 0.1470 - val_acc: 0.9584\n",
      "Epoch 25/30\n",
      "55771/55771 [==============================] - 3s 52us/sample - loss: 0.0955 - acc: 0.9705 - val_loss: 0.1564 - val_acc: 0.9567\n",
      "Epoch 26/30\n",
      "55771/55771 [==============================] - 3s 55us/sample - loss: 0.0907 - acc: 0.9719 - val_loss: 0.1503 - val_acc: 0.9581\n",
      "Epoch 27/30\n",
      "55771/55771 [==============================] - 3s 52us/sample - loss: 0.0908 - acc: 0.9712 - val_loss: 0.1581 - val_acc: 0.9543\n",
      "Epoch 28/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.0912 - acc: 0.9717 - val_loss: 0.1621 - val_acc: 0.9537\n",
      "Epoch 29/30\n",
      "55771/55771 [==============================] - 3s 46us/sample - loss: 0.0879 - acc: 0.9715 - val_loss: 0.1665 - val_acc: 0.9563\n",
      "Epoch 30/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.0856 - acc: 0.9727 - val_loss: 0.1527 - val_acc: 0.9583\n",
      "test loss for 7th run:  0.1526904988243943\n",
      "test accuracy for 7th run:  0.9583\n",
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_84 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_28 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 55771 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "54656/55771 [============================>.] - ETA: 0s - loss: 0.5979 - acc: 0.8159"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55771/55771 [==============================] - 4s 64us/sample - loss: 0.5927 - acc: 0.8176 - val_loss: 0.2458 - val_acc: 0.9245\n",
      "Epoch 2/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.2829 - acc: 0.9177 - val_loss: 0.2040 - val_acc: 0.9363\n",
      "Epoch 3/30\n",
      "55771/55771 [==============================] - 3s 51us/sample - loss: 0.2268 - acc: 0.9342 - val_loss: 0.1878 - val_acc: 0.9443\n",
      "Epoch 4/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1969 - acc: 0.9426 - val_loss: 0.1953 - val_acc: 0.9422\n",
      "Epoch 5/30\n",
      "55771/55771 [==============================] - 3s 56us/sample - loss: 0.1768 - acc: 0.9469 - val_loss: 0.1571 - val_acc: 0.9526\n",
      "Epoch 6/30\n",
      "55771/55771 [==============================] - 3s 49us/sample - loss: 0.1653 - acc: 0.9509 - val_loss: 0.1850 - val_acc: 0.9441\n",
      "Epoch 7/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.1505 - acc: 0.9544 - val_loss: 0.1526 - val_acc: 0.9551\n",
      "Epoch 8/30\n",
      "55771/55771 [==============================] - 3s 49us/sample - loss: 0.1422 - acc: 0.9577 - val_loss: 0.1444 - val_acc: 0.9564\n",
      "Epoch 9/30\n",
      "55771/55771 [==============================] - 3s 58us/sample - loss: 0.1377 - acc: 0.9596 - val_loss: 0.1505 - val_acc: 0.9562\n",
      "Epoch 10/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1334 - acc: 0.9600 - val_loss: 0.1440 - val_acc: 0.9566\n",
      "Epoch 11/30\n",
      "55771/55771 [==============================] - 3s 52us/sample - loss: 0.1233 - acc: 0.9625 - val_loss: 0.1670 - val_acc: 0.9529\n",
      "Epoch 12/30\n",
      "55771/55771 [==============================] - 3s 52us/sample - loss: 0.1181 - acc: 0.9641 - val_loss: 0.1432 - val_acc: 0.9583\n",
      "Epoch 13/30\n",
      "55771/55771 [==============================] - 3s 51us/sample - loss: 0.1179 - acc: 0.9647 - val_loss: 0.1344 - val_acc: 0.9600\n",
      "Epoch 14/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.1121 - acc: 0.9661 - val_loss: 0.1462 - val_acc: 0.9591\n",
      "Epoch 15/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1097 - acc: 0.9669 - val_loss: 0.1438 - val_acc: 0.9602\n",
      "Epoch 16/30\n",
      "55771/55771 [==============================] - 3s 54us/sample - loss: 0.1053 - acc: 0.9667 - val_loss: 0.1381 - val_acc: 0.9604\n",
      "Epoch 17/30\n",
      "55771/55771 [==============================] - 3s 54us/sample - loss: 0.1025 - acc: 0.9685 - val_loss: 0.1355 - val_acc: 0.9616\n",
      "Epoch 18/30\n",
      "55771/55771 [==============================] - 3s 53us/sample - loss: 0.0987 - acc: 0.9692 - val_loss: 0.1335 - val_acc: 0.9629\n",
      "Epoch 19/30\n",
      "55771/55771 [==============================] - 3s 52us/sample - loss: 0.0967 - acc: 0.9696 - val_loss: 0.1411 - val_acc: 0.9608\n",
      "Epoch 20/30\n",
      "55771/55771 [==============================] - 3s 56us/sample - loss: 0.0943 - acc: 0.9704 - val_loss: 0.1432 - val_acc: 0.9601\n",
      "Epoch 21/30\n",
      "55771/55771 [==============================] - 3s 53us/sample - loss: 0.0915 - acc: 0.9717 - val_loss: 0.1560 - val_acc: 0.9554\n",
      "Epoch 22/30\n",
      "55771/55771 [==============================] - 3s 51us/sample - loss: 0.0915 - acc: 0.9712 - val_loss: 0.1502 - val_acc: 0.9584\n",
      "Epoch 23/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.0898 - acc: 0.9718 - val_loss: 0.1437 - val_acc: 0.9611\n",
      "Epoch 24/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.0848 - acc: 0.9737 - val_loss: 0.1504 - val_acc: 0.9585\n",
      "Epoch 25/30\n",
      "55771/55771 [==============================] - 3s 55us/sample - loss: 0.0873 - acc: 0.9725 - val_loss: 0.1447 - val_acc: 0.9620\n",
      "Epoch 26/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.0857 - acc: 0.9735 - val_loss: 0.1527 - val_acc: 0.9597\n",
      "Epoch 27/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.0843 - acc: 0.9732 - val_loss: 0.1639 - val_acc: 0.9555\n",
      "Epoch 28/30\n",
      "55771/55771 [==============================] - 3s 53us/sample - loss: 0.0802 - acc: 0.9753 - val_loss: 0.1565 - val_acc: 0.9584\n",
      "Epoch 29/30\n",
      "55771/55771 [==============================] - 3s 55us/sample - loss: 0.0843 - acc: 0.9732 - val_loss: 0.1508 - val_acc: 0.9586\n",
      "Epoch 30/30\n",
      "55771/55771 [==============================] - 3s 55us/sample - loss: 0.0790 - acc: 0.9757 - val_loss: 0.1407 - val_acc: 0.9620\n",
      "test loss for 8th run:  0.1407347920720931\n",
      "test accuracy for 8th run:  0.962\n",
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_87 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_59 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_29 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 55771 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "54976/55771 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.8181"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55771/55771 [==============================] - 4s 72us/sample - loss: 0.5996 - acc: 0.8190 - val_loss: 0.2365 - val_acc: 0.9276\n",
      "Epoch 2/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.2826 - acc: 0.9168 - val_loss: 0.2022 - val_acc: 0.9375\n",
      "Epoch 3/30\n",
      "55771/55771 [==============================] - 3s 49us/sample - loss: 0.2272 - acc: 0.9336 - val_loss: 0.1845 - val_acc: 0.9463\n",
      "Epoch 4/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.2001 - acc: 0.9409 - val_loss: 0.1836 - val_acc: 0.9461\n",
      "Epoch 5/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.1831 - acc: 0.9449 - val_loss: 0.1730 - val_acc: 0.9481\n",
      "Epoch 6/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.1675 - acc: 0.9498 - val_loss: 0.1624 - val_acc: 0.9531\n",
      "Epoch 7/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.1538 - acc: 0.9549 - val_loss: 0.1601 - val_acc: 0.9541\n",
      "Epoch 8/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.1448 - acc: 0.9560 - val_loss: 0.1647 - val_acc: 0.9514\n",
      "Epoch 9/30\n",
      "55771/55771 [==============================] - 3s 51us/sample - loss: 0.1360 - acc: 0.9603 - val_loss: 0.1510 - val_acc: 0.9567\n",
      "Epoch 10/30\n",
      "55771/55771 [==============================] - 3s 52us/sample - loss: 0.1286 - acc: 0.9602 - val_loss: 0.1559 - val_acc: 0.9566\n",
      "Epoch 11/30\n",
      "55771/55771 [==============================] - 3s 52us/sample - loss: 0.1236 - acc: 0.9622 - val_loss: 0.1559 - val_acc: 0.9573\n",
      "Epoch 12/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.1195 - acc: 0.9635 - val_loss: 0.1558 - val_acc: 0.9589\n",
      "Epoch 13/30\n",
      "55771/55771 [==============================] - 3s 51us/sample - loss: 0.1180 - acc: 0.9646 - val_loss: 0.1481 - val_acc: 0.9594\n",
      "Epoch 14/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.1129 - acc: 0.9654 - val_loss: 0.1529 - val_acc: 0.9589\n",
      "Epoch 15/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.1093 - acc: 0.9665 - val_loss: 0.1532 - val_acc: 0.9572\n",
      "Epoch 16/30\n",
      "55771/55771 [==============================] - 3s 54us/sample - loss: 0.1058 - acc: 0.9673 - val_loss: 0.1565 - val_acc: 0.9543\n",
      "Epoch 17/30\n",
      "55771/55771 [==============================] - 3s 55us/sample - loss: 0.1057 - acc: 0.9675 - val_loss: 0.1667 - val_acc: 0.9526\n",
      "Epoch 18/30\n",
      "55771/55771 [==============================] - 3s 53us/sample - loss: 0.1020 - acc: 0.9690 - val_loss: 0.1537 - val_acc: 0.9592\n",
      "Epoch 19/30\n",
      "55771/55771 [==============================] - 3s 53us/sample - loss: 0.0980 - acc: 0.9700 - val_loss: 0.1702 - val_acc: 0.9525\n",
      "Epoch 20/30\n",
      "55771/55771 [==============================] - 3s 56us/sample - loss: 0.0942 - acc: 0.9702 - val_loss: 0.1598 - val_acc: 0.9567\n",
      "Epoch 21/30\n",
      "55771/55771 [==============================] - 3s 49us/sample - loss: 0.0941 - acc: 0.9708 - val_loss: 0.1574 - val_acc: 0.9569\n",
      "Epoch 22/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.0947 - acc: 0.9709 - val_loss: 0.1657 - val_acc: 0.9560\n",
      "Epoch 23/30\n",
      "55771/55771 [==============================] - 3s 50us/sample - loss: 0.0916 - acc: 0.9714 - val_loss: 0.1676 - val_acc: 0.9563\n",
      "Epoch 24/30\n",
      "55771/55771 [==============================] - 3s 49us/sample - loss: 0.0902 - acc: 0.9722 - val_loss: 0.1629 - val_acc: 0.9565\n",
      "Epoch 25/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.0868 - acc: 0.9734 - val_loss: 0.1661 - val_acc: 0.9561\n",
      "Epoch 26/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.0857 - acc: 0.9728 - val_loss: 0.1501 - val_acc: 0.9585\n",
      "Epoch 27/30\n",
      "55771/55771 [==============================] - 3s 47us/sample - loss: 0.0890 - acc: 0.9725 - val_loss: 0.1600 - val_acc: 0.9571\n",
      "Epoch 28/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.0828 - acc: 0.9744 - val_loss: 0.1573 - val_acc: 0.9572\n",
      "Epoch 29/30\n",
      "55771/55771 [==============================] - 3s 48us/sample - loss: 0.0812 - acc: 0.9745 - val_loss: 0.1582 - val_acc: 0.9585\n",
      "Epoch 30/30\n",
      "55771/55771 [==============================] - 3s 52us/sample - loss: 0.0812 - acc: 0.9751 - val_loss: 0.1619 - val_acc: 0.9581\n",
      "test loss for 9th run:  0.16187421993380413\n",
      "test accuracy for 9th run:  0.9581\n"
     ]
    }
   ],
   "source": [
    "results_list =[]\n",
    "for i in range(10):\n",
    "    gen_samples, gen_labels = generate_samples()\n",
    "    X = np.concatenate([train_x, gen_samples])\n",
    "    Y = np.concatenate([np.reshape(gr_labels, -1), gen_labels])\n",
    "\n",
    "    Y_oh = np.array(tf.keras.utils.to_categorical(Y, num_classes=10, dtype='float32'))\n",
    "\n",
    "    aug_model = build_model()\n",
    "    aug_history = aug_model.fit(X, Y_oh, batch_size=batch_size,\n",
    "                        epochs=epochs, validation_data=(test_images, test_y))\n",
    "\n",
    "    aug_score = aug_model.evaluate(test_images, test_y, verbose=0)\n",
    "    print('test loss for {}th run: '.format(i), aug_score[0])\n",
    "    print('test accuracy for {}th run: '.format(i), aug_score[1] )\n",
    "    \n",
    "    y_pred_aug_oh = bl_model.predict(test_images)\n",
    "    y_pred_aug = y_pred_aug_oh.argmax(axis=-1)\n",
    "    results_list.append(classification_report(test_labels, y_pred_aug, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7e13359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of runs: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.993158</td>\n",
       "      <td>0.888776</td>\n",
       "      <td>0.938072</td>\n",
       "      <td>980.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.995256</td>\n",
       "      <td>0.924229</td>\n",
       "      <td>0.958429</td>\n",
       "      <td>1135.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.951932</td>\n",
       "      <td>0.978682</td>\n",
       "      <td>0.965122</td>\n",
       "      <td>1032.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.955426</td>\n",
       "      <td>0.976238</td>\n",
       "      <td>0.965720</td>\n",
       "      <td>1010.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.973469</td>\n",
       "      <td>0.971487</td>\n",
       "      <td>0.972477</td>\n",
       "      <td>982.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.947137</td>\n",
       "      <td>0.964126</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>892.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.917315</td>\n",
       "      <td>0.984342</td>\n",
       "      <td>0.949648</td>\n",
       "      <td>958.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.973581</td>\n",
       "      <td>0.967899</td>\n",
       "      <td>0.970732</td>\n",
       "      <td>1028.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.906008</td>\n",
       "      <td>0.959959</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>974.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.963221</td>\n",
       "      <td>0.960357</td>\n",
       "      <td>0.961787</td>\n",
       "      <td>1009.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.957200</td>\n",
       "      <td>0.957200</td>\n",
       "      <td>0.957200</td>\n",
       "      <td>0.9572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.957650</td>\n",
       "      <td>0.957609</td>\n",
       "      <td>0.956974</td>\n",
       "      <td>10000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.958505</td>\n",
       "      <td>0.957200</td>\n",
       "      <td>0.957192</td>\n",
       "      <td>10000.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.993158  0.888776  0.938072    980.0000\n",
       "1              0.995256  0.924229  0.958429   1135.0000\n",
       "2              0.951932  0.978682  0.965122   1032.0000\n",
       "3              0.955426  0.976238  0.965720   1010.0000\n",
       "4              0.973469  0.971487  0.972477    982.0000\n",
       "5              0.947137  0.964126  0.955556    892.0000\n",
       "6              0.917315  0.984342  0.949648    958.0000\n",
       "7              0.973581  0.967899  0.970732   1028.0000\n",
       "8              0.906008  0.959959  0.932203    974.0000\n",
       "9              0.963221  0.960357  0.961787   1009.0000\n",
       "accuracy       0.957200  0.957200  0.957200      0.9572\n",
       "macro avg      0.957650  0.957609  0.956974  10000.0000\n",
       "weighted avg   0.958505  0.957200  0.957192  10000.0000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_process_results(results_list, 'results_csv/mnist_VAE.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7b88e9",
   "metadata": {},
   "source": [
    "# Random UnderSmapling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2458bbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_90 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_60 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_61 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_30 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "2304/2500 [==========================>...] - ETA: 0s - loss: 2.1591 - acc: 0.3390"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 1s 524us/sample - loss: 2.1040 - acc: 0.3512 - val_loss: 1.7635 - val_acc: 0.5809\n",
      "Epoch 2/30\n",
      "2500/2500 [==============================] - 0s 146us/sample - loss: 1.1332 - acc: 0.6256 - val_loss: 1.3408 - val_acc: 0.7369\n",
      "Epoch 3/30\n",
      "2500/2500 [==============================] - 0s 148us/sample - loss: 0.8479 - acc: 0.7312 - val_loss: 0.9910 - val_acc: 0.8116\n",
      "Epoch 4/30\n",
      "2500/2500 [==============================] - 0s 163us/sample - loss: 0.7122 - acc: 0.7764 - val_loss: 0.7369 - val_acc: 0.8476\n",
      "Epoch 5/30\n",
      "2500/2500 [==============================] - 0s 166us/sample - loss: 0.6120 - acc: 0.8088 - val_loss: 0.5964 - val_acc: 0.8617\n",
      "Epoch 6/30\n",
      "2500/2500 [==============================] - 0s 159us/sample - loss: 0.5582 - acc: 0.8340 - val_loss: 0.5067 - val_acc: 0.8735\n",
      "Epoch 7/30\n",
      "2500/2500 [==============================] - 0s 183us/sample - loss: 0.5018 - acc: 0.8496 - val_loss: 0.4239 - val_acc: 0.8888\n",
      "Epoch 8/30\n",
      "2500/2500 [==============================] - 0s 166us/sample - loss: 0.4439 - acc: 0.8680 - val_loss: 0.3977 - val_acc: 0.8882\n",
      "Epoch 9/30\n",
      "2500/2500 [==============================] - 0s 170us/sample - loss: 0.4206 - acc: 0.8752 - val_loss: 0.3685 - val_acc: 0.8925\n",
      "Epoch 10/30\n",
      "2500/2500 [==============================] - 0s 186us/sample - loss: 0.3767 - acc: 0.8876 - val_loss: 0.3681 - val_acc: 0.8952\n",
      "Epoch 11/30\n",
      "2500/2500 [==============================] - 0s 166us/sample - loss: 0.3437 - acc: 0.8980 - val_loss: 0.3444 - val_acc: 0.8998\n",
      "Epoch 12/30\n",
      "2500/2500 [==============================] - 0s 179us/sample - loss: 0.3102 - acc: 0.9124 - val_loss: 0.3358 - val_acc: 0.9020\n",
      "Epoch 13/30\n",
      "2500/2500 [==============================] - 0s 175us/sample - loss: 0.3050 - acc: 0.9128 - val_loss: 0.3229 - val_acc: 0.9043\n",
      "Epoch 14/30\n",
      "2500/2500 [==============================] - 1s 205us/sample - loss: 0.2911 - acc: 0.9168 - val_loss: 0.3291 - val_acc: 0.9049\n",
      "Epoch 15/30\n",
      "2500/2500 [==============================] - 1s 226us/sample - loss: 0.2516 - acc: 0.9348 - val_loss: 0.3315 - val_acc: 0.9019\n",
      "Epoch 16/30\n",
      "2500/2500 [==============================] - 0s 186us/sample - loss: 0.2711 - acc: 0.9228 - val_loss: 0.3297 - val_acc: 0.9014\n",
      "Epoch 17/30\n",
      "2500/2500 [==============================] - 0s 172us/sample - loss: 0.2632 - acc: 0.9240 - val_loss: 0.3237 - val_acc: 0.9053\n",
      "Epoch 18/30\n",
      "2500/2500 [==============================] - 0s 152us/sample - loss: 0.2487 - acc: 0.9248 - val_loss: 0.3243 - val_acc: 0.9044\n",
      "Epoch 19/30\n",
      "2500/2500 [==============================] - 0s 158us/sample - loss: 0.2582 - acc: 0.9148 - val_loss: 0.3328 - val_acc: 0.9024\n",
      "Epoch 20/30\n",
      "2500/2500 [==============================] - 0s 156us/sample - loss: 0.2416 - acc: 0.9272 - val_loss: 0.3261 - val_acc: 0.9024\n",
      "Epoch 21/30\n",
      "2500/2500 [==============================] - 0s 178us/sample - loss: 0.2169 - acc: 0.9404 - val_loss: 0.3337 - val_acc: 0.9045\n",
      "Epoch 22/30\n",
      "2500/2500 [==============================] - 1s 214us/sample - loss: 0.2305 - acc: 0.9320 - val_loss: 0.3133 - val_acc: 0.9100\n",
      "Epoch 23/30\n",
      "2500/2500 [==============================] - 1s 234us/sample - loss: 0.2429 - acc: 0.9304 - val_loss: 0.3302 - val_acc: 0.9051\n",
      "Epoch 24/30\n",
      "2500/2500 [==============================] - 0s 168us/sample - loss: 0.1913 - acc: 0.9432 - val_loss: 0.3230 - val_acc: 0.9054\n",
      "Epoch 25/30\n",
      "2500/2500 [==============================] - 0s 179us/sample - loss: 0.2025 - acc: 0.9360 - val_loss: 0.3277 - val_acc: 0.9067\n",
      "Epoch 26/30\n",
      "2500/2500 [==============================] - 0s 153us/sample - loss: 0.1904 - acc: 0.9392 - val_loss: 0.3191 - val_acc: 0.9092\n",
      "Epoch 27/30\n",
      "2500/2500 [==============================] - 0s 150us/sample - loss: 0.1688 - acc: 0.9536 - val_loss: 0.3103 - val_acc: 0.9100\n",
      "Epoch 28/30\n",
      "2500/2500 [==============================] - 0s 157us/sample - loss: 0.1933 - acc: 0.9376 - val_loss: 0.3239 - val_acc: 0.9062\n",
      "Epoch 29/30\n",
      "2500/2500 [==============================] - 0s 156us/sample - loss: 0.1630 - acc: 0.9512 - val_loss: 0.3379 - val_acc: 0.9048\n",
      "Epoch 30/30\n",
      "2500/2500 [==============================] - 0s 159us/sample - loss: 0.1709 - acc: 0.9464 - val_loss: 0.3247 - val_acc: 0.9097\n",
      "undersampling test loss:  0.32471906616538765\n",
      "undersampling accuracy:  0.9097\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_93 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_62 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_63 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_31 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "2240/2500 [=========================>....] - ETA: 0s - loss: 2.1399 - acc: 0.3335"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 1s 595us/sample - loss: 2.0635 - acc: 0.3508 - val_loss: 1.8096 - val_acc: 0.5242\n",
      "Epoch 2/30\n",
      "2500/2500 [==============================] - 0s 162us/sample - loss: 1.1649 - acc: 0.6244 - val_loss: 1.4464 - val_acc: 0.7274\n",
      "Epoch 3/30\n",
      "2500/2500 [==============================] - 0s 177us/sample - loss: 0.8786 - acc: 0.7168 - val_loss: 1.0929 - val_acc: 0.8135\n",
      "Epoch 4/30\n",
      "2500/2500 [==============================] - 0s 162us/sample - loss: 0.6985 - acc: 0.7888 - val_loss: 0.8209 - val_acc: 0.8462\n",
      "Epoch 5/30\n",
      "2500/2500 [==============================] - 0s 150us/sample - loss: 0.6134 - acc: 0.8136 - val_loss: 0.6218 - val_acc: 0.8666\n",
      "Epoch 6/30\n",
      "2500/2500 [==============================] - 0s 170us/sample - loss: 0.5404 - acc: 0.8404 - val_loss: 0.5083 - val_acc: 0.8782\n",
      "Epoch 7/30\n",
      "2500/2500 [==============================] - 0s 155us/sample - loss: 0.4637 - acc: 0.8652 - val_loss: 0.4478 - val_acc: 0.8808\n",
      "Epoch 8/30\n",
      "2500/2500 [==============================] - 0s 160us/sample - loss: 0.3981 - acc: 0.8964 - val_loss: 0.3792 - val_acc: 0.8916\n",
      "Epoch 9/30\n",
      "2500/2500 [==============================] - 0s 158us/sample - loss: 0.3574 - acc: 0.8988 - val_loss: 0.3678 - val_acc: 0.8904\n",
      "Epoch 10/30\n",
      "2500/2500 [==============================] - 0s 147us/sample - loss: 0.3295 - acc: 0.9060 - val_loss: 0.3435 - val_acc: 0.8964\n",
      "Epoch 11/30\n",
      "2500/2500 [==============================] - 0s 162us/sample - loss: 0.3350 - acc: 0.9036 - val_loss: 0.3476 - val_acc: 0.8942\n",
      "Epoch 12/30\n",
      "2500/2500 [==============================] - 0s 173us/sample - loss: 0.2920 - acc: 0.9148 - val_loss: 0.3355 - val_acc: 0.8984\n",
      "Epoch 13/30\n",
      "2500/2500 [==============================] - 0s 156us/sample - loss: 0.2771 - acc: 0.9188 - val_loss: 0.3271 - val_acc: 0.9018\n",
      "Epoch 14/30\n",
      "2500/2500 [==============================] - 0s 152us/sample - loss: 0.2487 - acc: 0.9320 - val_loss: 0.3265 - val_acc: 0.9024\n",
      "Epoch 15/30\n",
      "2500/2500 [==============================] - 0s 147us/sample - loss: 0.2476 - acc: 0.9304 - val_loss: 0.3256 - val_acc: 0.9018\n",
      "Epoch 16/30\n",
      "2500/2500 [==============================] - 0s 160us/sample - loss: 0.2581 - acc: 0.9240 - val_loss: 0.3405 - val_acc: 0.8985\n",
      "Epoch 17/30\n",
      "2500/2500 [==============================] - 0s 165us/sample - loss: 0.2278 - acc: 0.9400 - val_loss: 0.3374 - val_acc: 0.9008\n",
      "Epoch 18/30\n",
      "2500/2500 [==============================] - 0s 167us/sample - loss: 0.2139 - acc: 0.9388 - val_loss: 0.3226 - val_acc: 0.9035\n",
      "Epoch 19/30\n",
      "2500/2500 [==============================] - 0s 147us/sample - loss: 0.1845 - acc: 0.9488 - val_loss: 0.3072 - val_acc: 0.9085\n",
      "Epoch 20/30\n",
      "2500/2500 [==============================] - 0s 147us/sample - loss: 0.1777 - acc: 0.9528 - val_loss: 0.3173 - val_acc: 0.9069\n",
      "Epoch 21/30\n",
      "2500/2500 [==============================] - 0s 157us/sample - loss: 0.1735 - acc: 0.9512 - val_loss: 0.3104 - val_acc: 0.9103\n",
      "Epoch 22/30\n",
      "2500/2500 [==============================] - 0s 161us/sample - loss: 0.1562 - acc: 0.9524 - val_loss: 0.3060 - val_acc: 0.9124\n",
      "Epoch 23/30\n",
      "2500/2500 [==============================] - 0s 177us/sample - loss: 0.1381 - acc: 0.9640 - val_loss: 0.3096 - val_acc: 0.9099\n",
      "Epoch 24/30\n",
      "2500/2500 [==============================] - 1s 209us/sample - loss: 0.1577 - acc: 0.9544 - val_loss: 0.3503 - val_acc: 0.9000\n",
      "Epoch 25/30\n",
      "2500/2500 [==============================] - 1s 204us/sample - loss: 0.1513 - acc: 0.9560 - val_loss: 0.3315 - val_acc: 0.9026\n",
      "Epoch 26/30\n",
      "2500/2500 [==============================] - 0s 161us/sample - loss: 0.1694 - acc: 0.9548 - val_loss: 0.3683 - val_acc: 0.8946\n",
      "Epoch 27/30\n",
      "2500/2500 [==============================] - 0s 148us/sample - loss: 0.1742 - acc: 0.9532 - val_loss: 0.3607 - val_acc: 0.8987\n",
      "Epoch 28/30\n",
      "2500/2500 [==============================] - 0s 151us/sample - loss: 0.1624 - acc: 0.9472 - val_loss: 0.3496 - val_acc: 0.9019\n",
      "Epoch 29/30\n",
      "2500/2500 [==============================] - 0s 149us/sample - loss: 0.1545 - acc: 0.9540 - val_loss: 0.3309 - val_acc: 0.9062\n",
      "Epoch 30/30\n",
      "2500/2500 [==============================] - 0s 187us/sample - loss: 0.1617 - acc: 0.9544 - val_loss: 0.4063 - val_acc: 0.8845\n",
      "undersampling test loss:  0.40631377737522123\n",
      "undersampling accuracy:  0.8845\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_96 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_64 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_65 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_32 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "2368/2500 [===========================>..] - ETA: 0s - loss: 2.1751 - acc: 0.3619"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 1s 567us/sample - loss: 2.1260 - acc: 0.3744 - val_loss: 1.8166 - val_acc: 0.6210\n",
      "Epoch 2/30\n",
      "2500/2500 [==============================] - 0s 157us/sample - loss: 1.0659 - acc: 0.6580 - val_loss: 1.3829 - val_acc: 0.7267\n",
      "Epoch 3/30\n",
      "2500/2500 [==============================] - 0s 154us/sample - loss: 0.7673 - acc: 0.7540 - val_loss: 1.0062 - val_acc: 0.8244\n",
      "Epoch 4/30\n",
      "2500/2500 [==============================] - 0s 160us/sample - loss: 0.6217 - acc: 0.8088 - val_loss: 0.7397 - val_acc: 0.8515\n",
      "Epoch 5/30\n",
      "2500/2500 [==============================] - 0s 154us/sample - loss: 0.5265 - acc: 0.8416 - val_loss: 0.5659 - val_acc: 0.8728\n",
      "Epoch 6/30\n",
      "2500/2500 [==============================] - 0s 167us/sample - loss: 0.4808 - acc: 0.8524 - val_loss: 0.4605 - val_acc: 0.8798\n",
      "Epoch 7/30\n",
      "2500/2500 [==============================] - 0s 157us/sample - loss: 0.4276 - acc: 0.8700 - val_loss: 0.4112 - val_acc: 0.8822\n",
      "Epoch 8/30\n",
      "2500/2500 [==============================] - 0s 158us/sample - loss: 0.3838 - acc: 0.8884 - val_loss: 0.3832 - val_acc: 0.8911\n",
      "Epoch 9/30\n",
      "2500/2500 [==============================] - 0s 164us/sample - loss: 0.3456 - acc: 0.8972 - val_loss: 0.3563 - val_acc: 0.8930\n",
      "Epoch 10/30\n",
      "2500/2500 [==============================] - 0s 166us/sample - loss: 0.3329 - acc: 0.9060 - val_loss: 0.3413 - val_acc: 0.8953\n",
      "Epoch 11/30\n",
      "2500/2500 [==============================] - 0s 173us/sample - loss: 0.3104 - acc: 0.9108 - val_loss: 0.3289 - val_acc: 0.9019\n",
      "Epoch 12/30\n",
      "2500/2500 [==============================] - 0s 157us/sample - loss: 0.2891 - acc: 0.9204 - val_loss: 0.3328 - val_acc: 0.9036\n",
      "Epoch 13/30\n",
      "2500/2500 [==============================] - 0s 155us/sample - loss: 0.2708 - acc: 0.9180 - val_loss: 0.3329 - val_acc: 0.9034\n",
      "Epoch 14/30\n",
      "2500/2500 [==============================] - 0s 151us/sample - loss: 0.2271 - acc: 0.9364 - val_loss: 0.3700 - val_acc: 0.8958\n",
      "Epoch 15/30\n",
      "2500/2500 [==============================] - 0s 155us/sample - loss: 0.2320 - acc: 0.9312 - val_loss: 0.3259 - val_acc: 0.9057\n",
      "Epoch 16/30\n",
      "2500/2500 [==============================] - 0s 157us/sample - loss: 0.2390 - acc: 0.9280 - val_loss: 0.3288 - val_acc: 0.9036\n",
      "Epoch 17/30\n",
      "2500/2500 [==============================] - 0s 149us/sample - loss: 0.2192 - acc: 0.9356 - val_loss: 0.3261 - val_acc: 0.9065\n",
      "Epoch 18/30\n",
      "2500/2500 [==============================] - 0s 166us/sample - loss: 0.1986 - acc: 0.9388 - val_loss: 0.3334 - val_acc: 0.9024\n",
      "Epoch 19/30\n",
      "2500/2500 [==============================] - 0s 167us/sample - loss: 0.1869 - acc: 0.9428 - val_loss: 0.3192 - val_acc: 0.9081\n",
      "Epoch 20/30\n",
      "2500/2500 [==============================] - 0s 159us/sample - loss: 0.2296 - acc: 0.9352 - val_loss: 0.3554 - val_acc: 0.8988\n",
      "Epoch 21/30\n",
      "2500/2500 [==============================] - 0s 158us/sample - loss: 0.2136 - acc: 0.9412 - val_loss: 0.3731 - val_acc: 0.8955\n",
      "Epoch 22/30\n",
      "2500/2500 [==============================] - 0s 159us/sample - loss: 0.1969 - acc: 0.9420 - val_loss: 0.3457 - val_acc: 0.9012\n",
      "Epoch 23/30\n",
      "2500/2500 [==============================] - 0s 160us/sample - loss: 0.1861 - acc: 0.9420 - val_loss: 0.3663 - val_acc: 0.8948\n",
      "Epoch 24/30\n",
      "2500/2500 [==============================] - 0s 163us/sample - loss: 0.1738 - acc: 0.9500 - val_loss: 0.3536 - val_acc: 0.9035\n",
      "Epoch 25/30\n",
      "2500/2500 [==============================] - 0s 161us/sample - loss: 0.1873 - acc: 0.9384 - val_loss: 0.3345 - val_acc: 0.9077\n",
      "Epoch 26/30\n",
      "2500/2500 [==============================] - 0s 158us/sample - loss: 0.1846 - acc: 0.9416 - val_loss: 0.3332 - val_acc: 0.9069\n",
      "Epoch 27/30\n",
      "2500/2500 [==============================] - 0s 155us/sample - loss: 0.2071 - acc: 0.9356 - val_loss: 0.3473 - val_acc: 0.9030\n",
      "Epoch 28/30\n",
      "2500/2500 [==============================] - 0s 156us/sample - loss: 0.1598 - acc: 0.9532 - val_loss: 0.3274 - val_acc: 0.9084\n",
      "Epoch 29/30\n",
      "2500/2500 [==============================] - 0s 156us/sample - loss: 0.1539 - acc: 0.9552 - val_loss: 0.3179 - val_acc: 0.9128\n",
      "Epoch 30/30\n",
      "2500/2500 [==============================] - 0s 152us/sample - loss: 0.1487 - acc: 0.9556 - val_loss: 0.3288 - val_acc: 0.9104\n",
      "undersampling test loss:  0.32875758036151526\n",
      "undersampling accuracy:  0.9104\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_99 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_66 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_67 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_33 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "2176/2500 [=========================>....] - ETA: 0s - loss: 2.2070 - acc: 0.3244"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 2s 628us/sample - loss: 2.1267 - acc: 0.3448 - val_loss: 1.8420 - val_acc: 0.4671\n",
      "Epoch 2/30\n",
      "2500/2500 [==============================] - 0s 155us/sample - loss: 1.1181 - acc: 0.6212 - val_loss: 1.3695 - val_acc: 0.7401\n",
      "Epoch 3/30\n",
      "2500/2500 [==============================] - 0s 157us/sample - loss: 0.8244 - acc: 0.7284 - val_loss: 1.0135 - val_acc: 0.8224\n",
      "Epoch 4/30\n",
      "2500/2500 [==============================] - 0s 159us/sample - loss: 0.6729 - acc: 0.7868 - val_loss: 0.7274 - val_acc: 0.8577\n",
      "Epoch 5/30\n",
      "2500/2500 [==============================] - 0s 170us/sample - loss: 0.5624 - acc: 0.8324 - val_loss: 0.5625 - val_acc: 0.8749\n",
      "Epoch 6/30\n",
      "2500/2500 [==============================] - 0s 163us/sample - loss: 0.4920 - acc: 0.8488 - val_loss: 0.4514 - val_acc: 0.8870\n",
      "Epoch 7/30\n",
      "2500/2500 [==============================] - 0s 158us/sample - loss: 0.4498 - acc: 0.8724 - val_loss: 0.4078 - val_acc: 0.8917\n",
      "Epoch 8/30\n",
      "2500/2500 [==============================] - 0s 162us/sample - loss: 0.3883 - acc: 0.8844 - val_loss: 0.3780 - val_acc: 0.8933\n",
      "Epoch 9/30\n",
      "2500/2500 [==============================] - 0s 164us/sample - loss: 0.4042 - acc: 0.8788 - val_loss: 0.3872 - val_acc: 0.8832\n",
      "Epoch 10/30\n",
      "2500/2500 [==============================] - 0s 157us/sample - loss: 0.3579 - acc: 0.8920 - val_loss: 0.3622 - val_acc: 0.8921\n",
      "Epoch 11/30\n",
      "2500/2500 [==============================] - 0s 157us/sample - loss: 0.3367 - acc: 0.8996 - val_loss: 0.3536 - val_acc: 0.8936\n",
      "Epoch 12/30\n",
      "2500/2500 [==============================] - 0s 177us/sample - loss: 0.3120 - acc: 0.9072 - val_loss: 0.3503 - val_acc: 0.8978\n",
      "Epoch 13/30\n",
      "2500/2500 [==============================] - 0s 159us/sample - loss: 0.2745 - acc: 0.9204 - val_loss: 0.3382 - val_acc: 0.9005\n",
      "Epoch 14/30\n",
      "2500/2500 [==============================] - 0s 160us/sample - loss: 0.2599 - acc: 0.9256 - val_loss: 0.3297 - val_acc: 0.9036\n",
      "Epoch 15/30\n",
      "2500/2500 [==============================] - 0s 170us/sample - loss: 0.2684 - acc: 0.9256 - val_loss: 0.3700 - val_acc: 0.8908\n",
      "Epoch 16/30\n",
      "2500/2500 [==============================] - 0s 160us/sample - loss: 0.2433 - acc: 0.9332 - val_loss: 0.3493 - val_acc: 0.9000\n",
      "Epoch 17/30\n",
      "2500/2500 [==============================] - 0s 155us/sample - loss: 0.2388 - acc: 0.9276 - val_loss: 0.3408 - val_acc: 0.9002\n",
      "Epoch 18/30\n",
      "2500/2500 [==============================] - 0s 154us/sample - loss: 0.2190 - acc: 0.9336 - val_loss: 0.3271 - val_acc: 0.9019\n",
      "Epoch 19/30\n",
      "2500/2500 [==============================] - 0s 164us/sample - loss: 0.1967 - acc: 0.9460 - val_loss: 0.3371 - val_acc: 0.9004\n",
      "Epoch 20/30\n",
      "2500/2500 [==============================] - 0s 166us/sample - loss: 0.1987 - acc: 0.9400 - val_loss: 0.3252 - val_acc: 0.9063\n",
      "Epoch 21/30\n",
      "2500/2500 [==============================] - 0s 157us/sample - loss: 0.1876 - acc: 0.9436 - val_loss: 0.3373 - val_acc: 0.9041\n",
      "Epoch 22/30\n",
      "2500/2500 [==============================] - 0s 158us/sample - loss: 0.1787 - acc: 0.9468 - val_loss: 0.3322 - val_acc: 0.9061\n",
      "Epoch 23/30\n",
      "2500/2500 [==============================] - 0s 181us/sample - loss: 0.2468 - acc: 0.9252 - val_loss: 0.3677 - val_acc: 0.8963\n",
      "Epoch 24/30\n",
      "2500/2500 [==============================] - 0s 155us/sample - loss: 0.2135 - acc: 0.9332 - val_loss: 0.3333 - val_acc: 0.9045\n",
      "Epoch 25/30\n",
      "2500/2500 [==============================] - 0s 164us/sample - loss: 0.2271 - acc: 0.9324 - val_loss: 0.3660 - val_acc: 0.8986\n",
      "Epoch 26/30\n",
      "2500/2500 [==============================] - 0s 160us/sample - loss: 0.1833 - acc: 0.9452 - val_loss: 0.3374 - val_acc: 0.9037\n",
      "Epoch 27/30\n",
      "2500/2500 [==============================] - 0s 155us/sample - loss: 0.1584 - acc: 0.9504 - val_loss: 0.3184 - val_acc: 0.9119\n",
      "Epoch 28/30\n",
      "2500/2500 [==============================] - 0s 165us/sample - loss: 0.1267 - acc: 0.9644 - val_loss: 0.3133 - val_acc: 0.9128\n",
      "Epoch 29/30\n",
      "2500/2500 [==============================] - 0s 169us/sample - loss: 0.1407 - acc: 0.9656 - val_loss: 0.3228 - val_acc: 0.9117\n",
      "Epoch 30/30\n",
      "2500/2500 [==============================] - 0s 158us/sample - loss: 0.1343 - acc: 0.9624 - val_loss: 0.3463 - val_acc: 0.9028\n",
      "undersampling test loss:  0.3462519222155213\n",
      "undersampling accuracy:  0.9028\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_102 (Dense)            (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_68 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_69 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_34 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "2048/2500 [=======================>......] - ETA: 0s - loss: 2.2700 - acc: 0.3042"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 2s 719us/sample - loss: 2.1296 - acc: 0.3416 - val_loss: 1.8583 - val_acc: 0.4737\n",
      "Epoch 2/30\n",
      "2500/2500 [==============================] - 0s 173us/sample - loss: 1.1197 - acc: 0.6156 - val_loss: 1.4580 - val_acc: 0.7137\n",
      "Epoch 3/30\n",
      "2500/2500 [==============================] - 0s 196us/sample - loss: 0.8012 - acc: 0.7448 - val_loss: 1.0516 - val_acc: 0.8351\n",
      "Epoch 4/30\n",
      "2500/2500 [==============================] - 0s 187us/sample - loss: 0.6239 - acc: 0.8064 - val_loss: 0.7449 - val_acc: 0.8645\n",
      "Epoch 5/30\n",
      "2500/2500 [==============================] - 0s 173us/sample - loss: 0.5369 - acc: 0.8408 - val_loss: 0.5772 - val_acc: 0.8735\n",
      "Epoch 6/30\n",
      "2500/2500 [==============================] - 0s 163us/sample - loss: 0.4730 - acc: 0.8536 - val_loss: 0.4724 - val_acc: 0.8867\n",
      "Epoch 7/30\n",
      "2500/2500 [==============================] - 0s 171us/sample - loss: 0.4261 - acc: 0.8692 - val_loss: 0.3851 - val_acc: 0.8955\n",
      "Epoch 8/30\n",
      "2500/2500 [==============================] - 0s 189us/sample - loss: 0.3953 - acc: 0.8776 - val_loss: 0.3572 - val_acc: 0.8985\n",
      "Epoch 9/30\n",
      "2500/2500 [==============================] - 1s 212us/sample - loss: 0.3488 - acc: 0.8988 - val_loss: 0.3374 - val_acc: 0.9018\n",
      "Epoch 10/30\n",
      "2500/2500 [==============================] - 1s 257us/sample - loss: 0.3436 - acc: 0.9008 - val_loss: 0.3211 - val_acc: 0.9021\n",
      "Epoch 11/30\n",
      "2500/2500 [==============================] - 0s 194us/sample - loss: 0.2988 - acc: 0.9092 - val_loss: 0.3179 - val_acc: 0.9040\n",
      "Epoch 12/30\n",
      "2500/2500 [==============================] - 1s 215us/sample - loss: 0.2785 - acc: 0.9244 - val_loss: 0.3082 - val_acc: 0.9074\n",
      "Epoch 13/30\n",
      "2500/2500 [==============================] - 0s 186us/sample - loss: 0.2696 - acc: 0.9240 - val_loss: 0.2900 - val_acc: 0.9102\n",
      "Epoch 14/30\n",
      "2500/2500 [==============================] - 0s 194us/sample - loss: 0.2692 - acc: 0.9168 - val_loss: 0.3018 - val_acc: 0.9088\n",
      "Epoch 15/30\n",
      "2500/2500 [==============================] - 0s 176us/sample - loss: 0.2415 - acc: 0.9296 - val_loss: 0.2883 - val_acc: 0.9137\n",
      "Epoch 16/30\n",
      "2500/2500 [==============================] - 0s 176us/sample - loss: 0.2265 - acc: 0.9372 - val_loss: 0.2918 - val_acc: 0.9105\n",
      "Epoch 17/30\n",
      "2500/2500 [==============================] - 0s 165us/sample - loss: 0.2171 - acc: 0.9412 - val_loss: 0.3051 - val_acc: 0.9080\n",
      "Epoch 18/30\n",
      "2500/2500 [==============================] - 0s 164us/sample - loss: 0.1984 - acc: 0.9452 - val_loss: 0.2983 - val_acc: 0.9121\n",
      "Epoch 19/30\n",
      "2500/2500 [==============================] - 0s 156us/sample - loss: 0.2182 - acc: 0.9352 - val_loss: 0.3006 - val_acc: 0.9099\n",
      "Epoch 20/30\n",
      "2500/2500 [==============================] - 0s 159us/sample - loss: 0.1995 - acc: 0.9448 - val_loss: 0.2980 - val_acc: 0.9096\n",
      "Epoch 21/30\n",
      "2500/2500 [==============================] - 0s 154us/sample - loss: 0.1779 - acc: 0.9552 - val_loss: 0.2852 - val_acc: 0.9164\n",
      "Epoch 22/30\n",
      "2500/2500 [==============================] - 0s 156us/sample - loss: 0.1457 - acc: 0.9656 - val_loss: 0.2919 - val_acc: 0.9151\n",
      "Epoch 23/30\n",
      "2500/2500 [==============================] - 0s 158us/sample - loss: 0.1456 - acc: 0.9604 - val_loss: 0.2990 - val_acc: 0.9113\n",
      "Epoch 24/30\n",
      "2500/2500 [==============================] - 0s 169us/sample - loss: 0.1469 - acc: 0.9568 - val_loss: 0.2938 - val_acc: 0.9161\n",
      "Epoch 25/30\n",
      "2500/2500 [==============================] - 0s 174us/sample - loss: 0.1540 - acc: 0.9564 - val_loss: 0.2964 - val_acc: 0.9144\n",
      "Epoch 26/30\n",
      "2500/2500 [==============================] - 0s 162us/sample - loss: 0.1396 - acc: 0.9632 - val_loss: 0.2946 - val_acc: 0.9165\n",
      "Epoch 27/30\n",
      "2500/2500 [==============================] - 0s 159us/sample - loss: 0.1402 - acc: 0.9552 - val_loss: 0.3193 - val_acc: 0.9083\n",
      "Epoch 28/30\n",
      "2500/2500 [==============================] - 0s 159us/sample - loss: 0.1143 - acc: 0.9708 - val_loss: 0.3071 - val_acc: 0.9113\n",
      "Epoch 29/30\n",
      "2500/2500 [==============================] - 0s 157us/sample - loss: 0.1135 - acc: 0.9700 - val_loss: 0.3037 - val_acc: 0.9131\n",
      "Epoch 30/30\n",
      "2500/2500 [==============================] - 0s 154us/sample - loss: 0.1108 - acc: 0.9712 - val_loss: 0.3195 - val_acc: 0.9096\n",
      "undersampling test loss:  0.31952516957968474\n",
      "undersampling accuracy:  0.9096\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_105 (Dense)            (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_70 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_71 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_35 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "2048/2500 [=======================>......] - ETA: 0s - loss: 2.1650 - acc: 0.3359"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 2s 670us/sample - loss: 2.0049 - acc: 0.3784 - val_loss: 1.7380 - val_acc: 0.6092\n",
      "Epoch 2/30\n",
      "2500/2500 [==============================] - 0s 183us/sample - loss: 1.0793 - acc: 0.6516 - val_loss: 1.3327 - val_acc: 0.7851\n",
      "Epoch 3/30\n",
      "2500/2500 [==============================] - 0s 171us/sample - loss: 0.7972 - acc: 0.7484 - val_loss: 0.9749 - val_acc: 0.8378\n",
      "Epoch 4/30\n",
      "2500/2500 [==============================] - 0s 170us/sample - loss: 0.6713 - acc: 0.7864 - val_loss: 0.7138 - val_acc: 0.8617\n",
      "Epoch 5/30\n",
      "2500/2500 [==============================] - 1s 209us/sample - loss: 0.5628 - acc: 0.8224 - val_loss: 0.5638 - val_acc: 0.8762\n",
      "Epoch 6/30\n",
      "2500/2500 [==============================] - 0s 173us/sample - loss: 0.4999 - acc: 0.8440 - val_loss: 0.4762 - val_acc: 0.8793\n",
      "Epoch 7/30\n",
      "2500/2500 [==============================] - 0s 172us/sample - loss: 0.4688 - acc: 0.8676 - val_loss: 0.3984 - val_acc: 0.8897\n",
      "Epoch 8/30\n",
      "2500/2500 [==============================] - 0s 171us/sample - loss: 0.4008 - acc: 0.8776 - val_loss: 0.3750 - val_acc: 0.8905\n",
      "Epoch 9/30\n",
      "2500/2500 [==============================] - 0s 174us/sample - loss: 0.3753 - acc: 0.8888 - val_loss: 0.3521 - val_acc: 0.8979\n",
      "Epoch 10/30\n",
      "2500/2500 [==============================] - 0s 185us/sample - loss: 0.3347 - acc: 0.8956 - val_loss: 0.3356 - val_acc: 0.8998\n",
      "Epoch 11/30\n",
      "2500/2500 [==============================] - 0s 160us/sample - loss: 0.3149 - acc: 0.9084 - val_loss: 0.3256 - val_acc: 0.9047\n",
      "Epoch 12/30\n",
      "2500/2500 [==============================] - 0s 154us/sample - loss: 0.2939 - acc: 0.9088 - val_loss: 0.3380 - val_acc: 0.9004\n",
      "Epoch 13/30\n",
      "2500/2500 [==============================] - 0s 162us/sample - loss: 0.2711 - acc: 0.9184 - val_loss: 0.3310 - val_acc: 0.9036\n",
      "Epoch 14/30\n",
      "2500/2500 [==============================] - 1s 201us/sample - loss: 0.2541 - acc: 0.9276 - val_loss: 0.3150 - val_acc: 0.9083\n",
      "Epoch 15/30\n",
      "2500/2500 [==============================] - 0s 191us/sample - loss: 0.2544 - acc: 0.9224 - val_loss: 0.3101 - val_acc: 0.9075\n",
      "Epoch 16/30\n",
      "2500/2500 [==============================] - 0s 175us/sample - loss: 0.2454 - acc: 0.9264 - val_loss: 0.3220 - val_acc: 0.9055\n",
      "Epoch 17/30\n",
      "2500/2500 [==============================] - 0s 173us/sample - loss: 0.2391 - acc: 0.9284 - val_loss: 0.3135 - val_acc: 0.9082\n",
      "Epoch 18/30\n",
      "2500/2500 [==============================] - 0s 163us/sample - loss: 0.2071 - acc: 0.9452 - val_loss: 0.3045 - val_acc: 0.9107\n",
      "Epoch 19/30\n",
      "2500/2500 [==============================] - 0s 161us/sample - loss: 0.1838 - acc: 0.9500 - val_loss: 0.2969 - val_acc: 0.9110\n",
      "Epoch 20/30\n",
      "2500/2500 [==============================] - 0s 160us/sample - loss: 0.1923 - acc: 0.9432 - val_loss: 0.3136 - val_acc: 0.9061\n",
      "Epoch 21/30\n",
      "2500/2500 [==============================] - 0s 162us/sample - loss: 0.1944 - acc: 0.9428 - val_loss: 0.3412 - val_acc: 0.8958\n",
      "Epoch 22/30\n",
      "2500/2500 [==============================] - 0s 169us/sample - loss: 0.1908 - acc: 0.9432 - val_loss: 0.2988 - val_acc: 0.9120\n",
      "Epoch 23/30\n",
      "2500/2500 [==============================] - 0s 171us/sample - loss: 0.1836 - acc: 0.9464 - val_loss: 0.3019 - val_acc: 0.9090\n",
      "Epoch 24/30\n",
      "2500/2500 [==============================] - 0s 165us/sample - loss: 0.1580 - acc: 0.9580 - val_loss: 0.3019 - val_acc: 0.9109\n",
      "Epoch 25/30\n",
      "2500/2500 [==============================] - 0s 162us/sample - loss: 0.1594 - acc: 0.9572 - val_loss: 0.3142 - val_acc: 0.9111\n",
      "Epoch 26/30\n",
      "2500/2500 [==============================] - 0s 161us/sample - loss: 0.1450 - acc: 0.9572 - val_loss: 0.3071 - val_acc: 0.9132\n",
      "Epoch 27/30\n",
      "2500/2500 [==============================] - 0s 172us/sample - loss: 0.1694 - acc: 0.9464 - val_loss: 0.3203 - val_acc: 0.9105\n",
      "Epoch 28/30\n",
      "2500/2500 [==============================] - 0s 169us/sample - loss: 0.1652 - acc: 0.9528 - val_loss: 0.3217 - val_acc: 0.9101\n",
      "Epoch 29/30\n",
      "2500/2500 [==============================] - 0s 165us/sample - loss: 0.1257 - acc: 0.9624 - val_loss: 0.3183 - val_acc: 0.9115\n",
      "Epoch 30/30\n",
      "2500/2500 [==============================] - 0s 160us/sample - loss: 0.1337 - acc: 0.9584 - val_loss: 0.3106 - val_acc: 0.9131\n",
      "undersampling test loss:  0.31058369831740856\n",
      "undersampling accuracy:  0.9131\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_108 (Dense)            (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_72 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_73 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_36 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "1792/2500 [====================>.........] - ETA: 0s - loss: 2.4183 - acc: 0.2779"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 2s 799us/sample - loss: 2.1981 - acc: 0.3360 - val_loss: 1.9242 - val_acc: 0.5963\n",
      "Epoch 2/30\n",
      "2500/2500 [==============================] - 1s 277us/sample - loss: 1.2093 - acc: 0.6104 - val_loss: 1.5194 - val_acc: 0.7444\n",
      "Epoch 3/30\n",
      "2500/2500 [==============================] - 0s 194us/sample - loss: 0.8851 - acc: 0.7136 - val_loss: 1.1499 - val_acc: 0.8112\n",
      "Epoch 4/30\n",
      "2500/2500 [==============================] - 0s 181us/sample - loss: 0.7031 - acc: 0.7824 - val_loss: 0.8625 - val_acc: 0.8530\n",
      "Epoch 5/30\n",
      "2500/2500 [==============================] - 0s 194us/sample - loss: 0.6067 - acc: 0.8188 - val_loss: 0.6308 - val_acc: 0.8759\n",
      "Epoch 6/30\n",
      "2500/2500 [==============================] - 1s 219us/sample - loss: 0.5164 - acc: 0.8484 - val_loss: 0.5094 - val_acc: 0.8835\n",
      "Epoch 7/30\n",
      "2500/2500 [==============================] - 1s 221us/sample - loss: 0.4615 - acc: 0.8648 - val_loss: 0.4183 - val_acc: 0.8951\n",
      "Epoch 8/30\n",
      "2500/2500 [==============================] - 1s 236us/sample - loss: 0.4101 - acc: 0.8768 - val_loss: 0.3645 - val_acc: 0.8989\n",
      "Epoch 9/30\n",
      "2500/2500 [==============================] - 1s 225us/sample - loss: 0.3911 - acc: 0.8840 - val_loss: 0.3478 - val_acc: 0.8999\n",
      "Epoch 10/30\n",
      "2500/2500 [==============================] - 0s 179us/sample - loss: 0.3482 - acc: 0.9028 - val_loss: 0.3324 - val_acc: 0.9021\n",
      "Epoch 11/30\n",
      "2500/2500 [==============================] - 0s 163us/sample - loss: 0.3209 - acc: 0.9028 - val_loss: 0.3035 - val_acc: 0.9114\n",
      "Epoch 12/30\n",
      "2500/2500 [==============================] - 0s 179us/sample - loss: 0.2997 - acc: 0.9116 - val_loss: 0.3075 - val_acc: 0.9058\n",
      "Epoch 13/30\n",
      "2500/2500 [==============================] - 0s 182us/sample - loss: 0.2905 - acc: 0.9116 - val_loss: 0.2898 - val_acc: 0.9122\n",
      "Epoch 14/30\n",
      "2500/2500 [==============================] - 0s 197us/sample - loss: 0.2580 - acc: 0.9304 - val_loss: 0.2882 - val_acc: 0.9111\n",
      "Epoch 15/30\n",
      "2500/2500 [==============================] - 0s 189us/sample - loss: 0.2446 - acc: 0.9328 - val_loss: 0.2827 - val_acc: 0.9146\n",
      "Epoch 16/30\n",
      "2500/2500 [==============================] - 1s 240us/sample - loss: 0.2284 - acc: 0.9352 - val_loss: 0.2843 - val_acc: 0.9143\n",
      "Epoch 17/30\n",
      "2500/2500 [==============================] - 1s 231us/sample - loss: 0.1987 - acc: 0.9508 - val_loss: 0.2788 - val_acc: 0.9162\n",
      "Epoch 18/30\n",
      "2500/2500 [==============================] - 0s 191us/sample - loss: 0.1871 - acc: 0.9448 - val_loss: 0.2849 - val_acc: 0.9139\n",
      "Epoch 19/30\n",
      "2500/2500 [==============================] - 0s 161us/sample - loss: 0.2322 - acc: 0.9344 - val_loss: 0.3348 - val_acc: 0.9034\n",
      "Epoch 20/30\n",
      "2500/2500 [==============================] - 0s 164us/sample - loss: 0.1994 - acc: 0.9460 - val_loss: 0.3142 - val_acc: 0.9097\n",
      "Epoch 21/30\n",
      "2500/2500 [==============================] - 0s 160us/sample - loss: 0.2049 - acc: 0.9332 - val_loss: 0.2984 - val_acc: 0.9122\n",
      "Epoch 22/30\n",
      "2500/2500 [==============================] - 0s 168us/sample - loss: 0.1651 - acc: 0.9548 - val_loss: 0.3035 - val_acc: 0.9127\n",
      "Epoch 23/30\n",
      "2500/2500 [==============================] - 0s 162us/sample - loss: 0.1632 - acc: 0.9556 - val_loss: 0.2963 - val_acc: 0.9146\n",
      "Epoch 24/30\n",
      "2500/2500 [==============================] - 0s 164us/sample - loss: 0.1592 - acc: 0.9524 - val_loss: 0.3035 - val_acc: 0.9128\n",
      "Epoch 25/30\n",
      "2500/2500 [==============================] - 0s 176us/sample - loss: 0.1537 - acc: 0.9564 - val_loss: 0.3032 - val_acc: 0.9135\n",
      "Epoch 26/30\n",
      "2500/2500 [==============================] - 0s 158us/sample - loss: 0.1545 - acc: 0.9540 - val_loss: 0.3131 - val_acc: 0.9073\n",
      "Epoch 27/30\n",
      "2500/2500 [==============================] - 0s 160us/sample - loss: 0.1546 - acc: 0.9512 - val_loss: 0.3111 - val_acc: 0.9120\n",
      "Epoch 28/30\n",
      "2500/2500 [==============================] - 0s 161us/sample - loss: 0.1228 - acc: 0.9628 - val_loss: 0.3090 - val_acc: 0.9131\n",
      "Epoch 29/30\n",
      "2500/2500 [==============================] - 0s 162us/sample - loss: 0.1267 - acc: 0.9628 - val_loss: 0.3117 - val_acc: 0.9111\n",
      "Epoch 30/30\n",
      "2500/2500 [==============================] - 0s 160us/sample - loss: 0.1385 - acc: 0.9564 - val_loss: 0.3232 - val_acc: 0.9083\n",
      "undersampling test loss:  0.3231779220841825\n",
      "undersampling accuracy:  0.9083\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_111 (Dense)            (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_74 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_75 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_37 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "2240/2500 [=========================>....] - ETA: 0s - loss: 2.2519 - acc: 0.3129"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 2s 626us/sample - loss: 2.1637 - acc: 0.3384 - val_loss: 1.8132 - val_acc: 0.6802\n",
      "Epoch 2/30\n",
      "2500/2500 [==============================] - 0s 160us/sample - loss: 1.1288 - acc: 0.6384 - val_loss: 1.4083 - val_acc: 0.7777\n",
      "Epoch 3/30\n",
      "2500/2500 [==============================] - 0s 166us/sample - loss: 0.8259 - acc: 0.7344 - val_loss: 1.0490 - val_acc: 0.8285\n",
      "Epoch 4/30\n",
      "2500/2500 [==============================] - 0s 164us/sample - loss: 0.6853 - acc: 0.7920 - val_loss: 0.7787 - val_acc: 0.8477\n",
      "Epoch 5/30\n",
      "2500/2500 [==============================] - 0s 161us/sample - loss: 0.6029 - acc: 0.8204 - val_loss: 0.6198 - val_acc: 0.8614\n",
      "Epoch 6/30\n",
      "2500/2500 [==============================] - 0s 166us/sample - loss: 0.5444 - acc: 0.8380 - val_loss: 0.5056 - val_acc: 0.8709\n",
      "Epoch 7/30\n",
      "2500/2500 [==============================] - 0s 164us/sample - loss: 0.4983 - acc: 0.8524 - val_loss: 0.4557 - val_acc: 0.8751\n",
      "Epoch 8/30\n",
      "2500/2500 [==============================] - 0s 180us/sample - loss: 0.4641 - acc: 0.8624 - val_loss: 0.4105 - val_acc: 0.8816\n",
      "Epoch 9/30\n",
      "2500/2500 [==============================] - 0s 162us/sample - loss: 0.4281 - acc: 0.8752 - val_loss: 0.3805 - val_acc: 0.8928\n",
      "Epoch 10/30\n",
      "2500/2500 [==============================] - 0s 165us/sample - loss: 0.3820 - acc: 0.8924 - val_loss: 0.3675 - val_acc: 0.8940\n",
      "Epoch 11/30\n",
      "2500/2500 [==============================] - 0s 165us/sample - loss: 0.3526 - acc: 0.9016 - val_loss: 0.3540 - val_acc: 0.8970\n",
      "Epoch 12/30\n",
      "2500/2500 [==============================] - 0s 164us/sample - loss: 0.3235 - acc: 0.9064 - val_loss: 0.3445 - val_acc: 0.8980\n",
      "Epoch 13/30\n",
      "2500/2500 [==============================] - 0s 197us/sample - loss: 0.3014 - acc: 0.9120 - val_loss: 0.3443 - val_acc: 0.8987\n",
      "Epoch 14/30\n",
      "2500/2500 [==============================] - 0s 165us/sample - loss: 0.2647 - acc: 0.9256 - val_loss: 0.3453 - val_acc: 0.8979\n",
      "Epoch 15/30\n",
      "2500/2500 [==============================] - 0s 165us/sample - loss: 0.2470 - acc: 0.9352 - val_loss: 0.3459 - val_acc: 0.8993\n",
      "Epoch 16/30\n",
      "2500/2500 [==============================] - 0s 167us/sample - loss: 0.2425 - acc: 0.9376 - val_loss: 0.3430 - val_acc: 0.8988\n",
      "Epoch 17/30\n",
      "2500/2500 [==============================] - 0s 168us/sample - loss: 0.2612 - acc: 0.9212 - val_loss: 0.3761 - val_acc: 0.8874\n",
      "Epoch 18/30\n",
      "2500/2500 [==============================] - 0s 167us/sample - loss: 0.2469 - acc: 0.9276 - val_loss: 0.3366 - val_acc: 0.9010\n",
      "Epoch 19/30\n",
      "2500/2500 [==============================] - 0s 165us/sample - loss: 0.2274 - acc: 0.9380 - val_loss: 0.3421 - val_acc: 0.9005\n",
      "Epoch 20/30\n",
      "2500/2500 [==============================] - 0s 166us/sample - loss: 0.2226 - acc: 0.9336 - val_loss: 0.3497 - val_acc: 0.8993\n",
      "Epoch 21/30\n",
      "2500/2500 [==============================] - 0s 164us/sample - loss: 0.2115 - acc: 0.9376 - val_loss: 0.3525 - val_acc: 0.8963\n",
      "Epoch 22/30\n",
      "2500/2500 [==============================] - 0s 165us/sample - loss: 0.2047 - acc: 0.9400 - val_loss: 0.3400 - val_acc: 0.9012\n",
      "Epoch 23/30\n",
      "2500/2500 [==============================] - 0s 174us/sample - loss: 0.2048 - acc: 0.9360 - val_loss: 0.3271 - val_acc: 0.9072\n",
      "Epoch 24/30\n",
      "2500/2500 [==============================] - 0s 166us/sample - loss: 0.1797 - acc: 0.9492 - val_loss: 0.3292 - val_acc: 0.9090\n",
      "Epoch 25/30\n",
      "2500/2500 [==============================] - 0s 164us/sample - loss: 0.1585 - acc: 0.9564 - val_loss: 0.3273 - val_acc: 0.9064\n",
      "Epoch 26/30\n",
      "2500/2500 [==============================] - 0s 165us/sample - loss: 0.1587 - acc: 0.9592 - val_loss: 0.3327 - val_acc: 0.9051\n",
      "Epoch 27/30\n",
      "2500/2500 [==============================] - 0s 163us/sample - loss: 0.1585 - acc: 0.9560 - val_loss: 0.3450 - val_acc: 0.9053\n",
      "Epoch 28/30\n",
      "2500/2500 [==============================] - 0s 166us/sample - loss: 0.1803 - acc: 0.9492 - val_loss: 0.3602 - val_acc: 0.9029\n",
      "Epoch 29/30\n",
      "2500/2500 [==============================] - 0s 165us/sample - loss: 0.1414 - acc: 0.9580 - val_loss: 0.3641 - val_acc: 0.9034\n",
      "Epoch 30/30\n",
      "2500/2500 [==============================] - 0s 166us/sample - loss: 0.1612 - acc: 0.9460 - val_loss: 0.3884 - val_acc: 0.8951\n",
      "undersampling test loss:  0.3884236992061138\n",
      "undersampling accuracy:  0.8951\n",
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_114 (Dense)            (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_76 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_77 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_38 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "2176/2500 [=========================>....] - ETA: 0s - loss: 2.3953 - acc: 0.2872"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 2s 634us/sample - loss: 2.2872 - acc: 0.3056 - val_loss: 1.9140 - val_acc: 0.5326\n",
      "Epoch 2/30\n",
      "2500/2500 [==============================] - 0s 168us/sample - loss: 1.2162 - acc: 0.5964 - val_loss: 1.5020 - val_acc: 0.7490\n",
      "Epoch 3/30\n",
      "2500/2500 [==============================] - 0s 167us/sample - loss: 0.8952 - acc: 0.7168 - val_loss: 1.1053 - val_acc: 0.8274\n",
      "Epoch 4/30\n",
      "2500/2500 [==============================] - 0s 165us/sample - loss: 0.7072 - acc: 0.7844 - val_loss: 0.8181 - val_acc: 0.8529\n",
      "Epoch 5/30\n",
      "2500/2500 [==============================] - 0s 166us/sample - loss: 0.5929 - acc: 0.8200 - val_loss: 0.6091 - val_acc: 0.8690\n",
      "Epoch 6/30\n",
      "2500/2500 [==============================] - 0s 166us/sample - loss: 0.5156 - acc: 0.8460 - val_loss: 0.4784 - val_acc: 0.8831\n",
      "Epoch 7/30\n",
      "2500/2500 [==============================] - 0s 165us/sample - loss: 0.4353 - acc: 0.8652 - val_loss: 0.4073 - val_acc: 0.8934\n",
      "Epoch 8/30\n",
      "2500/2500 [==============================] - 0s 168us/sample - loss: 0.3865 - acc: 0.8840 - val_loss: 0.3733 - val_acc: 0.8948\n",
      "Epoch 9/30\n",
      "2500/2500 [==============================] - 0s 169us/sample - loss: 0.3547 - acc: 0.8988 - val_loss: 0.3501 - val_acc: 0.9014\n",
      "Epoch 10/30\n",
      "2500/2500 [==============================] - 0s 167us/sample - loss: 0.3228 - acc: 0.9060 - val_loss: 0.3361 - val_acc: 0.9008\n",
      "Epoch 11/30\n",
      "2500/2500 [==============================] - 0s 169us/sample - loss: 0.2960 - acc: 0.9108 - val_loss: 0.3293 - val_acc: 0.9008\n",
      "Epoch 12/30\n",
      "2500/2500 [==============================] - 0s 167us/sample - loss: 0.2826 - acc: 0.9208 - val_loss: 0.3194 - val_acc: 0.9052\n",
      "Epoch 13/30\n",
      "2500/2500 [==============================] - 0s 168us/sample - loss: 0.2431 - acc: 0.9368 - val_loss: 0.3175 - val_acc: 0.9043\n",
      "Epoch 14/30\n",
      "2500/2500 [==============================] - 0s 168us/sample - loss: 0.2573 - acc: 0.9240 - val_loss: 0.3279 - val_acc: 0.9020\n",
      "Epoch 15/30\n",
      "2500/2500 [==============================] - 0s 169us/sample - loss: 0.2233 - acc: 0.9412 - val_loss: 0.3431 - val_acc: 0.8965\n",
      "Epoch 16/30\n",
      "2500/2500 [==============================] - 0s 167us/sample - loss: 0.2243 - acc: 0.9308 - val_loss: 0.3170 - val_acc: 0.9057\n",
      "Epoch 17/30\n",
      "2500/2500 [==============================] - 0s 169us/sample - loss: 0.2109 - acc: 0.9396 - val_loss: 0.3253 - val_acc: 0.9035\n",
      "Epoch 18/30\n",
      "2500/2500 [==============================] - 0s 168us/sample - loss: 0.1872 - acc: 0.9472 - val_loss: 0.3275 - val_acc: 0.9049\n",
      "Epoch 19/30\n",
      "2500/2500 [==============================] - 0s 169us/sample - loss: 0.1983 - acc: 0.9448 - val_loss: 0.3186 - val_acc: 0.9098\n",
      "Epoch 20/30\n",
      "2500/2500 [==============================] - 0s 167us/sample - loss: 0.1682 - acc: 0.9572 - val_loss: 0.3197 - val_acc: 0.9054\n",
      "Epoch 21/30\n",
      "2500/2500 [==============================] - 0s 168us/sample - loss: 0.1577 - acc: 0.9508 - val_loss: 0.3137 - val_acc: 0.9087\n",
      "Epoch 22/30\n",
      "2500/2500 [==============================] - 0s 167us/sample - loss: 0.1436 - acc: 0.9604 - val_loss: 0.3117 - val_acc: 0.9105\n",
      "Epoch 23/30\n",
      "2500/2500 [==============================] - 0s 168us/sample - loss: 0.1260 - acc: 0.9660 - val_loss: 0.3156 - val_acc: 0.9098\n",
      "Epoch 24/30\n",
      "2500/2500 [==============================] - 0s 168us/sample - loss: 0.1195 - acc: 0.9712 - val_loss: 0.3338 - val_acc: 0.9060\n",
      "Epoch 25/30\n",
      "2500/2500 [==============================] - 0s 167us/sample - loss: 0.1154 - acc: 0.9708 - val_loss: 0.3295 - val_acc: 0.9076\n",
      "Epoch 26/30\n",
      "2500/2500 [==============================] - 0s 169us/sample - loss: 0.1253 - acc: 0.9696 - val_loss: 0.3351 - val_acc: 0.9080\n",
      "Epoch 27/30\n",
      "2500/2500 [==============================] - 0s 169us/sample - loss: 0.1132 - acc: 0.9736 - val_loss: 0.3637 - val_acc: 0.8984\n",
      "Epoch 28/30\n",
      "2500/2500 [==============================] - 0s 171us/sample - loss: 0.1298 - acc: 0.9600 - val_loss: 0.3450 - val_acc: 0.9051\n",
      "Epoch 29/30\n",
      "2500/2500 [==============================] - 0s 168us/sample - loss: 0.1012 - acc: 0.9720 - val_loss: 0.3474 - val_acc: 0.9064\n",
      "Epoch 30/30\n",
      "2500/2500 [==============================] - 0s 167us/sample - loss: 0.1227 - acc: 0.9644 - val_loss: 0.3425 - val_acc: 0.9062\n",
      "undersampling test loss:  0.3425176846351475\n",
      "undersampling accuracy:  0.9062\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_117 (Dense)            (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_78 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_79 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_39 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "2112/2500 [========================>.....] - ETA: 0s - loss: 2.1573 - acc: 0.3281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 2s 650us/sample - loss: 2.0604 - acc: 0.3604 - val_loss: 1.7823 - val_acc: 0.5810\n",
      "Epoch 2/30\n",
      "2500/2500 [==============================] - 0s 168us/sample - loss: 1.1133 - acc: 0.6356 - val_loss: 1.4027 - val_acc: 0.7176\n",
      "Epoch 3/30\n",
      "2500/2500 [==============================] - 0s 173us/sample - loss: 0.8310 - acc: 0.7408 - val_loss: 1.0509 - val_acc: 0.8162\n",
      "Epoch 4/30\n",
      "2500/2500 [==============================] - 0s 170us/sample - loss: 0.6746 - acc: 0.7948 - val_loss: 0.7783 - val_acc: 0.8537\n",
      "Epoch 5/30\n",
      "2500/2500 [==============================] - 0s 169us/sample - loss: 0.5854 - acc: 0.8224 - val_loss: 0.5865 - val_acc: 0.8768\n",
      "Epoch 6/30\n",
      "2500/2500 [==============================] - 0s 166us/sample - loss: 0.4907 - acc: 0.8492 - val_loss: 0.4924 - val_acc: 0.8817\n",
      "Epoch 7/30\n",
      "2500/2500 [==============================] - 0s 166us/sample - loss: 0.4582 - acc: 0.8612 - val_loss: 0.4279 - val_acc: 0.8900\n",
      "Epoch 8/30\n",
      "2500/2500 [==============================] - 0s 170us/sample - loss: 0.3957 - acc: 0.8852 - val_loss: 0.3789 - val_acc: 0.8973\n",
      "Epoch 9/30\n",
      "2500/2500 [==============================] - 0s 173us/sample - loss: 0.3551 - acc: 0.8960 - val_loss: 0.3628 - val_acc: 0.8942\n",
      "Epoch 10/30\n",
      "2500/2500 [==============================] - 0s 167us/sample - loss: 0.3314 - acc: 0.9040 - val_loss: 0.3283 - val_acc: 0.9028\n",
      "Epoch 11/30\n",
      "2500/2500 [==============================] - 0s 173us/sample - loss: 0.3118 - acc: 0.9048 - val_loss: 0.3165 - val_acc: 0.9061\n",
      "Epoch 12/30\n",
      "2500/2500 [==============================] - 0s 170us/sample - loss: 0.2882 - acc: 0.9184 - val_loss: 0.3153 - val_acc: 0.9060\n",
      "Epoch 13/30\n",
      "2500/2500 [==============================] - 0s 170us/sample - loss: 0.2792 - acc: 0.9204 - val_loss: 0.3213 - val_acc: 0.9050\n",
      "Epoch 14/30\n",
      "2500/2500 [==============================] - 0s 169us/sample - loss: 0.2344 - acc: 0.9296 - val_loss: 0.3046 - val_acc: 0.9088\n",
      "Epoch 15/30\n",
      "2500/2500 [==============================] - 0s 172us/sample - loss: 0.2178 - acc: 0.9416 - val_loss: 0.3042 - val_acc: 0.9079\n",
      "Epoch 16/30\n",
      "2500/2500 [==============================] - 0s 172us/sample - loss: 0.2354 - acc: 0.9300 - val_loss: 0.3040 - val_acc: 0.9089\n",
      "Epoch 17/30\n",
      "2500/2500 [==============================] - 0s 171us/sample - loss: 0.2179 - acc: 0.9332 - val_loss: 0.3155 - val_acc: 0.9062\n",
      "Epoch 18/30\n",
      "2500/2500 [==============================] - 0s 171us/sample - loss: 0.2319 - acc: 0.9340 - val_loss: 0.3078 - val_acc: 0.9095\n",
      "Epoch 19/30\n",
      "2500/2500 [==============================] - 0s 170us/sample - loss: 0.2228 - acc: 0.9344 - val_loss: 0.3197 - val_acc: 0.9065\n",
      "Epoch 20/30\n",
      "2500/2500 [==============================] - 0s 173us/sample - loss: 0.2316 - acc: 0.9336 - val_loss: 0.3208 - val_acc: 0.9062\n",
      "Epoch 21/30\n",
      "2500/2500 [==============================] - 0s 171us/sample - loss: 0.2070 - acc: 0.9364 - val_loss: 0.3235 - val_acc: 0.9053\n",
      "Epoch 22/30\n",
      "2500/2500 [==============================] - 0s 169us/sample - loss: 0.1946 - acc: 0.9456 - val_loss: 0.3403 - val_acc: 0.9017\n",
      "Epoch 23/30\n",
      "2500/2500 [==============================] - 0s 168us/sample - loss: 0.2213 - acc: 0.9340 - val_loss: 0.3545 - val_acc: 0.8975\n",
      "Epoch 24/30\n",
      "2500/2500 [==============================] - 0s 171us/sample - loss: 0.1861 - acc: 0.9512 - val_loss: 0.3130 - val_acc: 0.9094\n",
      "Epoch 25/30\n",
      "2500/2500 [==============================] - 0s 171us/sample - loss: 0.1912 - acc: 0.9464 - val_loss: 0.3603 - val_acc: 0.8938\n",
      "Epoch 26/30\n",
      "2500/2500 [==============================] - 0s 171us/sample - loss: 0.1836 - acc: 0.9432 - val_loss: 0.3212 - val_acc: 0.9082\n",
      "Epoch 27/30\n",
      "2500/2500 [==============================] - 1s 200us/sample - loss: 0.1599 - acc: 0.9560 - val_loss: 0.3069 - val_acc: 0.9118\n",
      "Epoch 28/30\n",
      "2500/2500 [==============================] - 0s 199us/sample - loss: 0.1376 - acc: 0.9632 - val_loss: 0.3096 - val_acc: 0.9124\n",
      "Epoch 29/30\n",
      "2500/2500 [==============================] - 0s 198us/sample - loss: 0.1612 - acc: 0.9540 - val_loss: 0.3352 - val_acc: 0.9061\n",
      "Epoch 30/30\n",
      "2500/2500 [==============================] - 0s 198us/sample - loss: 0.1626 - acc: 0.9496 - val_loss: 0.3216 - val_acc: 0.9122\n",
      "undersampling test loss:  0.32157239745259286\n",
      "undersampling accuracy:  0.9122\n"
     ]
    }
   ],
   "source": [
    "undersampling_list =[]\n",
    "for i in range(10):\n",
    "    bl_model = build_model()\n",
    "    batch_size=64\n",
    "    epochs=30\n",
    "    \n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_train, y_train1 = rus.fit_resample(train_x, train_y)\n",
    "    \n",
    "    bl_history = bl_model.fit(X_train, y_train1, batch_size=batch_size,\n",
    "                        epochs=epochs, validation_data=(test_images, test_y))\n",
    "\n",
    "    bl_score = bl_model.evaluate(test_images, test_y, verbose=0)\n",
    "    print('undersampling test loss: ', bl_score[0])\n",
    "    print('undersampling accuracy: ', bl_score[1] )\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    y_pred_oh = bl_model.predict(test_images)\n",
    "    y_pred_baseline = y_pred_oh.argmax(axis=-1)\n",
    "    undersampling_list.append(classification_report(test_labels, y_pred_baseline, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b78c4c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of runs: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.938053</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.948713</td>\n",
       "      <td>980.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.968414</td>\n",
       "      <td>0.965991</td>\n",
       "      <td>0.967101</td>\n",
       "      <td>1135.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.921075</td>\n",
       "      <td>0.874128</td>\n",
       "      <td>0.896311</td>\n",
       "      <td>1032.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.891500</td>\n",
       "      <td>0.884653</td>\n",
       "      <td>0.887797</td>\n",
       "      <td>1010.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.886977</td>\n",
       "      <td>0.920570</td>\n",
       "      <td>0.902655</td>\n",
       "      <td>982.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.877994</td>\n",
       "      <td>0.868049</td>\n",
       "      <td>0.872619</td>\n",
       "      <td>892.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.905320</td>\n",
       "      <td>0.944885</td>\n",
       "      <td>0.924225</td>\n",
       "      <td>958.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.931264</td>\n",
       "      <td>0.895039</td>\n",
       "      <td>0.912391</td>\n",
       "      <td>1028.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.881879</td>\n",
       "      <td>0.832546</td>\n",
       "      <td>0.854881</td>\n",
       "      <td>974.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.855928</td>\n",
       "      <td>0.896531</td>\n",
       "      <td>0.873564</td>\n",
       "      <td>1009.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.905190</td>\n",
       "      <td>0.905190</td>\n",
       "      <td>0.905190</td>\n",
       "      <td>0.90519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.905840</td>\n",
       "      <td>0.904239</td>\n",
       "      <td>0.904026</td>\n",
       "      <td>10000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.907081</td>\n",
       "      <td>0.905190</td>\n",
       "      <td>0.905128</td>\n",
       "      <td>10000.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.938053  0.960000  0.948713    980.00000\n",
       "1              0.968414  0.965991  0.967101   1135.00000\n",
       "2              0.921075  0.874128  0.896311   1032.00000\n",
       "3              0.891500  0.884653  0.887797   1010.00000\n",
       "4              0.886977  0.920570  0.902655    982.00000\n",
       "5              0.877994  0.868049  0.872619    892.00000\n",
       "6              0.905320  0.944885  0.924225    958.00000\n",
       "7              0.931264  0.895039  0.912391   1028.00000\n",
       "8              0.881879  0.832546  0.854881    974.00000\n",
       "9              0.855928  0.896531  0.873564   1009.00000\n",
       "accuracy       0.905190  0.905190  0.905190      0.90519\n",
       "macro avg      0.905840  0.904239  0.904026  10000.00000\n",
       "weighted avg   0.907081  0.905190  0.905128  10000.00000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_process_results(undersampling_list, 'results_csv/mnist_undersampling.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322faf9",
   "metadata": {},
   "source": [
    "# Random OverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20f7fc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_120 (Dense)            (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_80 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_81 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_40 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 62650 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "61760/62650 [============================>.] - ETA: 0s - loss: 0.5003 - acc: 0.8496"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62650/62650 [==============================] - 5s 77us/sample - loss: 0.4972 - acc: 0.8505 - val_loss: 0.2235 - val_acc: 0.9292\n",
      "Epoch 2/30\n",
      "62650/62650 [==============================] - 4s 56us/sample - loss: 0.2316 - acc: 0.9332 - val_loss: 0.1836 - val_acc: 0.9454\n",
      "Epoch 3/30\n",
      "62650/62650 [==============================] - 4s 64us/sample - loss: 0.1859 - acc: 0.9456 - val_loss: 0.1609 - val_acc: 0.9535\n",
      "Epoch 4/30\n",
      "62650/62650 [==============================] - 4s 63us/sample - loss: 0.1543 - acc: 0.9557 - val_loss: 0.1707 - val_acc: 0.9507\n",
      "Epoch 5/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.1362 - acc: 0.9611 - val_loss: 0.2041 - val_acc: 0.9435\n",
      "Epoch 6/30\n",
      "62650/62650 [==============================] - 4s 66us/sample - loss: 0.1262 - acc: 0.9627 - val_loss: 0.1659 - val_acc: 0.9535\n",
      "Epoch 7/30\n",
      "62650/62650 [==============================] - 4s 64us/sample - loss: 0.1146 - acc: 0.9664 - val_loss: 0.2112 - val_acc: 0.9450\n",
      "Epoch 8/30\n",
      "62650/62650 [==============================] - 4s 63us/sample - loss: 0.1065 - acc: 0.9685 - val_loss: 0.1888 - val_acc: 0.9506\n",
      "Epoch 9/30\n",
      "62650/62650 [==============================] - 4s 63us/sample - loss: 0.1017 - acc: 0.9698 - val_loss: 0.1725 - val_acc: 0.9554\n",
      "Epoch 10/30\n",
      "62650/62650 [==============================] - 4s 61us/sample - loss: 0.0959 - acc: 0.9714 - val_loss: 0.1666 - val_acc: 0.9596\n",
      "Epoch 11/30\n",
      "62650/62650 [==============================] - 4s 63us/sample - loss: 0.0906 - acc: 0.9730 - val_loss: 0.1543 - val_acc: 0.9620\n",
      "Epoch 12/30\n",
      "62650/62650 [==============================] - 4s 61us/sample - loss: 0.0861 - acc: 0.9740 - val_loss: 0.1814 - val_acc: 0.9562\n",
      "Epoch 13/30\n",
      "62650/62650 [==============================] - 4s 59us/sample - loss: 0.0841 - acc: 0.9743 - val_loss: 0.1672 - val_acc: 0.9583\n",
      "Epoch 14/30\n",
      "62650/62650 [==============================] - 4s 61us/sample - loss: 0.0798 - acc: 0.9755 - val_loss: 0.1649 - val_acc: 0.9601\n",
      "Epoch 15/30\n",
      "62650/62650 [==============================] - 4s 66us/sample - loss: 0.0765 - acc: 0.9764 - val_loss: 0.1746 - val_acc: 0.9599\n",
      "Epoch 16/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0743 - acc: 0.9779 - val_loss: 0.1647 - val_acc: 0.9619\n",
      "Epoch 17/30\n",
      "62650/62650 [==============================] - 4s 61us/sample - loss: 0.0720 - acc: 0.9777 - val_loss: 0.1855 - val_acc: 0.9568\n",
      "Epoch 18/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0714 - acc: 0.9774 - val_loss: 0.2204 - val_acc: 0.9536\n",
      "Epoch 19/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0708 - acc: 0.9781 - val_loss: 0.1960 - val_acc: 0.9583\n",
      "Epoch 20/30\n",
      "62650/62650 [==============================] - 4s 68us/sample - loss: 0.0675 - acc: 0.9789 - val_loss: 0.2112 - val_acc: 0.9542\n",
      "Epoch 21/30\n",
      "62650/62650 [==============================] - 4s 57us/sample - loss: 0.0633 - acc: 0.9798 - val_loss: 0.2135 - val_acc: 0.9542\n",
      "Epoch 22/30\n",
      "62650/62650 [==============================] - 4s 57us/sample - loss: 0.0655 - acc: 0.9790 - val_loss: 0.1949 - val_acc: 0.9573\n",
      "Epoch 23/30\n",
      "62650/62650 [==============================] - 4s 57us/sample - loss: 0.0612 - acc: 0.9809 - val_loss: 0.2062 - val_acc: 0.9568\n",
      "Epoch 24/30\n",
      "62650/62650 [==============================] - 4s 57us/sample - loss: 0.0607 - acc: 0.9810 - val_loss: 0.2103 - val_acc: 0.9556\n",
      "Epoch 25/30\n",
      "62650/62650 [==============================] - 4s 57us/sample - loss: 0.0598 - acc: 0.9811 - val_loss: 0.2296 - val_acc: 0.9523\n",
      "Epoch 26/30\n",
      "62650/62650 [==============================] - 4s 56us/sample - loss: 0.0579 - acc: 0.9818 - val_loss: 0.2098 - val_acc: 0.9557\n",
      "Epoch 27/30\n",
      "62650/62650 [==============================] - 4s 57us/sample - loss: 0.0593 - acc: 0.9812 - val_loss: 0.2377 - val_acc: 0.9508\n",
      "Epoch 28/30\n",
      "62650/62650 [==============================] - 4s 57us/sample - loss: 0.0557 - acc: 0.9821 - val_loss: 0.2109 - val_acc: 0.9567\n",
      "Epoch 29/30\n",
      "62650/62650 [==============================] - 4s 57us/sample - loss: 0.0558 - acc: 0.9824 - val_loss: 0.2162 - val_acc: 0.9565\n",
      "Epoch 30/30\n",
      "62650/62650 [==============================] - 4s 56us/sample - loss: 0.0530 - acc: 0.9835 - val_loss: 0.2110 - val_acc: 0.9554\n",
      "oversampling test loss:  0.21102053592905867\n",
      "oversampling accuracy:  0.9554\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_123 (Dense)            (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_82 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_83 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_41 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 62650 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "61760/62650 [============================>.] - ETA: 0s - loss: 0.5187 - acc: 0.8427"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62650/62650 [==============================] - 5s 81us/sample - loss: 0.5157 - acc: 0.8437 - val_loss: 0.2254 - val_acc: 0.9315\n",
      "Epoch 2/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.2483 - acc: 0.9276 - val_loss: 0.1982 - val_acc: 0.9401\n",
      "Epoch 3/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.1953 - acc: 0.9418 - val_loss: 0.1632 - val_acc: 0.9511\n",
      "Epoch 4/30\n",
      "62650/62650 [==============================] - 4s 57us/sample - loss: 0.1701 - acc: 0.9483 - val_loss: 0.1877 - val_acc: 0.9428\n",
      "Epoch 5/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.1556 - acc: 0.9542 - val_loss: 0.1656 - val_acc: 0.9528\n",
      "Epoch 6/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.1431 - acc: 0.9576 - val_loss: 0.1518 - val_acc: 0.9541\n",
      "Epoch 7/30\n",
      "62650/62650 [==============================] - 4s 59us/sample - loss: 0.1286 - acc: 0.9601 - val_loss: 0.1689 - val_acc: 0.9509\n",
      "Epoch 8/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.1195 - acc: 0.9640 - val_loss: 0.1634 - val_acc: 0.9565\n",
      "Epoch 9/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.1163 - acc: 0.9659 - val_loss: 0.1678 - val_acc: 0.9538\n",
      "Epoch 10/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.1124 - acc: 0.9658 - val_loss: 0.1516 - val_acc: 0.9567\n",
      "Epoch 11/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.1048 - acc: 0.9686 - val_loss: 0.1566 - val_acc: 0.9566\n",
      "Epoch 12/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.1021 - acc: 0.9692 - val_loss: 0.1590 - val_acc: 0.9579\n",
      "Epoch 13/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0997 - acc: 0.9690 - val_loss: 0.1720 - val_acc: 0.9519\n",
      "Epoch 14/30\n",
      "62650/62650 [==============================] - 4s 59us/sample - loss: 0.0959 - acc: 0.9703 - val_loss: 0.1732 - val_acc: 0.9533\n",
      "Epoch 15/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0900 - acc: 0.9722 - val_loss: 0.1547 - val_acc: 0.9567\n",
      "Epoch 16/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0870 - acc: 0.9733 - val_loss: 0.1619 - val_acc: 0.9583\n",
      "Epoch 17/30\n",
      "62650/62650 [==============================] - 4s 59us/sample - loss: 0.0856 - acc: 0.9736 - val_loss: 0.1586 - val_acc: 0.9559\n",
      "Epoch 18/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0836 - acc: 0.9741 - val_loss: 0.1610 - val_acc: 0.9574\n",
      "Epoch 19/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0826 - acc: 0.9740 - val_loss: 0.1531 - val_acc: 0.9580\n",
      "Epoch 20/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0797 - acc: 0.9749 - val_loss: 0.1844 - val_acc: 0.9532\n",
      "Epoch 21/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0752 - acc: 0.9761 - val_loss: 0.1721 - val_acc: 0.9577\n",
      "Epoch 22/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0763 - acc: 0.9761 - val_loss: 0.1570 - val_acc: 0.9586\n",
      "Epoch 23/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0739 - acc: 0.9766 - val_loss: 0.1631 - val_acc: 0.9599\n",
      "Epoch 24/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0696 - acc: 0.9782 - val_loss: 0.1716 - val_acc: 0.9555\n",
      "Epoch 25/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0716 - acc: 0.9774 - val_loss: 0.1513 - val_acc: 0.9613\n",
      "Epoch 26/30\n",
      "62650/62650 [==============================] - 4s 59us/sample - loss: 0.0717 - acc: 0.9780 - val_loss: 0.1839 - val_acc: 0.9550\n",
      "Epoch 27/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0696 - acc: 0.9782 - val_loss: 0.1723 - val_acc: 0.9569\n",
      "Epoch 28/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0688 - acc: 0.9785 - val_loss: 0.1758 - val_acc: 0.9583\n",
      "Epoch 29/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0661 - acc: 0.9793 - val_loss: 0.1709 - val_acc: 0.9560\n",
      "Epoch 30/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0672 - acc: 0.9790 - val_loss: 0.1660 - val_acc: 0.9600\n",
      "oversampling test loss:  0.16597198657928966\n",
      "oversampling accuracy:  0.96\n",
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_126 (Dense)            (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_84 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_127 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_85 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_42 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_128 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 62650 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "62464/62650 [============================>.] - ETA: 0s - loss: 0.5251 - acc: 0.8422"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62650/62650 [==============================] - 5s 80us/sample - loss: 0.5244 - acc: 0.8424 - val_loss: 0.2094 - val_acc: 0.9345\n",
      "Epoch 2/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.2427 - acc: 0.9291 - val_loss: 0.1833 - val_acc: 0.9427\n",
      "Epoch 3/30\n",
      "62650/62650 [==============================] - 4s 59us/sample - loss: 0.1930 - acc: 0.9429 - val_loss: 0.1767 - val_acc: 0.9471\n",
      "Epoch 4/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.1632 - acc: 0.9520 - val_loss: 0.1722 - val_acc: 0.9502\n",
      "Epoch 5/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.1461 - acc: 0.9570 - val_loss: 0.1566 - val_acc: 0.9558\n",
      "Epoch 6/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.1325 - acc: 0.9613 - val_loss: 0.1655 - val_acc: 0.9547\n",
      "Epoch 7/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.1245 - acc: 0.9633 - val_loss: 0.1637 - val_acc: 0.9545\n",
      "Epoch 8/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.1153 - acc: 0.9658 - val_loss: 0.1802 - val_acc: 0.9520\n",
      "Epoch 9/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.1079 - acc: 0.9688 - val_loss: 0.1657 - val_acc: 0.9565\n",
      "Epoch 10/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.1019 - acc: 0.9693 - val_loss: 0.1827 - val_acc: 0.9533\n",
      "Epoch 11/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0995 - acc: 0.9700 - val_loss: 0.1768 - val_acc: 0.9559\n",
      "Epoch 12/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0955 - acc: 0.9714 - val_loss: 0.1470 - val_acc: 0.9615\n",
      "Epoch 13/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0897 - acc: 0.9721 - val_loss: 0.1582 - val_acc: 0.9607\n",
      "Epoch 14/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0839 - acc: 0.9741 - val_loss: 0.1671 - val_acc: 0.9581\n",
      "Epoch 15/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0830 - acc: 0.9739 - val_loss: 0.1662 - val_acc: 0.9624\n",
      "Epoch 16/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0789 - acc: 0.9763 - val_loss: 0.1847 - val_acc: 0.9553\n",
      "Epoch 17/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0794 - acc: 0.9760 - val_loss: 0.1644 - val_acc: 0.9588\n",
      "Epoch 18/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0746 - acc: 0.9767 - val_loss: 0.2014 - val_acc: 0.9530\n",
      "Epoch 19/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0737 - acc: 0.9773 - val_loss: 0.1821 - val_acc: 0.9594\n",
      "Epoch 20/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0722 - acc: 0.9777 - val_loss: 0.1728 - val_acc: 0.9597\n",
      "Epoch 21/30\n",
      "62650/62650 [==============================] - 4s 59us/sample - loss: 0.0671 - acc: 0.9795 - val_loss: 0.1707 - val_acc: 0.9632\n",
      "Epoch 22/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0662 - acc: 0.9794 - val_loss: 0.1900 - val_acc: 0.9569\n",
      "Epoch 23/30\n",
      "62650/62650 [==============================] - 4s 59us/sample - loss: 0.0669 - acc: 0.9792 - val_loss: 0.1866 - val_acc: 0.9584\n",
      "Epoch 24/30\n",
      "62650/62650 [==============================] - 4s 59us/sample - loss: 0.0650 - acc: 0.9795 - val_loss: 0.1976 - val_acc: 0.9559\n",
      "Epoch 25/30\n",
      "62650/62650 [==============================] - 4s 61us/sample - loss: 0.0643 - acc: 0.9798 - val_loss: 0.1807 - val_acc: 0.9617\n",
      "Epoch 26/30\n",
      "62650/62650 [==============================] - 4s 63us/sample - loss: 0.0644 - acc: 0.9796 - val_loss: 0.2038 - val_acc: 0.9582\n",
      "Epoch 27/30\n",
      "62650/62650 [==============================] - 4s 67us/sample - loss: 0.0610 - acc: 0.9806 - val_loss: 0.1780 - val_acc: 0.9609\n",
      "Epoch 28/30\n",
      "62650/62650 [==============================] - 4s 68us/sample - loss: 0.0604 - acc: 0.9808 - val_loss: 0.2072 - val_acc: 0.9582\n",
      "Epoch 29/30\n",
      "62650/62650 [==============================] - 4s 69us/sample - loss: 0.0601 - acc: 0.9808 - val_loss: 0.1893 - val_acc: 0.9601\n",
      "Epoch 30/30\n",
      "62650/62650 [==============================] - 4s 65us/sample - loss: 0.0581 - acc: 0.9818 - val_loss: 0.1923 - val_acc: 0.9591\n",
      "oversampling test loss:  0.19227420876640244\n",
      "oversampling accuracy:  0.9591\n",
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_129 (Dense)            (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_86 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_130 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_87 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_43 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_131 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 62650 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "62080/62650 [============================>.] - ETA: 0s - loss: 0.5324 - acc: 0.8397"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62650/62650 [==============================] - 6s 96us/sample - loss: 0.5303 - acc: 0.8404 - val_loss: 0.2361 - val_acc: 0.9292\n",
      "Epoch 2/30\n",
      "62650/62650 [==============================] - 4s 65us/sample - loss: 0.2358 - acc: 0.9317 - val_loss: 0.1728 - val_acc: 0.9473\n",
      "Epoch 3/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.1945 - acc: 0.9430 - val_loss: 0.1607 - val_acc: 0.9513\n",
      "Epoch 4/30\n",
      "62650/62650 [==============================] - 4s 64us/sample - loss: 0.1639 - acc: 0.9521 - val_loss: 0.1688 - val_acc: 0.9511\n",
      "Epoch 5/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.1489 - acc: 0.9564 - val_loss: 0.1692 - val_acc: 0.9518\n",
      "Epoch 6/30\n",
      "62650/62650 [==============================] - 4s 71us/sample - loss: 0.1364 - acc: 0.9595 - val_loss: 0.1498 - val_acc: 0.9573\n",
      "Epoch 7/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.1238 - acc: 0.9637 - val_loss: 0.1446 - val_acc: 0.9596\n",
      "Epoch 8/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.1172 - acc: 0.9651 - val_loss: 0.1517 - val_acc: 0.9581\n",
      "Epoch 9/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.1132 - acc: 0.9665 - val_loss: 0.1712 - val_acc: 0.9549\n",
      "Epoch 10/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.1082 - acc: 0.9674 - val_loss: 0.1571 - val_acc: 0.9576\n",
      "Epoch 11/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.1028 - acc: 0.9686 - val_loss: 0.1806 - val_acc: 0.9536\n",
      "Epoch 12/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.0986 - acc: 0.9710 - val_loss: 0.1625 - val_acc: 0.9579\n",
      "Epoch 13/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.0935 - acc: 0.9723 - val_loss: 0.1809 - val_acc: 0.9542\n",
      "Epoch 14/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.0931 - acc: 0.9713 - val_loss: 0.1728 - val_acc: 0.9572\n",
      "Epoch 15/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.0856 - acc: 0.9735 - val_loss: 0.1680 - val_acc: 0.9596\n",
      "Epoch 16/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.0825 - acc: 0.9748 - val_loss: 0.1629 - val_acc: 0.9599\n",
      "Epoch 17/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.0840 - acc: 0.9746 - val_loss: 0.1858 - val_acc: 0.9561\n",
      "Epoch 18/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.0790 - acc: 0.9760 - val_loss: 0.1849 - val_acc: 0.9566\n",
      "Epoch 19/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.0771 - acc: 0.9765 - val_loss: 0.1646 - val_acc: 0.9619\n",
      "Epoch 20/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.0749 - acc: 0.9774 - val_loss: 0.1794 - val_acc: 0.9596\n",
      "Epoch 21/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.0764 - acc: 0.9766 - val_loss: 0.1647 - val_acc: 0.9619\n",
      "Epoch 22/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.0722 - acc: 0.9773 - val_loss: 0.1756 - val_acc: 0.9592\n",
      "Epoch 23/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.0696 - acc: 0.9786 - val_loss: 0.1889 - val_acc: 0.9598\n",
      "Epoch 24/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.0686 - acc: 0.9782 - val_loss: 0.1871 - val_acc: 0.9584\n",
      "Epoch 25/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.0677 - acc: 0.9790 - val_loss: 0.2116 - val_acc: 0.9572\n",
      "Epoch 26/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.0668 - acc: 0.9790 - val_loss: 0.2351 - val_acc: 0.9504\n",
      "Epoch 27/30\n",
      "62650/62650 [==============================] - 4s 59us/sample - loss: 0.0658 - acc: 0.9793 - val_loss: 0.1839 - val_acc: 0.9599\n",
      "Epoch 28/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.0647 - acc: 0.9798 - val_loss: 0.1941 - val_acc: 0.9596\n",
      "Epoch 29/30\n",
      "62650/62650 [==============================] - 4s 59us/sample - loss: 0.0643 - acc: 0.9801 - val_loss: 0.1967 - val_acc: 0.9591\n",
      "Epoch 30/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.0621 - acc: 0.9804 - val_loss: 0.1935 - val_acc: 0.9597\n",
      "oversampling test loss:  0.19350566735644825\n",
      "oversampling accuracy:  0.9597\n",
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_132 (Dense)            (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_88 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_89 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_44 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 62650 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "62144/62650 [============================>.] - ETA: 0s - loss: 0.5244 - acc: 0.8426"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62650/62650 [==============================] - 5s 84us/sample - loss: 0.5227 - acc: 0.8433 - val_loss: 0.2157 - val_acc: 0.9321\n",
      "Epoch 2/30\n",
      "62650/62650 [==============================] - 4s 68us/sample - loss: 0.2368 - acc: 0.9310 - val_loss: 0.1816 - val_acc: 0.9449\n",
      "Epoch 3/30\n",
      "62650/62650 [==============================] - 4s 56us/sample - loss: 0.1866 - acc: 0.9450 - val_loss: 0.1854 - val_acc: 0.9446\n",
      "Epoch 4/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.1585 - acc: 0.9540 - val_loss: 0.1746 - val_acc: 0.9486\n",
      "Epoch 5/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.1443 - acc: 0.9584 - val_loss: 0.1687 - val_acc: 0.9502\n",
      "Epoch 6/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.1301 - acc: 0.9621 - val_loss: 0.1621 - val_acc: 0.9545\n",
      "Epoch 7/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.1181 - acc: 0.9644 - val_loss: 0.1629 - val_acc: 0.9555\n",
      "Epoch 8/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.1122 - acc: 0.9674 - val_loss: 0.1534 - val_acc: 0.9591\n",
      "Epoch 9/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.1082 - acc: 0.9671 - val_loss: 0.1392 - val_acc: 0.9638\n",
      "Epoch 10/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.1004 - acc: 0.9689 - val_loss: 0.1574 - val_acc: 0.9581\n",
      "Epoch 11/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0977 - acc: 0.9704 - val_loss: 0.1596 - val_acc: 0.9593\n",
      "Epoch 12/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.0937 - acc: 0.9713 - val_loss: 0.1679 - val_acc: 0.9587\n",
      "Epoch 13/30\n",
      "62650/62650 [==============================] - 4s 56us/sample - loss: 0.0893 - acc: 0.9721 - val_loss: 0.1830 - val_acc: 0.9552\n",
      "Epoch 14/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0858 - acc: 0.9731 - val_loss: 0.1521 - val_acc: 0.9622\n",
      "Epoch 15/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.0842 - acc: 0.9742 - val_loss: 0.1662 - val_acc: 0.9593\n",
      "Epoch 16/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0813 - acc: 0.9742 - val_loss: 0.1863 - val_acc: 0.9554\n",
      "Epoch 17/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0782 - acc: 0.9752 - val_loss: 0.1663 - val_acc: 0.9566\n",
      "Epoch 18/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0785 - acc: 0.9753 - val_loss: 0.1632 - val_acc: 0.9601\n",
      "Epoch 19/30\n",
      "62650/62650 [==============================] - 3s 51us/sample - loss: 0.0765 - acc: 0.9770 - val_loss: 0.1702 - val_acc: 0.9580\n",
      "Epoch 20/30\n",
      "62650/62650 [==============================] - 3s 51us/sample - loss: 0.0716 - acc: 0.9779 - val_loss: 0.1846 - val_acc: 0.9572\n",
      "Epoch 21/30\n",
      "62650/62650 [==============================] - 3s 51us/sample - loss: 0.0733 - acc: 0.9772 - val_loss: 0.1761 - val_acc: 0.9588\n",
      "Epoch 22/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0704 - acc: 0.9779 - val_loss: 0.1793 - val_acc: 0.9588\n",
      "Epoch 23/30\n",
      "62650/62650 [==============================] - 3s 51us/sample - loss: 0.0699 - acc: 0.9781 - val_loss: 0.1719 - val_acc: 0.9610\n",
      "Epoch 24/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0665 - acc: 0.9789 - val_loss: 0.1844 - val_acc: 0.9596\n",
      "Epoch 25/30\n",
      "62650/62650 [==============================] - 3s 51us/sample - loss: 0.0669 - acc: 0.9790 - val_loss: 0.1817 - val_acc: 0.9592\n",
      "Epoch 26/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0648 - acc: 0.9797 - val_loss: 0.1774 - val_acc: 0.9571\n",
      "Epoch 27/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0611 - acc: 0.9809 - val_loss: 0.1792 - val_acc: 0.9616\n",
      "Epoch 28/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0627 - acc: 0.9800 - val_loss: 0.2110 - val_acc: 0.9575\n",
      "Epoch 29/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0620 - acc: 0.9806 - val_loss: 0.1989 - val_acc: 0.9571\n",
      "Epoch 30/30\n",
      "62650/62650 [==============================] - 3s 51us/sample - loss: 0.0606 - acc: 0.9810 - val_loss: 0.1754 - val_acc: 0.9599\n",
      "oversampling test loss:  0.17541848064859514\n",
      "oversampling accuracy:  0.9599\n",
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_135 (Dense)            (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_90 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_91 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_45 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 62650 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "61760/62650 [============================>.] - ETA: 0s - loss: 0.5351 - acc: 0.8395"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62650/62650 [==============================] - 5s 74us/sample - loss: 0.5316 - acc: 0.8406 - val_loss: 0.2193 - val_acc: 0.9344\n",
      "Epoch 2/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.2412 - acc: 0.9302 - val_loss: 0.1726 - val_acc: 0.9491\n",
      "Epoch 3/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.1897 - acc: 0.9445 - val_loss: 0.1692 - val_acc: 0.9512\n",
      "Epoch 4/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.1641 - acc: 0.9521 - val_loss: 0.1701 - val_acc: 0.9521\n",
      "Epoch 5/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.1456 - acc: 0.9576 - val_loss: 0.1508 - val_acc: 0.9575\n",
      "Epoch 6/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.1359 - acc: 0.9604 - val_loss: 0.1869 - val_acc: 0.9478\n",
      "Epoch 7/30\n",
      "62650/62650 [==============================] - 3s 51us/sample - loss: 0.1260 - acc: 0.9638 - val_loss: 0.1597 - val_acc: 0.9562\n",
      "Epoch 8/30\n",
      "62650/62650 [==============================] - 3s 51us/sample - loss: 0.1137 - acc: 0.9661 - val_loss: 0.1642 - val_acc: 0.9563\n",
      "Epoch 9/30\n",
      "62650/62650 [==============================] - 3s 51us/sample - loss: 0.1062 - acc: 0.9690 - val_loss: 0.1522 - val_acc: 0.9596\n",
      "Epoch 10/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.1047 - acc: 0.9693 - val_loss: 0.1823 - val_acc: 0.9527\n",
      "Epoch 11/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0949 - acc: 0.9712 - val_loss: 0.1875 - val_acc: 0.9528\n",
      "Epoch 12/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0944 - acc: 0.9719 - val_loss: 0.1512 - val_acc: 0.9600\n",
      "Epoch 13/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0871 - acc: 0.9734 - val_loss: 0.1643 - val_acc: 0.9571\n",
      "Epoch 14/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0873 - acc: 0.9738 - val_loss: 0.1564 - val_acc: 0.9600\n",
      "Epoch 15/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0830 - acc: 0.9754 - val_loss: 0.1504 - val_acc: 0.9613\n",
      "Epoch 16/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0819 - acc: 0.9749 - val_loss: 0.1938 - val_acc: 0.9542\n",
      "Epoch 17/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0756 - acc: 0.9767 - val_loss: 0.1700 - val_acc: 0.9576\n",
      "Epoch 18/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0762 - acc: 0.9766 - val_loss: 0.1751 - val_acc: 0.9570\n",
      "Epoch 19/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0749 - acc: 0.9769 - val_loss: 0.1700 - val_acc: 0.9592\n",
      "Epoch 20/30\n",
      "62650/62650 [==============================] - 3s 51us/sample - loss: 0.0724 - acc: 0.9772 - val_loss: 0.1722 - val_acc: 0.9581\n",
      "Epoch 21/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.0701 - acc: 0.9785 - val_loss: 0.1536 - val_acc: 0.9621\n",
      "Epoch 22/30\n",
      "62650/62650 [==============================] - 3s 51us/sample - loss: 0.0691 - acc: 0.9787 - val_loss: 0.1812 - val_acc: 0.9573\n",
      "Epoch 23/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0649 - acc: 0.9802 - val_loss: 0.1774 - val_acc: 0.9585\n",
      "Epoch 24/30\n",
      "62650/62650 [==============================] - 3s 51us/sample - loss: 0.0639 - acc: 0.9805 - val_loss: 0.1819 - val_acc: 0.9582\n",
      "Epoch 25/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0684 - acc: 0.9788 - val_loss: 0.1679 - val_acc: 0.9604\n",
      "Epoch 26/30\n",
      "62650/62650 [==============================] - 3s 51us/sample - loss: 0.0637 - acc: 0.9806 - val_loss: 0.2039 - val_acc: 0.9559\n",
      "Epoch 27/30\n",
      "62650/62650 [==============================] - 3s 51us/sample - loss: 0.0652 - acc: 0.9796 - val_loss: 0.1840 - val_acc: 0.9574\n",
      "Epoch 28/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0619 - acc: 0.9808 - val_loss: 0.1820 - val_acc: 0.9574\n",
      "Epoch 29/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0595 - acc: 0.9820 - val_loss: 0.2038 - val_acc: 0.9562\n",
      "Epoch 30/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0588 - acc: 0.9817 - val_loss: 0.1781 - val_acc: 0.9613\n",
      "oversampling test loss:  0.1781435848956229\n",
      "oversampling accuracy:  0.9613\n",
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_138 (Dense)            (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_92 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_93 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_46 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_140 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 62650 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "62336/62650 [============================>.] - ETA: 0s - loss: 0.5344 - acc: 0.8405"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62650/62650 [==============================] - 5s 74us/sample - loss: 0.5329 - acc: 0.8410 - val_loss: 0.2184 - val_acc: 0.9337\n",
      "Epoch 2/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.2396 - acc: 0.9309 - val_loss: 0.1792 - val_acc: 0.9446\n",
      "Epoch 3/30\n",
      "62650/62650 [==============================] - 3s 54us/sample - loss: 0.1872 - acc: 0.9461 - val_loss: 0.1563 - val_acc: 0.9526\n",
      "Epoch 4/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.1567 - acc: 0.9542 - val_loss: 0.1486 - val_acc: 0.9552\n",
      "Epoch 5/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.1381 - acc: 0.9597 - val_loss: 0.1456 - val_acc: 0.9576\n",
      "Epoch 6/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.1269 - acc: 0.9631 - val_loss: 0.1555 - val_acc: 0.9554\n",
      "Epoch 7/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.1143 - acc: 0.9660 - val_loss: 0.1474 - val_acc: 0.9583\n",
      "Epoch 8/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.1086 - acc: 0.9684 - val_loss: 0.1521 - val_acc: 0.9561\n",
      "Epoch 9/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.1038 - acc: 0.9693 - val_loss: 0.1569 - val_acc: 0.9560\n",
      "Epoch 10/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.0997 - acc: 0.9701 - val_loss: 0.1545 - val_acc: 0.9578\n",
      "Epoch 11/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.0923 - acc: 0.9722 - val_loss: 0.1418 - val_acc: 0.9612\n",
      "Epoch 12/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0891 - acc: 0.9730 - val_loss: 0.1575 - val_acc: 0.9585\n",
      "Epoch 13/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.0858 - acc: 0.9742 - val_loss: 0.1466 - val_acc: 0.9636\n",
      "Epoch 14/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.0825 - acc: 0.9751 - val_loss: 0.1507 - val_acc: 0.9626\n",
      "Epoch 15/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.0789 - acc: 0.9752 - val_loss: 0.1443 - val_acc: 0.9624\n",
      "Epoch 16/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0794 - acc: 0.9761 - val_loss: 0.1699 - val_acc: 0.9555\n",
      "Epoch 17/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.0761 - acc: 0.9764 - val_loss: 0.1648 - val_acc: 0.9593\n",
      "Epoch 18/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0743 - acc: 0.9778 - val_loss: 0.1508 - val_acc: 0.9634\n",
      "Epoch 19/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.0736 - acc: 0.9774 - val_loss: 0.1577 - val_acc: 0.9619\n",
      "Epoch 20/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.0698 - acc: 0.9786 - val_loss: 0.1494 - val_acc: 0.9637\n",
      "Epoch 21/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.0702 - acc: 0.9781 - val_loss: 0.1480 - val_acc: 0.9623\n",
      "Epoch 22/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0660 - acc: 0.9799 - val_loss: 0.1600 - val_acc: 0.9629\n",
      "Epoch 23/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.0687 - acc: 0.9797 - val_loss: 0.1676 - val_acc: 0.9615\n",
      "Epoch 24/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.0655 - acc: 0.9799 - val_loss: 0.1531 - val_acc: 0.9633\n",
      "Epoch 25/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.0655 - acc: 0.9792 - val_loss: 0.1680 - val_acc: 0.9625\n",
      "Epoch 26/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.0613 - acc: 0.9805 - val_loss: 0.1553 - val_acc: 0.9641\n",
      "Epoch 27/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.0639 - acc: 0.9801 - val_loss: 0.1670 - val_acc: 0.9594\n",
      "Epoch 28/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0613 - acc: 0.9812 - val_loss: 0.1661 - val_acc: 0.9614\n",
      "Epoch 29/30\n",
      "62650/62650 [==============================] - 3s 52us/sample - loss: 0.0580 - acc: 0.9816 - val_loss: 0.1795 - val_acc: 0.9576\n",
      "Epoch 30/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.0592 - acc: 0.9813 - val_loss: 0.1821 - val_acc: 0.9607\n",
      "oversampling test loss:  0.18211883830363512\n",
      "oversampling accuracy:  0.9607\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_141 (Dense)            (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_94 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_47 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 62650 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "62144/62650 [============================>.] - ETA: 0s - loss: 0.5094 - acc: 0.8468"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62650/62650 [==============================] - 5s 75us/sample - loss: 0.5081 - acc: 0.8474 - val_loss: 0.2280 - val_acc: 0.9294\n",
      "Epoch 2/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.2333 - acc: 0.9334 - val_loss: 0.1802 - val_acc: 0.9452\n",
      "Epoch 3/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.1808 - acc: 0.9478 - val_loss: 0.1460 - val_acc: 0.9575\n",
      "Epoch 4/30\n",
      "62650/62650 [==============================] - 3s 53us/sample - loss: 0.1589 - acc: 0.9552 - val_loss: 0.1427 - val_acc: 0.9582\n",
      "Epoch 5/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.1361 - acc: 0.9605 - val_loss: 0.1364 - val_acc: 0.9597\n",
      "Epoch 6/30\n",
      "62650/62650 [==============================] - 4s 66us/sample - loss: 0.1282 - acc: 0.9635 - val_loss: 0.1498 - val_acc: 0.9585\n",
      "Epoch 7/30\n",
      "62650/62650 [==============================] - 4s 61us/sample - loss: 0.1174 - acc: 0.9660 - val_loss: 0.1359 - val_acc: 0.9604\n",
      "Epoch 8/30\n",
      "62650/62650 [==============================] - 4s 61us/sample - loss: 0.1077 - acc: 0.9681 - val_loss: 0.1265 - val_acc: 0.9640\n",
      "Epoch 9/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.1008 - acc: 0.9704 - val_loss: 0.1312 - val_acc: 0.9644\n",
      "Epoch 10/30\n",
      "62650/62650 [==============================] - 4s 61us/sample - loss: 0.0915 - acc: 0.9727 - val_loss: 0.1404 - val_acc: 0.9616\n",
      "Epoch 11/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0899 - acc: 0.9728 - val_loss: 0.1278 - val_acc: 0.9653\n",
      "Epoch 12/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0860 - acc: 0.9740 - val_loss: 0.1449 - val_acc: 0.9593\n",
      "Epoch 13/30\n",
      "62650/62650 [==============================] - 4s 61us/sample - loss: 0.0828 - acc: 0.9753 - val_loss: 0.1422 - val_acc: 0.9612\n",
      "Epoch 14/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0803 - acc: 0.9760 - val_loss: 0.1418 - val_acc: 0.9631\n",
      "Epoch 15/30\n",
      "62650/62650 [==============================] - 4s 61us/sample - loss: 0.0794 - acc: 0.9762 - val_loss: 0.1360 - val_acc: 0.9638\n",
      "Epoch 16/30\n",
      "62650/62650 [==============================] - 4s 61us/sample - loss: 0.0725 - acc: 0.9783 - val_loss: 0.1503 - val_acc: 0.9611\n",
      "Epoch 17/30\n",
      "62650/62650 [==============================] - 4s 61us/sample - loss: 0.0720 - acc: 0.9780 - val_loss: 0.1376 - val_acc: 0.9650\n",
      "Epoch 18/30\n",
      "62650/62650 [==============================] - 4s 61us/sample - loss: 0.0723 - acc: 0.9782 - val_loss: 0.1453 - val_acc: 0.9625\n",
      "Epoch 19/30\n",
      "62650/62650 [==============================] - 4s 61us/sample - loss: 0.0693 - acc: 0.9785 - val_loss: 0.1260 - val_acc: 0.9676\n",
      "Epoch 20/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0663 - acc: 0.9795 - val_loss: 0.1337 - val_acc: 0.9678\n",
      "Epoch 21/30\n",
      "62650/62650 [==============================] - 4s 61us/sample - loss: 0.0659 - acc: 0.9799 - val_loss: 0.1457 - val_acc: 0.9633\n",
      "Epoch 22/30\n",
      "62650/62650 [==============================] - 4s 61us/sample - loss: 0.0627 - acc: 0.9805 - val_loss: 0.1448 - val_acc: 0.9628\n",
      "Epoch 23/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0633 - acc: 0.9798 - val_loss: 0.1284 - val_acc: 0.9686\n",
      "Epoch 24/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0609 - acc: 0.9806 - val_loss: 0.1436 - val_acc: 0.9641\n",
      "Epoch 25/30\n",
      "62650/62650 [==============================] - 4s 61us/sample - loss: 0.0613 - acc: 0.9813 - val_loss: 0.1403 - val_acc: 0.9657\n",
      "Epoch 26/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0598 - acc: 0.9816 - val_loss: 0.1461 - val_acc: 0.9650\n",
      "Epoch 27/30\n",
      "62650/62650 [==============================] - 4s 61us/sample - loss: 0.0589 - acc: 0.9816 - val_loss: 0.1397 - val_acc: 0.9660\n",
      "Epoch 28/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0576 - acc: 0.9827 - val_loss: 0.1304 - val_acc: 0.9686\n",
      "Epoch 29/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0572 - acc: 0.9817 - val_loss: 0.1440 - val_acc: 0.9657\n",
      "Epoch 30/30\n",
      "62650/62650 [==============================] - 4s 61us/sample - loss: 0.0566 - acc: 0.9829 - val_loss: 0.1578 - val_acc: 0.9632\n",
      "oversampling test loss:  0.15780240595800568\n",
      "oversampling accuracy:  0.9632\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_144 (Dense)            (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_96 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_145 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_97 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_48 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_146 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 62650 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "62016/62650 [============================>.] - ETA: 0s - loss: 0.5164 - acc: 0.8438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62650/62650 [==============================] - 5s 85us/sample - loss: 0.5139 - acc: 0.8446 - val_loss: 0.2017 - val_acc: 0.9372\n",
      "Epoch 2/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.2428 - acc: 0.9293 - val_loss: 0.1771 - val_acc: 0.9462\n",
      "Epoch 3/30\n",
      "62650/62650 [==============================] - 4s 63us/sample - loss: 0.1955 - acc: 0.9424 - val_loss: 0.1820 - val_acc: 0.9459\n",
      "Epoch 4/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.1647 - acc: 0.9513 - val_loss: 0.1660 - val_acc: 0.9515\n",
      "Epoch 5/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.1459 - acc: 0.9575 - val_loss: 0.1811 - val_acc: 0.9493\n",
      "Epoch 6/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.1338 - acc: 0.9597 - val_loss: 0.1573 - val_acc: 0.9539\n",
      "Epoch 7/30\n",
      "62650/62650 [==============================] - 4s 63us/sample - loss: 0.1232 - acc: 0.9641 - val_loss: 0.1575 - val_acc: 0.9559\n",
      "Epoch 8/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.1106 - acc: 0.9663 - val_loss: 0.1591 - val_acc: 0.9565\n",
      "Epoch 9/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.1040 - acc: 0.9683 - val_loss: 0.1609 - val_acc: 0.9574\n",
      "Epoch 10/30\n",
      "62650/62650 [==============================] - 4s 63us/sample - loss: 0.1009 - acc: 0.9691 - val_loss: 0.1517 - val_acc: 0.9595\n",
      "Epoch 11/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0945 - acc: 0.9712 - val_loss: 0.1564 - val_acc: 0.9589\n",
      "Epoch 12/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0915 - acc: 0.9715 - val_loss: 0.1556 - val_acc: 0.9592\n",
      "Epoch 13/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0875 - acc: 0.9732 - val_loss: 0.1579 - val_acc: 0.9601\n",
      "Epoch 14/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0843 - acc: 0.9742 - val_loss: 0.1556 - val_acc: 0.9594\n",
      "Epoch 15/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0813 - acc: 0.9743 - val_loss: 0.1375 - val_acc: 0.9639\n",
      "Epoch 16/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0772 - acc: 0.9762 - val_loss: 0.1577 - val_acc: 0.9600\n",
      "Epoch 17/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0756 - acc: 0.9761 - val_loss: 0.1540 - val_acc: 0.9598\n",
      "Epoch 18/30\n",
      "62650/62650 [==============================] - 4s 63us/sample - loss: 0.0746 - acc: 0.9767 - val_loss: 0.1632 - val_acc: 0.9605\n",
      "Epoch 19/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0723 - acc: 0.9774 - val_loss: 0.1486 - val_acc: 0.9622\n",
      "Epoch 20/30\n",
      "62650/62650 [==============================] - 4s 63us/sample - loss: 0.0705 - acc: 0.9780 - val_loss: 0.1613 - val_acc: 0.9609\n",
      "Epoch 21/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0686 - acc: 0.9790 - val_loss: 0.1436 - val_acc: 0.9632\n",
      "Epoch 22/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0686 - acc: 0.9782 - val_loss: 0.1540 - val_acc: 0.9632\n",
      "Epoch 23/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0639 - acc: 0.9794 - val_loss: 0.1895 - val_acc: 0.9564\n",
      "Epoch 24/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0659 - acc: 0.9794 - val_loss: 0.1576 - val_acc: 0.9611\n",
      "Epoch 25/30\n",
      "62650/62650 [==============================] - 4s 63us/sample - loss: 0.0608 - acc: 0.9807 - val_loss: 0.1533 - val_acc: 0.9615\n",
      "Epoch 26/30\n",
      "62650/62650 [==============================] - 4s 63us/sample - loss: 0.0607 - acc: 0.9801 - val_loss: 0.1750 - val_acc: 0.9622\n",
      "Epoch 27/30\n",
      "62650/62650 [==============================] - 4s 63us/sample - loss: 0.0618 - acc: 0.9803 - val_loss: 0.1600 - val_acc: 0.9629\n",
      "Epoch 28/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0596 - acc: 0.9809 - val_loss: 0.1698 - val_acc: 0.9609\n",
      "Epoch 29/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0577 - acc: 0.9822 - val_loss: 0.1848 - val_acc: 0.9592\n",
      "Epoch 30/30\n",
      "62650/62650 [==============================] - 4s 62us/sample - loss: 0.0555 - acc: 0.9825 - val_loss: 0.1974 - val_acc: 0.9581\n",
      "oversampling test loss:  0.19742806965826312\n",
      "oversampling accuracy:  0.9581\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_147 (Dense)            (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_98 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_148 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_99 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_49 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 62650 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "62528/62650 [============================>.] - ETA: 0s - loss: 0.5036 - acc: 0.8496"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62650/62650 [==============================] - 5s 86us/sample - loss: 0.5033 - acc: 0.8498 - val_loss: 0.2183 - val_acc: 0.9330\n",
      "Epoch 2/30\n",
      "62650/62650 [==============================] - 4s 63us/sample - loss: 0.2313 - acc: 0.9329 - val_loss: 0.1647 - val_acc: 0.9497\n",
      "Epoch 3/30\n",
      "62650/62650 [==============================] - 4s 64us/sample - loss: 0.1777 - acc: 0.9487 - val_loss: 0.1559 - val_acc: 0.9559\n",
      "Epoch 4/30\n",
      "62650/62650 [==============================] - 4s 63us/sample - loss: 0.1492 - acc: 0.9567 - val_loss: 0.1507 - val_acc: 0.9568\n",
      "Epoch 5/30\n",
      "62650/62650 [==============================] - 4s 63us/sample - loss: 0.1302 - acc: 0.9617 - val_loss: 0.1544 - val_acc: 0.9569\n",
      "Epoch 6/30\n",
      "62650/62650 [==============================] - 4s 68us/sample - loss: 0.1215 - acc: 0.9636 - val_loss: 0.1358 - val_acc: 0.9627\n",
      "Epoch 7/30\n",
      "62650/62650 [==============================] - 4s 64us/sample - loss: 0.1086 - acc: 0.9675 - val_loss: 0.1493 - val_acc: 0.9590\n",
      "Epoch 8/30\n",
      "62650/62650 [==============================] - 4s 70us/sample - loss: 0.1018 - acc: 0.9701 - val_loss: 0.1416 - val_acc: 0.9607\n",
      "Epoch 9/30\n",
      "62650/62650 [==============================] - 4s 59us/sample - loss: 0.0995 - acc: 0.9697 - val_loss: 0.1238 - val_acc: 0.9660\n",
      "Epoch 10/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0907 - acc: 0.9725 - val_loss: 0.1429 - val_acc: 0.9621\n",
      "Epoch 11/30\n",
      "62650/62650 [==============================] - 4s 59us/sample - loss: 0.0851 - acc: 0.9745 - val_loss: 0.1458 - val_acc: 0.9616\n",
      "Epoch 12/30\n",
      "62650/62650 [==============================] - 4s 63us/sample - loss: 0.0833 - acc: 0.9747 - val_loss: 0.1443 - val_acc: 0.9627\n",
      "Epoch 13/30\n",
      "62650/62650 [==============================] - 4s 61us/sample - loss: 0.0788 - acc: 0.9762 - val_loss: 0.1505 - val_acc: 0.9612\n",
      "Epoch 14/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0775 - acc: 0.9759 - val_loss: 0.1340 - val_acc: 0.9646\n",
      "Epoch 15/30\n",
      "62650/62650 [==============================] - 4s 59us/sample - loss: 0.0787 - acc: 0.9764 - val_loss: 0.1445 - val_acc: 0.9623\n",
      "Epoch 16/30\n",
      "62650/62650 [==============================] - 4s 60us/sample - loss: 0.0717 - acc: 0.9781 - val_loss: 0.1569 - val_acc: 0.9612\n",
      "Epoch 17/30\n",
      "62650/62650 [==============================] - 4s 56us/sample - loss: 0.0700 - acc: 0.9788 - val_loss: 0.1425 - val_acc: 0.9634\n",
      "Epoch 18/30\n",
      "62650/62650 [==============================] - 3s 55us/sample - loss: 0.0700 - acc: 0.9791 - val_loss: 0.1251 - val_acc: 0.9687\n",
      "Epoch 19/30\n",
      "62650/62650 [==============================] - 3s 55us/sample - loss: 0.0682 - acc: 0.9786 - val_loss: 0.1502 - val_acc: 0.9634\n",
      "Epoch 20/30\n",
      "62650/62650 [==============================] - 3s 55us/sample - loss: 0.0668 - acc: 0.9789 - val_loss: 0.1426 - val_acc: 0.9655\n",
      "Epoch 21/30\n",
      "62650/62650 [==============================] - 3s 56us/sample - loss: 0.0655 - acc: 0.9792 - val_loss: 0.1589 - val_acc: 0.9623\n",
      "Epoch 22/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0601 - acc: 0.9812 - val_loss: 0.1741 - val_acc: 0.9587\n",
      "Epoch 23/30\n",
      "62650/62650 [==============================] - 4s 56us/sample - loss: 0.0633 - acc: 0.9800 - val_loss: 0.1697 - val_acc: 0.9605\n",
      "Epoch 24/30\n",
      "62650/62650 [==============================] - 4s 58us/sample - loss: 0.0572 - acc: 0.9814 - val_loss: 0.1572 - val_acc: 0.9637\n",
      "Epoch 25/30\n",
      "62650/62650 [==============================] - 4s 66us/sample - loss: 0.0570 - acc: 0.9821 - val_loss: 0.1655 - val_acc: 0.9604\n",
      "Epoch 26/30\n",
      "62650/62650 [==============================] - 4s 63us/sample - loss: 0.0572 - acc: 0.9823 - val_loss: 0.1632 - val_acc: 0.9622\n",
      "Epoch 27/30\n",
      "62650/62650 [==============================] - 4s 64us/sample - loss: 0.0560 - acc: 0.9824 - val_loss: 0.1488 - val_acc: 0.9642\n",
      "Epoch 28/30\n",
      "62650/62650 [==============================] - 4s 63us/sample - loss: 0.0530 - acc: 0.9826 - val_loss: 0.1747 - val_acc: 0.9603\n",
      "Epoch 29/30\n",
      "62650/62650 [==============================] - 4s 57us/sample - loss: 0.0533 - acc: 0.9836 - val_loss: 0.1710 - val_acc: 0.9613\n",
      "Epoch 30/30\n",
      "62650/62650 [==============================] - 4s 59us/sample - loss: 0.0509 - acc: 0.9835 - val_loss: 0.1579 - val_acc: 0.9638\n",
      "oversampling test loss:  0.1579386093760404\n",
      "oversampling accuracy:  0.9638\n"
     ]
    }
   ],
   "source": [
    "oversampling_list =[]\n",
    "for i in range(10):\n",
    "    bl_model = build_model()\n",
    "    batch_size=64\n",
    "    epochs=30\n",
    "    \n",
    "    rus = RandomOverSampler(random_state=42)\n",
    "    X_train, y_train1 = rus.fit_resample(train_x, train_y)\n",
    "    \n",
    "    bl_history = bl_model.fit(X_train, y_train1, batch_size=batch_size,\n",
    "                        epochs=epochs, validation_data=(test_images, test_y))\n",
    "\n",
    "    bl_score = bl_model.evaluate(test_images, test_y, verbose=0)\n",
    "    print('oversampling test loss: ', bl_score[0])\n",
    "    print('oversampling accuracy: ', bl_score[1] )\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    y_pred_oh = bl_model.predict(test_images)\n",
    "    y_pred_baseline = y_pred_oh.argmax(axis=-1)\n",
    "    oversampling_list.append(classification_report(test_labels, y_pred_baseline, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "baabc776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of runs: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.992434</td>\n",
       "      <td>0.922347</td>\n",
       "      <td>0.956056</td>\n",
       "      <td>980.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.996169</td>\n",
       "      <td>0.937093</td>\n",
       "      <td>0.965641</td>\n",
       "      <td>1135.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.959189</td>\n",
       "      <td>0.974709</td>\n",
       "      <td>0.966845</td>\n",
       "      <td>1032.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.953741</td>\n",
       "      <td>0.969802</td>\n",
       "      <td>0.961623</td>\n",
       "      <td>1010.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.964108</td>\n",
       "      <td>0.969857</td>\n",
       "      <td>0.966927</td>\n",
       "      <td>982.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.944908</td>\n",
       "      <td>0.962892</td>\n",
       "      <td>0.953676</td>\n",
       "      <td>892.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.953888</td>\n",
       "      <td>0.977453</td>\n",
       "      <td>0.965486</td>\n",
       "      <td>958.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.962812</td>\n",
       "      <td>0.968969</td>\n",
       "      <td>0.965796</td>\n",
       "      <td>1028.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.922545</td>\n",
       "      <td>0.962628</td>\n",
       "      <td>0.941973</td>\n",
       "      <td>974.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.953157</td>\n",
       "      <td>0.958276</td>\n",
       "      <td>0.955641</td>\n",
       "      <td>1009.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.960120</td>\n",
       "      <td>0.960120</td>\n",
       "      <td>0.960120</td>\n",
       "      <td>0.96012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.960295</td>\n",
       "      <td>0.960403</td>\n",
       "      <td>0.959966</td>\n",
       "      <td>10000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.960990</td>\n",
       "      <td>0.960120</td>\n",
       "      <td>0.960166</td>\n",
       "      <td>10000.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.992434  0.922347  0.956056    980.00000\n",
       "1              0.996169  0.937093  0.965641   1135.00000\n",
       "2              0.959189  0.974709  0.966845   1032.00000\n",
       "3              0.953741  0.969802  0.961623   1010.00000\n",
       "4              0.964108  0.969857  0.966927    982.00000\n",
       "5              0.944908  0.962892  0.953676    892.00000\n",
       "6              0.953888  0.977453  0.965486    958.00000\n",
       "7              0.962812  0.968969  0.965796   1028.00000\n",
       "8              0.922545  0.962628  0.941973    974.00000\n",
       "9              0.953157  0.958276  0.955641   1009.00000\n",
       "accuracy       0.960120  0.960120  0.960120      0.96012\n",
       "macro avg      0.960295  0.960403  0.959966  10000.00000\n",
       "weighted avg   0.960990  0.960120  0.960166  10000.00000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_process_results(oversampling_list, 'results_csv/mnist_oversampling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddc80a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4726aa8c3a011139ee9eae324612f94ded5d9a6c6a3363e2331a9b86c3055c02"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
