{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f719fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdaedbd",
   "metadata": {},
   "source": [
    "## To do list \n",
    "1. Add mlp\n",
    "2. run with smaller h_dim and z_dim \n",
    "3. minority class only VAE\n",
    "4. Meeting on monday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2632123e",
   "metadata": {},
   "source": [
    "# To do list 2\n",
    "1. drop smaller classes ecoli\n",
    "2. reduce the number of samples in the mnist (1:10, 1:15, 1:20)\n",
    "3. averaging all experiments over ten runs\n",
    "4. introduce imbalance in optical digits (one minority class)\n",
    "5. trying different encoding dim n/2, n/4, ... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58774d1d",
   "metadata": {},
   "source": [
    "# To do list 3\n",
    "1. resample after training vae and train a different classifier (x10 and report average)\n",
    "2. train ecoli over ten test-train split and average the results (mean and std  also)\n",
    "3. additional datasets for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83617bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images,test_labels) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "402b3004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = [np.reshape(x, -1) for x in train_images]\n",
    "test_images= [np.reshape(x, -1) for x in test_images]\n",
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0643ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def group_all_labels(data, num=100, minor=[]):\n",
    "    # this function is to limit the number of labels that are used\n",
    "    # it returns the indexes according the labels\n",
    "    # data is an array of labels\n",
    "    '''\n",
    "\n",
    "    :param data: array of labels\n",
    "    :param num: number required\n",
    "    :param minor: list of minority indexes\n",
    "    :return: array of labels indexes\n",
    "    '''\n",
    "\n",
    "    labels = np.unique(data)\n",
    "    co_l = []\n",
    "    min_col =[]\n",
    "    if not minor:\n",
    "        for l in labels:\n",
    "            el_l = np.where(np.array(data) == l)\n",
    "            co_l.append(el_l[0])\n",
    "\n",
    "    else:\n",
    "        for l in labels:\n",
    "            if l in minor:\n",
    "                el_l = np.where(np.array(data) == l)\n",
    "                co_l.append((el_l[0])[:num])\n",
    "                min_col.append((el_l[0])[:num])\n",
    "            else:\n",
    "                el_l = np.where(np.array(data) == l)\n",
    "                co_l.append(el_l[0])\n",
    "    return co_l, min_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e12951a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0 : number of samples : 250\n",
      "class 1 : number of samples : 250\n",
      "class 2 : number of samples : 5958\n",
      "class 3 : number of samples : 6131\n",
      "class 4 : number of samples : 5842\n",
      "class 5 : number of samples : 5421\n",
      "class 6 : number of samples : 5918\n",
      "class 7 : number of samples : 6265\n",
      "class 8 : number of samples : 5851\n",
      "class 9 : number of samples : 5949\n"
     ]
    }
   ],
   "source": [
    "grouped_labels, min_label = group_all_labels(train_labels, 250, [0, 1])\n",
    "gr_data = []\n",
    "gr_labels = [] \n",
    "for index, q in enumerate(grouped_labels):\n",
    "    print('class {} : number of samples : {}'.format(index,len(q)))\n",
    "    for r in q:\n",
    "        gr_data.append(train_images[r])\n",
    "        gr_labels.append(train_labels[r])\n",
    "\n",
    "gr_min_data = []\n",
    "gr_min_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5861d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority data only\n",
      "class 0 : number of samples : 250\n",
      "class 1 : number of samples : 250\n"
     ]
    }
   ],
   "source": [
    "print('minority data only')\n",
    "gr_min_data = []\n",
    "gr_min_labels = []\n",
    "for index, q in enumerate(min_label):\n",
    "    print('class {} : number of samples : {}'.format(index,len(q)))\n",
    "    for r in q:\n",
    "        gr_min_data.append(train_images[r])\n",
    "        gr_min_labels.append(train_labels[r])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45fad581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f889abee850>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_x = np.array(gr_data)\n",
    "train_x = (train_x.astype(np.float32) / 255.0) \n",
    "test_x =  np.array(test_images)\n",
    "test_images = (test_x.astype(np.float32)/255.0)\n",
    "\n",
    "train_min_x = np.array(gr_min_data)\n",
    "train_min_x = (train_min_x.astype(np.float32) / 255.0) \n",
    "\n",
    "train_y = tf.keras.utils.to_categorical(gr_labels, num_classes=10, dtype='float32')\n",
    "test_y = tf.keras.utils.to_categorical(test_labels, num_classes=10, dtype='float32')\n",
    "train_min_y = tf.keras.utils.to_categorical(gr_min_labels, num_classes=2, dtype='float32')\n",
    "\n",
    "print(test_y[0])\n",
    "plt.imshow(np.reshape(test_x[0],(28,28)), cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5635c9da",
   "metadata": {},
   "source": [
    "#  VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1039ac98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-11 07:19:21.727115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-11 07:19:21.842079: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2021-10-11 07:19:21.854457: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-11 07:19:21.856059: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1  as tf\n",
    "tf.disable_v2_behavior()\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "mb_size = 64\n",
    "z_dim = 100\n",
    "X_dim = 784\n",
    "y_dim = 10\n",
    "h_dim = 128\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "def plot(samples, sz, shape):\n",
    "    fig = plt.figure(figsize=(sz, sz))\n",
    "    gs = gridspec.GridSpec(sz, sz)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(shape, shape), cmap='Greys_r')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random.normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "\n",
    "# Q(z|X) \n",
    "\n",
    "X = tf.keras.Input(shape=(X_dim,))\n",
    "c = tf.keras.Input(shape=(y_dim,))\n",
    "z = tf.keras.Input(shape=(z_dim,))\n",
    "\n",
    "Q_W1 = tf.Variable(xavier_init([X_dim + y_dim, h_dim]))\n",
    "Q_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "Q_W2_mu = tf.Variable(xavier_init([h_dim, z_dim]))\n",
    "Q_b2_mu = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "Q_W2_sigma = tf.Variable(xavier_init([h_dim, z_dim]))\n",
    "Q_b2_sigma = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "\n",
    "def Q(X, c):\n",
    "    inputs = tf.concat(axis=1, values=[X, c])\n",
    "    h = tf.nn.relu(tf.matmul(inputs, Q_W1) + Q_b1)\n",
    "    z_mu = tf.matmul(h, Q_W2_mu) + Q_b2_mu\n",
    "    z_logvar = tf.matmul(h, Q_W2_sigma) + Q_b2_sigma\n",
    "    return z_mu, z_logvar\n",
    "\n",
    "\n",
    "def sample_z(mu, log_var):\n",
    "    eps = tf.random.normal(shape=tf.shape(mu))\n",
    "    return mu + tf.exp(log_var / 2) * eps\n",
    "\n",
    "\n",
    "# P(X|z)\n",
    "\n",
    "P_W1 = tf.Variable(xavier_init([z_dim + y_dim, h_dim]))\n",
    "P_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "P_W2 = tf.Variable(xavier_init([h_dim, X_dim]))\n",
    "P_b2 = tf.Variable(tf.zeros(shape=[X_dim]))\n",
    "\n",
    "\n",
    "def P(z, c):\n",
    "    inputs = tf.concat(axis=1, values=[z, c])\n",
    "    h = tf.nn.relu(tf.matmul(inputs, P_W1) + P_b1)\n",
    "    logits = tf.matmul(h, P_W2) + P_b2\n",
    "    prob = tf.nn.sigmoid(logits)\n",
    "    return prob, logits\n",
    "\n",
    "z_mu, z_logvar = Q(X, c)\n",
    "z_sample = sample_z(z_mu, z_logvar)\n",
    "_, logits = P(z_sample, c)\n",
    "\n",
    "# Sampling from random z\n",
    "X_samples, _ = P(z, c)\n",
    "\n",
    "# E[log P(X|z)]\n",
    "recon_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=X), 1)\n",
    "kl_loss = 0.5 * tf.reduce_sum(tf.exp(z_logvar) + z_mu**2 - 1. - z_logvar, 1)\n",
    "\n",
    "# VAE loss\n",
    "vae_loss = tf.reduce_mean(recon_loss + kl_loss)\n",
    "\n",
    "# gradient step\n",
    "solver = tf.compat.v1.train.AdamOptimizer().minimize(vae_loss)\n",
    "sess = tf.compat.v1.Session ()\n",
    "sess.run(\n",
    "tf.compat.v1.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b6f9797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0 Loss: 762.9\n",
      "Iter: 1000 Loss: 146.3\n",
      "Iter: 2000 Loss: 127.2\n",
      "Iter: 3000 Loss: 119.6\n",
      "Iter: 4000 Loss: 117.5\n",
      "Iter: 5000 Loss: 113.7\n",
      "Iter: 6000 Loss: 114.0\n",
      "Iter: 7000 Loss: 119.5\n",
      "Iter: 8000 Loss: 108.7\n",
      "Iter: 9000 Loss: 115.5\n",
      "Iter: 10000 Loss: 112.0\n",
      "Iter: 11000 Loss: 108.8\n",
      "Iter: 12000 Loss: 115.7\n",
      "Iter: 13000 Loss: 110.1\n",
      "Iter: 14000 Loss: 111.0\n",
      "Iter: 15000 Loss: 107.4\n",
      "Iter: 16000 Loss: 111.5\n",
      "Iter: 17000 Loss: 112.8\n",
      "Iter: 18000 Loss: 108.3\n",
      "Iter: 19000 Loss: 105.8\n",
      "Iter: 20000 Loss: 112.3\n",
      "Iter: 21000 Loss: 102.8\n",
      "Iter: 22000 Loss: 115.5\n",
      "Iter: 23000 Loss: 109.2\n",
      "Iter: 24000 Loss: 106.5\n",
      "Iter: 25000 Loss: 109.4\n",
      "Iter: 26000 Loss: 106.9\n",
      "Iter: 27000 Loss: 109.5\n",
      "Iter: 28000 Loss: 107.7\n",
      "Iter: 29000 Loss: 107.0\n",
      "Iter: 30000 Loss: 106.4\n",
      "Iter: 31000 Loss: 111.7\n",
      "Iter: 32000 Loss: 105.7\n",
      "Iter: 33000 Loss: 106.7\n",
      "Iter: 34000 Loss: 110.2\n",
      "Iter: 35000 Loss: 104.7\n",
      "Iter: 36000 Loss: 106.7\n",
      "Iter: 37000 Loss: 108.1\n",
      "Iter: 38000 Loss: 105.5\n",
      "Iter: 39000 Loss: 105.6\n",
      "Iter: 40000 Loss: 105.6\n",
      "Iter: 41000 Loss: 106.9\n",
      "Iter: 42000 Loss: 107.8\n",
      "Iter: 43000 Loss: 110.4\n",
      "Iter: 44000 Loss: 104.3\n",
      "Iter: 45000 Loss: 107.6\n",
      "Iter: 46000 Loss: 106.6\n",
      "Iter: 47000 Loss: 104.8\n",
      "Iter: 48000 Loss: 107.8\n",
      "Iter: 49000 Loss: 111.4\n",
      "Iter: 50000 Loss: 108.4\n",
      "Iter: 51000 Loss: 107.5\n",
      "Iter: 52000 Loss: 107.4\n",
      "Iter: 53000 Loss: 105.0\n",
      "Iter: 54000 Loss: 103.4\n",
      "Iter: 55000 Loss: 105.2\n",
      "Iter: 56000 Loss: 107.4\n",
      "Iter: 57000 Loss: 108.5\n",
      "Iter: 58000 Loss: 108.7\n",
      "Iter: 59000 Loss: 102.2\n",
      "Iter: 60000 Loss: 105.1\n",
      "Iter: 61000 Loss: 106.3\n",
      "Iter: 62000 Loss: 104.2\n",
      "Iter: 63000 Loss: 112.3\n",
      "Iter: 64000 Loss: 111.9\n",
      "Iter: 65000 Loss: 109.4\n",
      "Iter: 66000 Loss: 104.0\n",
      "Iter: 67000 Loss: 104.6\n",
      "Iter: 68000 Loss: 106.6\n",
      "Iter: 69000 Loss: 107.6\n",
      "Iter: 70000 Loss: 104.4\n",
      "Iter: 71000 Loss: 108.2\n",
      "Iter: 72000 Loss: 104.2\n",
      "Iter: 73000 Loss: 104.3\n",
      "Iter: 74000 Loss: 106.2\n",
      "Iter: 75000 Loss: 106.3\n",
      "Iter: 76000 Loss: 107.1\n",
      "Iter: 77000 Loss: 106.4\n",
      "Iter: 78000 Loss: 104.6\n",
      "Iter: 79000 Loss: 110.2\n",
      "Iter: 80000 Loss: 108.0\n",
      "Iter: 81000 Loss: 99.71\n",
      "Iter: 82000 Loss: 105.8\n",
      "Iter: 83000 Loss: 106.7\n",
      "Iter: 84000 Loss: 107.5\n",
      "Iter: 85000 Loss: 105.8\n",
      "Iter: 86000 Loss: 105.5\n",
      "Iter: 87000 Loss: 105.8\n",
      "Iter: 88000 Loss: 109.1\n",
      "Iter: 89000 Loss: 104.1\n",
      "Iter: 90000 Loss: 108.4\n",
      "Iter: 91000 Loss: 110.9\n",
      "Iter: 92000 Loss: 103.4\n",
      "Iter: 93000 Loss: 104.6\n",
      "Iter: 94000 Loss: 103.2\n",
      "Iter: 95000 Loss: 109.7\n",
      "Iter: 96000 Loss: 105.7\n",
      "Iter: 97000 Loss: 102.1\n",
      "Iter: 98000 Loss: 100.8\n",
      "Iter: 99000 Loss: 107.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# if not os.path.exists('vae_mnist_250/'):\n",
    "#     os.makedirs('vae_mnist_250/')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for it in range(100000):\n",
    "    ind = np.random.choice(train_x.shape[0], mb_size)\n",
    "    X_mb = np.array(train_x[ind])\n",
    "    y_mb = np.array(train_y[ind])\n",
    "    \n",
    "    _, loss = sess.run([solver, vae_loss], feed_dict={X: X_mb, c: y_mb})\n",
    "\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter: {:0} Loss: {:0.4}'.format(it, loss))\n",
    "        \n",
    "#         samples=[]\n",
    "#         for index in range(y_dim):\n",
    "#             y = np.zeros([y_dim, y_dim])\n",
    "#             y[range(y_dim), index] = 1\n",
    "#             samples.extend(sess.run(X_samples,\n",
    "#                            feed_dict={z: np.random.randn(y_dim, z_dim), c: y}))\n",
    "\n",
    "#         fig = plot(samples, 10, 28)\n",
    "#         plt.savefig('vae_mnist_250/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "#         i += 1\n",
    "#         plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b35943d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples():\n",
    "    samples = []\n",
    "    gen_labels =[]\n",
    "    for r in range(62):\n",
    "        for index in range(2):\n",
    "            gen_labels = gen_labels + [index]*mb_size\n",
    "            y = np.zeros([mb_size, y_dim])\n",
    "            y[range(mb_size), index] = 1\n",
    "            samples.extend(sess.run(X_samples,\n",
    "                                   feed_dict={z: np.random.randn(mb_size, z_dim), c: y}))\n",
    "\n",
    "    gen_samples = np.array(samples)\n",
    "    gen_labels = np.array(gen_labels)\n",
    "    print(gen_samples.shape)\n",
    "    print(gen_labels.shape)\n",
    "    print(gen_labels[0])\n",
    "    \n",
    "    return gen_samples, gen_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dcfc5e",
   "metadata": {},
   "source": [
    "# Visualize generated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90a9ed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f886c794340>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "aug_data, aug_labels = generate_samples()\n",
    "plt.imshow(np.reshape(aug_data[0],(28,28)), cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe354d",
   "metadata": {},
   "source": [
    "# MLP Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6c6de50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Input, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "def build_model(input_shape=(784,), num_classes=10):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_shape: shape of input_data\n",
    "    :param num_classes: number of classes\n",
    "    :return: keras.model.sequential compiled with categorical cross-entropy loss\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6037773",
   "metadata": {},
   "source": [
    "# Base line classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4588a026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/layers/normalization/batch_normalization.py:520: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 47835 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "47835/47835 [==============================] - 2s 40us/sample - loss: 0.6048 - acc: 0.8172 - val_loss: 0.4964 - val_acc: 0.8243\n",
      "Epoch 2/30\n",
      "   64/47835 [..............................] - ETA: 3s - loss: 0.5793 - acc: 0.8281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47835/47835 [==============================] - 2s 35us/sample - loss: 0.2890 - acc: 0.9147 - val_loss: 0.3026 - val_acc: 0.9043\n",
      "Epoch 3/30\n",
      "47835/47835 [==============================] - 2s 32us/sample - loss: 0.2317 - acc: 0.9331 - val_loss: 0.2817 - val_acc: 0.9135\n",
      "Epoch 4/30\n",
      "47835/47835 [==============================] - 1s 30us/sample - loss: 0.1953 - acc: 0.9426 - val_loss: 0.2346 - val_acc: 0.9273\n",
      "Epoch 5/30\n",
      "47835/47835 [==============================] - 1s 30us/sample - loss: 0.1760 - acc: 0.9477 - val_loss: 0.2103 - val_acc: 0.9364\n",
      "Epoch 6/30\n",
      "47835/47835 [==============================] - 1s 29us/sample - loss: 0.1600 - acc: 0.9523 - val_loss: 0.1734 - val_acc: 0.9471\n",
      "Epoch 7/30\n",
      "47835/47835 [==============================] - 1s 29us/sample - loss: 0.1444 - acc: 0.9565 - val_loss: 0.1817 - val_acc: 0.9455\n",
      "Epoch 8/30\n",
      "47835/47835 [==============================] - 1s 29us/sample - loss: 0.1428 - acc: 0.9575 - val_loss: 0.1588 - val_acc: 0.9515\n",
      "Epoch 9/30\n",
      "47835/47835 [==============================] - 1s 29us/sample - loss: 0.1331 - acc: 0.9587 - val_loss: 0.1517 - val_acc: 0.9549\n",
      "Epoch 10/30\n",
      "47835/47835 [==============================] - 1s 30us/sample - loss: 0.1251 - acc: 0.9621 - val_loss: 0.1704 - val_acc: 0.9489\n",
      "Epoch 11/30\n",
      "47835/47835 [==============================] - 2s 33us/sample - loss: 0.1195 - acc: 0.9649 - val_loss: 0.1713 - val_acc: 0.9481\n",
      "Epoch 12/30\n",
      "47835/47835 [==============================] - 1s 30us/sample - loss: 0.1165 - acc: 0.9647 - val_loss: 0.1475 - val_acc: 0.9560\n",
      "Epoch 13/30\n",
      "47835/47835 [==============================] - 1s 31us/sample - loss: 0.1092 - acc: 0.9674 - val_loss: 0.1719 - val_acc: 0.9502\n",
      "Epoch 14/30\n",
      "47835/47835 [==============================] - 1s 30us/sample - loss: 0.1064 - acc: 0.9671 - val_loss: 0.1453 - val_acc: 0.9584\n",
      "Epoch 15/30\n",
      "47835/47835 [==============================] - 1s 30us/sample - loss: 0.1028 - acc: 0.9693 - val_loss: 0.1488 - val_acc: 0.9565\n",
      "Epoch 16/30\n",
      "47835/47835 [==============================] - 1s 29us/sample - loss: 0.0991 - acc: 0.9692 - val_loss: 0.1341 - val_acc: 0.9621\n",
      "Epoch 17/30\n",
      "47835/47835 [==============================] - 1s 30us/sample - loss: 0.1009 - acc: 0.9692 - val_loss: 0.1509 - val_acc: 0.9569\n",
      "Epoch 18/30\n",
      "47835/47835 [==============================] - 1s 29us/sample - loss: 0.0939 - acc: 0.9722 - val_loss: 0.1753 - val_acc: 0.9476\n",
      "Epoch 19/30\n",
      "47835/47835 [==============================] - 1s 30us/sample - loss: 0.0949 - acc: 0.9714 - val_loss: 0.1867 - val_acc: 0.9454\n",
      "Epoch 20/30\n",
      "47835/47835 [==============================] - 1s 30us/sample - loss: 0.0908 - acc: 0.9715 - val_loss: 0.1528 - val_acc: 0.9548\n",
      "Epoch 21/30\n",
      "47835/47835 [==============================] - 2s 32us/sample - loss: 0.0896 - acc: 0.9727 - val_loss: 0.1623 - val_acc: 0.9534\n",
      "Epoch 22/30\n",
      "47835/47835 [==============================] - 2s 34us/sample - loss: 0.0848 - acc: 0.9736 - val_loss: 0.1496 - val_acc: 0.9581\n",
      "Epoch 23/30\n",
      "47835/47835 [==============================] - 2s 35us/sample - loss: 0.0859 - acc: 0.9738 - val_loss: 0.1247 - val_acc: 0.9651\n",
      "Epoch 24/30\n",
      "47835/47835 [==============================] - 1s 31us/sample - loss: 0.0813 - acc: 0.9747 - val_loss: 0.1363 - val_acc: 0.9609\n",
      "Epoch 25/30\n",
      "47835/47835 [==============================] - 2s 34us/sample - loss: 0.0816 - acc: 0.9747 - val_loss: 0.1544 - val_acc: 0.9558\n",
      "Epoch 26/30\n",
      "47835/47835 [==============================] - 2s 33us/sample - loss: 0.0801 - acc: 0.9750 - val_loss: 0.1388 - val_acc: 0.9605\n",
      "Epoch 27/30\n",
      "47835/47835 [==============================] - 2s 35us/sample - loss: 0.0776 - acc: 0.9759 - val_loss: 0.1573 - val_acc: 0.9559\n",
      "Epoch 28/30\n",
      "47835/47835 [==============================] - 1s 30us/sample - loss: 0.0778 - acc: 0.9756 - val_loss: 0.1472 - val_acc: 0.9569\n",
      "Epoch 29/30\n",
      "47835/47835 [==============================] - 1s 30us/sample - loss: 0.0779 - acc: 0.9755 - val_loss: 0.1496 - val_acc: 0.9586\n",
      "Epoch 30/30\n",
      "47835/47835 [==============================] - 1s 29us/sample - loss: 0.0739 - acc: 0.9770 - val_loss: 0.1361 - val_acc: 0.9629\n",
      "baseline test loss:  0.136085989095876\n",
      "baseline test accuracy:  0.9629\n",
      "MLP classification baseline\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95       980\n",
      "           1       0.99      0.94      0.97      1135\n",
      "           2       0.94      0.98      0.96      1032\n",
      "           3       0.95      0.98      0.96      1010\n",
      "           4       0.97      0.97      0.97       982\n",
      "           5       0.96      0.96      0.96       892\n",
      "           6       0.94      0.98      0.96       958\n",
      "           7       0.97      0.97      0.97      1028\n",
      "           8       0.95      0.96      0.95       974\n",
      "           9       0.96      0.97      0.97      1009\n",
      "\n",
      "    accuracy                           0.96     10000\n",
      "   macro avg       0.96      0.96      0.96     10000\n",
      "weighted avg       0.96      0.96      0.96     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bl_model = build_model()\n",
    "batch_size=64\n",
    "epochs=30\n",
    "bl_history = bl_model.fit(train_x, train_y, batch_size=batch_size,\n",
    "                    epochs=epochs, validation_data=(test_images, test_y))\n",
    "\n",
    "bl_score = bl_model.evaluate(test_images, test_y, verbose=0)\n",
    "print('baseline test loss: ', bl_score[0])\n",
    "print('baseline test accuracy: ', bl_score[1] )\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred_oh = bl_model.predict(test_images)\n",
    "y_pred_baseline = y_pred_oh.argmax(axis=-1)\n",
    "print('MLP classification baseline\\n', classification_report(test_labels, y_pred_baseline))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efe72b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(bl_history.history.keys())\n",
    "plt.plot(bl_history.history['acc'])\n",
    "plt.plot(bl_history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "plt.plot(bl_history.history['loss'])\n",
    "plt.plot(bl_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7165beb",
   "metadata": {},
   "source": [
    "#  Augmentation experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7d5b322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 55771 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.6194 - acc: 0.8110 - val_loss: 0.2512 - val_acc: 0.9218\n",
      "Epoch 2/30\n",
      "   64/55771 [..............................] - ETA: 3s - loss: 0.2510 - acc: 0.9219"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.2973 - acc: 0.9120 - val_loss: 0.1986 - val_acc: 0.9386\n",
      "Epoch 3/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.2373 - acc: 0.9308 - val_loss: 0.1755 - val_acc: 0.9461\n",
      "Epoch 4/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.2056 - acc: 0.9389 - val_loss: 0.1642 - val_acc: 0.9494\n",
      "Epoch 5/30\n",
      "55771/55771 [==============================] - 2s 33us/sample - loss: 0.1857 - acc: 0.9447 - val_loss: 0.1658 - val_acc: 0.9489\n",
      "Epoch 6/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1712 - acc: 0.9493 - val_loss: 0.1479 - val_acc: 0.9572\n",
      "Epoch 7/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1605 - acc: 0.9519 - val_loss: 0.1448 - val_acc: 0.9555\n",
      "Epoch 8/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1479 - acc: 0.9562 - val_loss: 0.1671 - val_acc: 0.9517\n",
      "Epoch 9/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1425 - acc: 0.9571 - val_loss: 0.1495 - val_acc: 0.9552\n",
      "Epoch 10/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1375 - acc: 0.9581 - val_loss: 0.1512 - val_acc: 0.9559\n",
      "Epoch 11/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1282 - acc: 0.9613 - val_loss: 0.1498 - val_acc: 0.9550\n",
      "Epoch 12/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1257 - acc: 0.9602 - val_loss: 0.1362 - val_acc: 0.9605\n",
      "Epoch 13/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1205 - acc: 0.9629 - val_loss: 0.1468 - val_acc: 0.9578\n",
      "Epoch 14/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1203 - acc: 0.9633 - val_loss: 0.1403 - val_acc: 0.9594\n",
      "Epoch 15/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1137 - acc: 0.9648 - val_loss: 0.1456 - val_acc: 0.9585\n",
      "Epoch 16/30\n",
      "55771/55771 [==============================] - 2s 37us/sample - loss: 0.1136 - acc: 0.9649 - val_loss: 0.1403 - val_acc: 0.9594\n",
      "Epoch 17/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.1108 - acc: 0.9654 - val_loss: 0.1455 - val_acc: 0.9592\n",
      "Epoch 18/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1054 - acc: 0.9669 - val_loss: 0.1556 - val_acc: 0.9561\n",
      "Epoch 19/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1017 - acc: 0.9682 - val_loss: 0.1383 - val_acc: 0.9612\n",
      "Epoch 20/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1009 - acc: 0.9689 - val_loss: 0.1597 - val_acc: 0.9563\n",
      "Epoch 21/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1004 - acc: 0.9688 - val_loss: 0.1467 - val_acc: 0.9568\n",
      "Epoch 22/30\n",
      "55771/55771 [==============================] - 2s 33us/sample - loss: 0.0992 - acc: 0.9693 - val_loss: 0.1428 - val_acc: 0.9605\n",
      "Epoch 23/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.0954 - acc: 0.9704 - val_loss: 0.1740 - val_acc: 0.9546\n",
      "Epoch 24/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.0926 - acc: 0.9711 - val_loss: 0.1526 - val_acc: 0.9584\n",
      "Epoch 25/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.0929 - acc: 0.9711 - val_loss: 0.1567 - val_acc: 0.9578\n",
      "Epoch 26/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.0904 - acc: 0.9714 - val_loss: 0.1446 - val_acc: 0.9606\n",
      "Epoch 27/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.0899 - acc: 0.9715 - val_loss: 0.1576 - val_acc: 0.9575\n",
      "Epoch 28/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.0898 - acc: 0.9718 - val_loss: 0.1542 - val_acc: 0.9594\n",
      "Epoch 29/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.0830 - acc: 0.9743 - val_loss: 0.1475 - val_acc: 0.9617\n",
      "Epoch 30/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.0868 - acc: 0.9724 - val_loss: 0.1415 - val_acc: 0.9640\n",
      "test loss for 0th run:  0.1414659970176057\n",
      "test accuracy for 0th run:  0.964\n",
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 55771 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "55040/55771 [============================>.] - ETA: 0s - loss: 0.5986 - acc: 0.8163"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55771/55771 [==============================] - 2s 39us/sample - loss: 0.5956 - acc: 0.8172 - val_loss: 0.2677 - val_acc: 0.9161\n",
      "Epoch 2/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.3043 - acc: 0.9112 - val_loss: 0.2512 - val_acc: 0.9239\n",
      "Epoch 3/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.2510 - acc: 0.9261 - val_loss: 0.1957 - val_acc: 0.9418\n",
      "Epoch 4/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.2218 - acc: 0.9341 - val_loss: 0.1961 - val_acc: 0.9423\n",
      "Epoch 5/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.2033 - acc: 0.9385 - val_loss: 0.1772 - val_acc: 0.9474\n",
      "Epoch 6/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1810 - acc: 0.9460 - val_loss: 0.1773 - val_acc: 0.9470\n",
      "Epoch 7/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1669 - acc: 0.9496 - val_loss: 0.1678 - val_acc: 0.9491\n",
      "Epoch 8/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1554 - acc: 0.9524 - val_loss: 0.1654 - val_acc: 0.9489\n",
      "Epoch 9/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1502 - acc: 0.9548 - val_loss: 0.1589 - val_acc: 0.9531\n",
      "Epoch 10/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1418 - acc: 0.9574 - val_loss: 0.2017 - val_acc: 0.9384\n",
      "Epoch 11/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1380 - acc: 0.9576 - val_loss: 0.1612 - val_acc: 0.9501\n",
      "Epoch 12/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1323 - acc: 0.9597 - val_loss: 0.1541 - val_acc: 0.9541\n",
      "Epoch 13/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1269 - acc: 0.9616 - val_loss: 0.1642 - val_acc: 0.9498\n",
      "Epoch 14/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1211 - acc: 0.9628 - val_loss: 0.1504 - val_acc: 0.9532\n",
      "Epoch 15/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1218 - acc: 0.9620 - val_loss: 0.1548 - val_acc: 0.9542\n",
      "Epoch 16/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1128 - acc: 0.9649 - val_loss: 0.1444 - val_acc: 0.9568\n",
      "Epoch 17/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1115 - acc: 0.9653 - val_loss: 0.1528 - val_acc: 0.9560\n",
      "Epoch 18/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1120 - acc: 0.9648 - val_loss: 0.1506 - val_acc: 0.9566\n",
      "Epoch 19/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1074 - acc: 0.9669 - val_loss: 0.1461 - val_acc: 0.9599\n",
      "Epoch 20/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1036 - acc: 0.9685 - val_loss: 0.1598 - val_acc: 0.9524\n",
      "Epoch 21/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1047 - acc: 0.9681 - val_loss: 0.1514 - val_acc: 0.9582\n",
      "Epoch 22/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1050 - acc: 0.9666 - val_loss: 0.1518 - val_acc: 0.9553\n",
      "Epoch 23/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1011 - acc: 0.9689 - val_loss: 0.1502 - val_acc: 0.9570\n",
      "Epoch 24/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.0974 - acc: 0.9697 - val_loss: 0.1610 - val_acc: 0.9513\n",
      "Epoch 25/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.0979 - acc: 0.9692 - val_loss: 0.1530 - val_acc: 0.9568\n",
      "Epoch 26/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.0946 - acc: 0.9694 - val_loss: 0.1474 - val_acc: 0.9573\n",
      "Epoch 27/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.0943 - acc: 0.9701 - val_loss: 0.1621 - val_acc: 0.9533\n",
      "Epoch 28/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.0913 - acc: 0.9712 - val_loss: 0.1934 - val_acc: 0.9460\n",
      "Epoch 29/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.0965 - acc: 0.9697 - val_loss: 0.1405 - val_acc: 0.9611\n",
      "Epoch 30/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.0875 - acc: 0.9722 - val_loss: 0.1655 - val_acc: 0.9519\n",
      "test loss for 1th run:  0.16548480467908083\n",
      "test accuracy for 1th run:  0.9519\n",
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 55771 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "55552/55771 [============================>.] - ETA: 0s - loss: 0.5966 - acc: 0.8210"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55771/55771 [==============================] - 2s 38us/sample - loss: 0.5956 - acc: 0.8213 - val_loss: 0.2683 - val_acc: 0.9162\n",
      "Epoch 2/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.3016 - acc: 0.9126 - val_loss: 0.2184 - val_acc: 0.9327\n",
      "Epoch 3/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.2432 - acc: 0.9296 - val_loss: 0.2164 - val_acc: 0.9336\n",
      "Epoch 4/30\n",
      "55771/55771 [==============================] - 2s 33us/sample - loss: 0.2169 - acc: 0.9375 - val_loss: 0.2053 - val_acc: 0.9388\n",
      "Epoch 5/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1949 - acc: 0.9443 - val_loss: 0.1828 - val_acc: 0.9454\n",
      "Epoch 6/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1801 - acc: 0.9470 - val_loss: 0.1916 - val_acc: 0.9400\n",
      "Epoch 7/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1677 - acc: 0.9498 - val_loss: 0.1780 - val_acc: 0.9449\n",
      "Epoch 8/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1535 - acc: 0.9554 - val_loss: 0.1591 - val_acc: 0.9507\n",
      "Epoch 9/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1479 - acc: 0.9571 - val_loss: 0.1626 - val_acc: 0.9503\n",
      "Epoch 10/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1409 - acc: 0.9584 - val_loss: 0.1682 - val_acc: 0.9494\n",
      "Epoch 11/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1359 - acc: 0.9594 - val_loss: 0.1686 - val_acc: 0.9497\n",
      "Epoch 12/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1308 - acc: 0.9616 - val_loss: 0.1729 - val_acc: 0.9496\n",
      "Epoch 13/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.1248 - acc: 0.9632 - val_loss: 0.1655 - val_acc: 0.9503\n",
      "Epoch 14/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.1214 - acc: 0.9637 - val_loss: 0.1467 - val_acc: 0.9550\n",
      "Epoch 15/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.1181 - acc: 0.9654 - val_loss: 0.1650 - val_acc: 0.9509\n",
      "Epoch 16/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.1149 - acc: 0.9656 - val_loss: 0.1641 - val_acc: 0.9537\n",
      "Epoch 17/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.1121 - acc: 0.9656 - val_loss: 0.1613 - val_acc: 0.9537\n",
      "Epoch 18/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1066 - acc: 0.9682 - val_loss: 0.1580 - val_acc: 0.9534\n",
      "Epoch 19/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.1078 - acc: 0.9673 - val_loss: 0.1616 - val_acc: 0.9518\n",
      "Epoch 20/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1036 - acc: 0.9681 - val_loss: 0.1621 - val_acc: 0.9544\n",
      "Epoch 21/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1009 - acc: 0.9696 - val_loss: 0.1682 - val_acc: 0.9525\n",
      "Epoch 22/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1024 - acc: 0.9692 - val_loss: 0.1731 - val_acc: 0.9508\n",
      "Epoch 23/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.1017 - acc: 0.9689 - val_loss: 0.1653 - val_acc: 0.9521\n",
      "Epoch 24/30\n",
      "55771/55771 [==============================] - 2s 33us/sample - loss: 0.0951 - acc: 0.9710 - val_loss: 0.1757 - val_acc: 0.9508\n",
      "Epoch 25/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.0941 - acc: 0.9711 - val_loss: 0.1624 - val_acc: 0.9530\n",
      "Epoch 26/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.0931 - acc: 0.9711 - val_loss: 0.1661 - val_acc: 0.9546\n",
      "Epoch 27/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.0897 - acc: 0.9724 - val_loss: 0.1597 - val_acc: 0.9557\n",
      "Epoch 28/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.0920 - acc: 0.9720 - val_loss: 0.1639 - val_acc: 0.9564\n",
      "Epoch 29/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.0881 - acc: 0.9729 - val_loss: 0.1559 - val_acc: 0.9568\n",
      "Epoch 30/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.0906 - acc: 0.9724 - val_loss: 0.1922 - val_acc: 0.9460\n",
      "test loss for 2th run:  0.19221084783701226\n",
      "test accuracy for 2th run:  0.946\n",
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 55771 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "55680/55771 [============================>.] - ETA: 0s - loss: 0.6064 - acc: 0.8153"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55771/55771 [==============================] - 2s 37us/sample - loss: 0.6058 - acc: 0.8155 - val_loss: 0.2601 - val_acc: 0.9183\n",
      "Epoch 2/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.3040 - acc: 0.9101 - val_loss: 0.1999 - val_acc: 0.9362\n",
      "Epoch 3/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.2495 - acc: 0.9260 - val_loss: 0.1893 - val_acc: 0.9415\n",
      "Epoch 4/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.2168 - acc: 0.9356 - val_loss: 0.1885 - val_acc: 0.9408\n",
      "Epoch 5/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1965 - acc: 0.9412 - val_loss: 0.1648 - val_acc: 0.9493\n",
      "Epoch 6/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1771 - acc: 0.9476 - val_loss: 0.1783 - val_acc: 0.9455\n",
      "Epoch 7/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1649 - acc: 0.9517 - val_loss: 0.1648 - val_acc: 0.9478\n",
      "Epoch 8/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1525 - acc: 0.9545 - val_loss: 0.1841 - val_acc: 0.9448\n",
      "Epoch 9/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1484 - acc: 0.9559 - val_loss: 0.1599 - val_acc: 0.9530\n",
      "Epoch 10/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1432 - acc: 0.9560 - val_loss: 0.1476 - val_acc: 0.9543\n",
      "Epoch 11/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1351 - acc: 0.9593 - val_loss: 0.1548 - val_acc: 0.9535\n",
      "Epoch 12/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1307 - acc: 0.9600 - val_loss: 0.1605 - val_acc: 0.9547\n",
      "Epoch 13/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1268 - acc: 0.9617 - val_loss: 0.1482 - val_acc: 0.9576\n",
      "Epoch 14/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1218 - acc: 0.9618 - val_loss: 0.1487 - val_acc: 0.9547\n",
      "Epoch 15/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1208 - acc: 0.9625 - val_loss: 0.1467 - val_acc: 0.9561\n",
      "Epoch 16/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1142 - acc: 0.9647 - val_loss: 0.1512 - val_acc: 0.9586\n",
      "Epoch 17/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.1106 - acc: 0.9656 - val_loss: 0.1509 - val_acc: 0.9553\n",
      "Epoch 18/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1086 - acc: 0.9665 - val_loss: 0.1514 - val_acc: 0.9567\n",
      "Epoch 19/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.1058 - acc: 0.9675 - val_loss: 0.1529 - val_acc: 0.9579\n",
      "Epoch 20/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.1022 - acc: 0.9689 - val_loss: 0.1514 - val_acc: 0.9591\n",
      "Epoch 21/30\n",
      "55771/55771 [==============================] - 2s 33us/sample - loss: 0.0997 - acc: 0.9688 - val_loss: 0.1627 - val_acc: 0.9549\n",
      "Epoch 22/30\n",
      "55771/55771 [==============================] - 2s 33us/sample - loss: 0.0978 - acc: 0.9700 - val_loss: 0.1627 - val_acc: 0.9575\n",
      "Epoch 23/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.0991 - acc: 0.9696 - val_loss: 0.1473 - val_acc: 0.9592\n",
      "Epoch 24/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.0936 - acc: 0.9713 - val_loss: 0.1499 - val_acc: 0.9583\n",
      "Epoch 25/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.0900 - acc: 0.9716 - val_loss: 0.1617 - val_acc: 0.9569\n",
      "Epoch 26/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.0914 - acc: 0.9718 - val_loss: 0.1507 - val_acc: 0.9591\n",
      "Epoch 27/30\n",
      "55771/55771 [==============================] - 2s 43us/sample - loss: 0.0842 - acc: 0.9731 - val_loss: 0.1756 - val_acc: 0.9539\n",
      "Epoch 28/30\n",
      "55771/55771 [==============================] - 2s 44us/sample - loss: 0.0869 - acc: 0.9726 - val_loss: 0.1586 - val_acc: 0.9578\n",
      "Epoch 29/30\n",
      "55771/55771 [==============================] - 2s 42us/sample - loss: 0.0844 - acc: 0.9733 - val_loss: 0.1587 - val_acc: 0.9575\n",
      "Epoch 30/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.0838 - acc: 0.9734 - val_loss: 0.1530 - val_acc: 0.9577\n",
      "test loss for 3th run:  0.15295954237377737\n",
      "test accuracy for 3th run:  0.9577\n",
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 55771 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "54016/55771 [============================>.] - ETA: 0s - loss: 0.6156 - acc: 0.8130"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.6074 - acc: 0.8156 - val_loss: 0.2480 - val_acc: 0.9255\n",
      "Epoch 2/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.3080 - acc: 0.9088 - val_loss: 0.2211 - val_acc: 0.9327\n",
      "Epoch 3/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.2514 - acc: 0.9273 - val_loss: 0.1861 - val_acc: 0.9436\n",
      "Epoch 4/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.2209 - acc: 0.9356 - val_loss: 0.1804 - val_acc: 0.9444\n",
      "Epoch 5/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1962 - acc: 0.9434 - val_loss: 0.1874 - val_acc: 0.9399\n",
      "Epoch 6/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1864 - acc: 0.9440 - val_loss: 0.1700 - val_acc: 0.9471\n",
      "Epoch 7/30\n",
      "55771/55771 [==============================] - 2s 30us/sample - loss: 0.1720 - acc: 0.9486 - val_loss: 0.1525 - val_acc: 0.9536\n",
      "Epoch 8/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1613 - acc: 0.9515 - val_loss: 0.1573 - val_acc: 0.9532\n",
      "Epoch 9/30\n",
      "55771/55771 [==============================] - 2s 41us/sample - loss: 0.1527 - acc: 0.9544 - val_loss: 0.1628 - val_acc: 0.9508\n",
      "Epoch 10/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1489 - acc: 0.9559 - val_loss: 0.1582 - val_acc: 0.9534\n",
      "Epoch 11/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1421 - acc: 0.9581 - val_loss: 0.1511 - val_acc: 0.9544\n",
      "Epoch 12/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.1374 - acc: 0.9588 - val_loss: 0.1574 - val_acc: 0.9521\n",
      "Epoch 13/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1311 - acc: 0.9608 - val_loss: 0.1476 - val_acc: 0.9578\n",
      "Epoch 14/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.1249 - acc: 0.9627 - val_loss: 0.1503 - val_acc: 0.9570\n",
      "Epoch 15/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.1225 - acc: 0.9630 - val_loss: 0.1445 - val_acc: 0.9569\n",
      "Epoch 16/30\n",
      "55771/55771 [==============================] - 2s 42us/sample - loss: 0.1181 - acc: 0.9643 - val_loss: 0.1469 - val_acc: 0.9564\n",
      "Epoch 17/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1187 - acc: 0.9635 - val_loss: 0.1551 - val_acc: 0.9555\n",
      "Epoch 18/30\n",
      "55771/55771 [==============================] - 2s 31us/sample - loss: 0.1174 - acc: 0.9645 - val_loss: 0.1519 - val_acc: 0.9560\n",
      "Epoch 19/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1103 - acc: 0.9666 - val_loss: 0.1561 - val_acc: 0.9544\n",
      "Epoch 20/30\n",
      "55771/55771 [==============================] - 2s 33us/sample - loss: 0.1089 - acc: 0.9660 - val_loss: 0.1376 - val_acc: 0.9605\n",
      "Epoch 21/30\n",
      "55771/55771 [==============================] - 2s 42us/sample - loss: 0.1070 - acc: 0.9667 - val_loss: 0.1461 - val_acc: 0.9590\n",
      "Epoch 22/30\n",
      "55771/55771 [==============================] - 2s 40us/sample - loss: 0.1051 - acc: 0.9677 - val_loss: 0.1554 - val_acc: 0.9542\n",
      "Epoch 23/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.1005 - acc: 0.9688 - val_loss: 0.1519 - val_acc: 0.9554\n",
      "Epoch 24/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.0980 - acc: 0.9698 - val_loss: 0.1448 - val_acc: 0.9566\n",
      "Epoch 25/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.1020 - acc: 0.9684 - val_loss: 0.1389 - val_acc: 0.9597\n",
      "Epoch 26/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.0970 - acc: 0.9688 - val_loss: 0.1446 - val_acc: 0.9586\n",
      "Epoch 27/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.0965 - acc: 0.9704 - val_loss: 0.1576 - val_acc: 0.9545\n",
      "Epoch 28/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.0940 - acc: 0.9700 - val_loss: 0.1549 - val_acc: 0.9561\n",
      "Epoch 29/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.0940 - acc: 0.9702 - val_loss: 0.1487 - val_acc: 0.9583\n",
      "Epoch 30/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.0900 - acc: 0.9715 - val_loss: 0.1403 - val_acc: 0.9593\n",
      "test loss for 4th run:  0.14025663061402738\n",
      "test accuracy for 4th run:  0.9593\n",
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 55771 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "55744/55771 [============================>.] - ETA: 0s - loss: 0.5981 - acc: 0.8213"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55771/55771 [==============================] - 2s 41us/sample - loss: 0.5981 - acc: 0.8213 - val_loss: 0.2475 - val_acc: 0.9241\n",
      "Epoch 2/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.2899 - acc: 0.9143 - val_loss: 0.2048 - val_acc: 0.9349\n",
      "Epoch 3/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.2308 - acc: 0.9326 - val_loss: 0.1712 - val_acc: 0.9462\n",
      "Epoch 4/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.2023 - acc: 0.9405 - val_loss: 0.1657 - val_acc: 0.9475\n",
      "Epoch 5/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1817 - acc: 0.9461 - val_loss: 0.1636 - val_acc: 0.9494\n",
      "Epoch 6/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1689 - acc: 0.9498 - val_loss: 0.1898 - val_acc: 0.9400\n",
      "Epoch 7/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1540 - acc: 0.9558 - val_loss: 0.1497 - val_acc: 0.9552\n",
      "Epoch 8/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1470 - acc: 0.9566 - val_loss: 0.1576 - val_acc: 0.9522\n",
      "Epoch 9/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1432 - acc: 0.9568 - val_loss: 0.1479 - val_acc: 0.9560\n",
      "Epoch 10/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.1332 - acc: 0.9605 - val_loss: 0.1479 - val_acc: 0.9556\n",
      "Epoch 11/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1308 - acc: 0.9611 - val_loss: 0.1514 - val_acc: 0.9556\n",
      "Epoch 12/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1238 - acc: 0.9634 - val_loss: 0.1455 - val_acc: 0.9567\n",
      "Epoch 13/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.1202 - acc: 0.9639 - val_loss: 0.2073 - val_acc: 0.9421\n",
      "Epoch 14/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1164 - acc: 0.9656 - val_loss: 0.1776 - val_acc: 0.9499\n",
      "Epoch 15/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1146 - acc: 0.9649 - val_loss: 0.1514 - val_acc: 0.9557\n",
      "Epoch 16/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.1127 - acc: 0.9657 - val_loss: 0.1411 - val_acc: 0.9602\n",
      "Epoch 17/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1095 - acc: 0.9663 - val_loss: 0.1507 - val_acc: 0.9556\n",
      "Epoch 18/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1091 - acc: 0.9670 - val_loss: 0.2013 - val_acc: 0.9459\n",
      "Epoch 19/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1028 - acc: 0.9685 - val_loss: 0.1501 - val_acc: 0.9570\n",
      "Epoch 20/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1031 - acc: 0.9689 - val_loss: 0.1532 - val_acc: 0.9573\n",
      "Epoch 21/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.0997 - acc: 0.9691 - val_loss: 0.1575 - val_acc: 0.9575\n",
      "Epoch 22/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.0949 - acc: 0.9708 - val_loss: 0.1497 - val_acc: 0.9602\n",
      "Epoch 23/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.0959 - acc: 0.9708 - val_loss: 0.1687 - val_acc: 0.9557\n",
      "Epoch 24/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.0966 - acc: 0.9703 - val_loss: 0.1591 - val_acc: 0.9537\n",
      "Epoch 25/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.0939 - acc: 0.9714 - val_loss: 0.1559 - val_acc: 0.9581\n",
      "Epoch 26/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.0931 - acc: 0.9708 - val_loss: 0.1799 - val_acc: 0.9521\n",
      "Epoch 27/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.0887 - acc: 0.9727 - val_loss: 0.1584 - val_acc: 0.9564\n",
      "Epoch 28/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.0878 - acc: 0.9720 - val_loss: 0.1668 - val_acc: 0.9540\n",
      "Epoch 29/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.0866 - acc: 0.9726 - val_loss: 0.1583 - val_acc: 0.9568\n",
      "Epoch 30/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.0886 - acc: 0.9719 - val_loss: 0.1654 - val_acc: 0.9545\n",
      "test loss for 5th run:  0.16540009384898002\n",
      "test accuracy for 5th run:  0.9545\n",
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 55771 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "54720/55771 [============================>.] - ETA: 0s - loss: 0.5914 - acc: 0.8229"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55771/55771 [==============================] - 2s 42us/sample - loss: 0.5874 - acc: 0.8242 - val_loss: 0.2689 - val_acc: 0.9179\n",
      "Epoch 2/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.2877 - acc: 0.9172 - val_loss: 0.2179 - val_acc: 0.9325\n",
      "Epoch 3/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.2316 - acc: 0.9327 - val_loss: 0.2146 - val_acc: 0.9342\n",
      "Epoch 4/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.2041 - acc: 0.9409 - val_loss: 0.1828 - val_acc: 0.9411\n",
      "Epoch 5/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.1832 - acc: 0.9463 - val_loss: 0.1656 - val_acc: 0.9479\n",
      "Epoch 6/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.1671 - acc: 0.9509 - val_loss: 0.1611 - val_acc: 0.9496\n",
      "Epoch 7/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.1532 - acc: 0.9547 - val_loss: 0.1565 - val_acc: 0.9523\n",
      "Epoch 8/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.1431 - acc: 0.9588 - val_loss: 0.1922 - val_acc: 0.9402\n",
      "Epoch 9/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.1416 - acc: 0.9575 - val_loss: 0.1503 - val_acc: 0.9545\n",
      "Epoch 10/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.1326 - acc: 0.9611 - val_loss: 0.1453 - val_acc: 0.9571\n",
      "Epoch 11/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.1268 - acc: 0.9635 - val_loss: 0.1510 - val_acc: 0.9523\n",
      "Epoch 12/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.1218 - acc: 0.9637 - val_loss: 0.1798 - val_acc: 0.9465\n",
      "Epoch 13/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.1146 - acc: 0.9652 - val_loss: 0.1534 - val_acc: 0.9556\n",
      "Epoch 14/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.1130 - acc: 0.9668 - val_loss: 0.1457 - val_acc: 0.9550\n",
      "Epoch 15/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.1117 - acc: 0.9673 - val_loss: 0.1684 - val_acc: 0.9514\n",
      "Epoch 16/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.1064 - acc: 0.9681 - val_loss: 0.1498 - val_acc: 0.9576\n",
      "Epoch 17/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.1049 - acc: 0.9691 - val_loss: 0.1415 - val_acc: 0.9595\n",
      "Epoch 18/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.0997 - acc: 0.9698 - val_loss: 0.1481 - val_acc: 0.9587\n",
      "Epoch 19/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.1005 - acc: 0.9692 - val_loss: 0.1425 - val_acc: 0.9581\n",
      "Epoch 20/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.0990 - acc: 0.9702 - val_loss: 0.1423 - val_acc: 0.9605\n",
      "Epoch 21/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.0976 - acc: 0.9704 - val_loss: 0.1700 - val_acc: 0.9498\n",
      "Epoch 22/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.0920 - acc: 0.9721 - val_loss: 0.1487 - val_acc: 0.9584\n",
      "Epoch 23/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.0905 - acc: 0.9725 - val_loss: 0.1475 - val_acc: 0.9583\n",
      "Epoch 24/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.0910 - acc: 0.9724 - val_loss: 0.1484 - val_acc: 0.9590\n",
      "Epoch 25/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.0880 - acc: 0.9725 - val_loss: 0.1506 - val_acc: 0.9576\n",
      "Epoch 26/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.0867 - acc: 0.9730 - val_loss: 0.1498 - val_acc: 0.9596\n",
      "Epoch 27/30\n",
      "55771/55771 [==============================] - 2s 37us/sample - loss: 0.0864 - acc: 0.9736 - val_loss: 0.1482 - val_acc: 0.9605\n",
      "Epoch 28/30\n",
      "55771/55771 [==============================] - 2s 41us/sample - loss: 0.0837 - acc: 0.9735 - val_loss: 0.1496 - val_acc: 0.9609\n",
      "Epoch 29/30\n",
      "55771/55771 [==============================] - 2s 43us/sample - loss: 0.0810 - acc: 0.9751 - val_loss: 0.1579 - val_acc: 0.9543\n",
      "Epoch 30/30\n",
      "55771/55771 [==============================] - 2s 40us/sample - loss: 0.0829 - acc: 0.9750 - val_loss: 0.1620 - val_acc: 0.9548\n",
      "test loss for 6th run:  0.16200211677771295\n",
      "test accuracy for 6th run:  0.9548\n",
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 55771 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "54848/55771 [============================>.] - ETA: 0s - loss: 0.6036 - acc: 0.8182"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55771/55771 [==============================] - 2s 38us/sample - loss: 0.5986 - acc: 0.8197 - val_loss: 0.2440 - val_acc: 0.9262\n",
      "Epoch 2/30\n",
      "55771/55771 [==============================] - 2s 33us/sample - loss: 0.2952 - acc: 0.9129 - val_loss: 0.2198 - val_acc: 0.9337\n",
      "Epoch 3/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.2405 - acc: 0.9301 - val_loss: 0.1935 - val_acc: 0.9399\n",
      "Epoch 4/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.2104 - acc: 0.9378 - val_loss: 0.2137 - val_acc: 0.9329\n",
      "Epoch 5/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.1870 - acc: 0.9447 - val_loss: 0.1775 - val_acc: 0.9485\n",
      "Epoch 6/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1711 - acc: 0.9495 - val_loss: 0.1702 - val_acc: 0.9491\n",
      "Epoch 7/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.1624 - acc: 0.9510 - val_loss: 0.1661 - val_acc: 0.9483\n",
      "Epoch 8/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1520 - acc: 0.9545 - val_loss: 0.1592 - val_acc: 0.9517\n",
      "Epoch 9/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1429 - acc: 0.9575 - val_loss: 0.1673 - val_acc: 0.9509\n",
      "Epoch 10/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1382 - acc: 0.9588 - val_loss: 0.1499 - val_acc: 0.9585\n",
      "Epoch 11/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1283 - acc: 0.9622 - val_loss: 0.1489 - val_acc: 0.9570\n",
      "Epoch 12/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1253 - acc: 0.9627 - val_loss: 0.1628 - val_acc: 0.9526\n",
      "Epoch 13/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1220 - acc: 0.9625 - val_loss: 0.1508 - val_acc: 0.9585\n",
      "Epoch 14/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1192 - acc: 0.9631 - val_loss: 0.1491 - val_acc: 0.9587\n",
      "Epoch 15/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1124 - acc: 0.9653 - val_loss: 0.1552 - val_acc: 0.9566\n",
      "Epoch 16/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1114 - acc: 0.9657 - val_loss: 0.1385 - val_acc: 0.9632\n",
      "Epoch 17/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1078 - acc: 0.9665 - val_loss: 0.1492 - val_acc: 0.9589\n",
      "Epoch 18/30\n",
      "55771/55771 [==============================] - 2s 33us/sample - loss: 0.1061 - acc: 0.9668 - val_loss: 0.1529 - val_acc: 0.9564\n",
      "Epoch 19/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1038 - acc: 0.9682 - val_loss: 0.1692 - val_acc: 0.9517\n",
      "Epoch 20/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1002 - acc: 0.9686 - val_loss: 0.1627 - val_acc: 0.9545\n",
      "Epoch 21/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.0990 - acc: 0.9692 - val_loss: 0.1493 - val_acc: 0.9574\n",
      "Epoch 22/30\n",
      "55771/55771 [==============================] - 2s 33us/sample - loss: 0.0971 - acc: 0.9701 - val_loss: 0.1546 - val_acc: 0.9579\n",
      "Epoch 23/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.0961 - acc: 0.9703 - val_loss: 0.1593 - val_acc: 0.9563\n",
      "Epoch 24/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.0925 - acc: 0.9711 - val_loss: 0.1628 - val_acc: 0.9566\n",
      "Epoch 25/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.0909 - acc: 0.9712 - val_loss: 0.1740 - val_acc: 0.9514\n",
      "Epoch 26/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.0938 - acc: 0.9712 - val_loss: 0.1412 - val_acc: 0.9629\n",
      "Epoch 27/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.0891 - acc: 0.9718 - val_loss: 0.1629 - val_acc: 0.9592\n",
      "Epoch 28/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.0918 - acc: 0.9705 - val_loss: 0.1579 - val_acc: 0.9570\n",
      "Epoch 29/30\n",
      "55771/55771 [==============================] - 2s 33us/sample - loss: 0.0859 - acc: 0.9733 - val_loss: 0.1571 - val_acc: 0.9548\n",
      "Epoch 30/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.0834 - acc: 0.9743 - val_loss: 0.1657 - val_acc: 0.9562\n",
      "test loss for 7th run:  0.16568376488797368\n",
      "test accuracy for 7th run:  0.9562\n",
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_27 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 55771 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "55744/55771 [============================>.] - ETA: 0s - loss: 0.5813 - acc: 0.8222"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55771/55771 [==============================] - 2s 42us/sample - loss: 0.5813 - acc: 0.8222 - val_loss: 0.2543 - val_acc: 0.9202\n",
      "Epoch 2/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.2959 - acc: 0.9122 - val_loss: 0.2132 - val_acc: 0.9310\n",
      "Epoch 3/30\n",
      "55771/55771 [==============================] - 2s 32us/sample - loss: 0.2398 - acc: 0.9288 - val_loss: 0.1823 - val_acc: 0.9454\n",
      "Epoch 4/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.2123 - acc: 0.9367 - val_loss: 0.1746 - val_acc: 0.9464\n",
      "Epoch 5/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.1866 - acc: 0.9445 - val_loss: 0.1611 - val_acc: 0.9507\n",
      "Epoch 6/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1728 - acc: 0.9486 - val_loss: 0.1615 - val_acc: 0.9504\n",
      "Epoch 7/30\n",
      "55771/55771 [==============================] - 2s 33us/sample - loss: 0.1639 - acc: 0.9514 - val_loss: 0.1561 - val_acc: 0.9501\n",
      "Epoch 8/30\n",
      "55771/55771 [==============================] - 2s 33us/sample - loss: 0.1531 - acc: 0.9550 - val_loss: 0.1473 - val_acc: 0.9546\n",
      "Epoch 9/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.1462 - acc: 0.9561 - val_loss: 0.1561 - val_acc: 0.9498\n",
      "Epoch 10/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.1388 - acc: 0.9585 - val_loss: 0.1536 - val_acc: 0.9545\n",
      "Epoch 11/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1353 - acc: 0.9595 - val_loss: 0.1427 - val_acc: 0.9561\n",
      "Epoch 12/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1283 - acc: 0.9622 - val_loss: 0.1444 - val_acc: 0.9573\n",
      "Epoch 13/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1242 - acc: 0.9624 - val_loss: 0.1520 - val_acc: 0.9535\n",
      "Epoch 14/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.1231 - acc: 0.9630 - val_loss: 0.1447 - val_acc: 0.9576\n",
      "Epoch 15/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1156 - acc: 0.9641 - val_loss: 0.1561 - val_acc: 0.9541\n",
      "Epoch 16/30\n",
      "55771/55771 [==============================] - 2s 34us/sample - loss: 0.1139 - acc: 0.9656 - val_loss: 0.1350 - val_acc: 0.9589\n",
      "Epoch 17/30\n",
      "55771/55771 [==============================] - 2s 35us/sample - loss: 0.1129 - acc: 0.9650 - val_loss: 0.1368 - val_acc: 0.9598\n",
      "Epoch 18/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.1085 - acc: 0.9671 - val_loss: 0.1338 - val_acc: 0.9587\n",
      "Epoch 19/30\n",
      "55771/55771 [==============================] - 2s 41us/sample - loss: 0.1048 - acc: 0.9680 - val_loss: 0.1522 - val_acc: 0.9554\n",
      "Epoch 20/30\n",
      "55771/55771 [==============================] - 2s 39us/sample - loss: 0.1053 - acc: 0.9675 - val_loss: 0.1458 - val_acc: 0.9577\n",
      "Epoch 21/30\n",
      "55771/55771 [==============================] - 2s 40us/sample - loss: 0.1024 - acc: 0.9683 - val_loss: 0.1352 - val_acc: 0.9614\n",
      "Epoch 22/30\n",
      "55771/55771 [==============================] - 2s 39us/sample - loss: 0.0952 - acc: 0.9703 - val_loss: 0.1352 - val_acc: 0.9623\n",
      "Epoch 23/30\n",
      "55771/55771 [==============================] - 2s 40us/sample - loss: 0.0962 - acc: 0.9702 - val_loss: 0.1430 - val_acc: 0.9592\n",
      "Epoch 24/30\n",
      "55771/55771 [==============================] - 2s 38us/sample - loss: 0.0954 - acc: 0.9699 - val_loss: 0.1432 - val_acc: 0.9579\n",
      "Epoch 25/30\n",
      "55771/55771 [==============================] - 2s 38us/sample - loss: 0.0955 - acc: 0.9706 - val_loss: 0.1365 - val_acc: 0.9620\n",
      "Epoch 26/30\n",
      "55771/55771 [==============================] - 2s 38us/sample - loss: 0.0918 - acc: 0.9722 - val_loss: 0.1330 - val_acc: 0.9631\n",
      "Epoch 27/30\n",
      "55771/55771 [==============================] - 2s 38us/sample - loss: 0.0930 - acc: 0.9711 - val_loss: 0.1430 - val_acc: 0.9603\n",
      "Epoch 28/30\n",
      "55771/55771 [==============================] - 2s 39us/sample - loss: 0.0870 - acc: 0.9729 - val_loss: 0.1664 - val_acc: 0.9552\n",
      "Epoch 29/30\n",
      "55771/55771 [==============================] - 2s 38us/sample - loss: 0.0864 - acc: 0.9728 - val_loss: 0.1566 - val_acc: 0.9558\n",
      "Epoch 30/30\n",
      "55771/55771 [==============================] - 2s 38us/sample - loss: 0.0874 - acc: 0.9724 - val_loss: 0.1462 - val_acc: 0.9597\n",
      "test loss for 8th run:  0.14621631766664794\n",
      "test accuracy for 8th run:  0.9597\n",
      "(7936, 784)\n",
      "(7936,)\n",
      "0\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,266\n",
      "Trainable params: 28,074\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 55771 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "55488/55771 [============================>.] - ETA: 0s - loss: 0.6122 - acc: 0.8168"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adamu/data/projects/notebooks/UCI_encoders/imbalance_venv/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55771/55771 [==============================] - 3s 45us/sample - loss: 0.6115 - acc: 0.8171 - val_loss: 0.2596 - val_acc: 0.9206\n",
      "Epoch 2/30\n",
      "55771/55771 [==============================] - 2s 41us/sample - loss: 0.2950 - acc: 0.9149 - val_loss: 0.2423 - val_acc: 0.9276\n",
      "Epoch 3/30\n",
      "55771/55771 [==============================] - 2s 40us/sample - loss: 0.2414 - acc: 0.9304 - val_loss: 0.2403 - val_acc: 0.9281\n",
      "Epoch 4/30\n",
      "55771/55771 [==============================] - 2s 39us/sample - loss: 0.2120 - acc: 0.9383 - val_loss: 0.1910 - val_acc: 0.9445\n",
      "Epoch 5/30\n",
      "55771/55771 [==============================] - 2s 38us/sample - loss: 0.1929 - acc: 0.9431 - val_loss: 0.1804 - val_acc: 0.9452\n",
      "Epoch 6/30\n",
      "55771/55771 [==============================] - 2s 38us/sample - loss: 0.1740 - acc: 0.9486 - val_loss: 0.1661 - val_acc: 0.9486\n",
      "Epoch 7/30\n",
      "55771/55771 [==============================] - 2s 37us/sample - loss: 0.1642 - acc: 0.9515 - val_loss: 0.1497 - val_acc: 0.9547\n",
      "Epoch 8/30\n",
      "55771/55771 [==============================] - 2s 37us/sample - loss: 0.1485 - acc: 0.9568 - val_loss: 0.1625 - val_acc: 0.9517\n",
      "Epoch 9/30\n",
      "55771/55771 [==============================] - 2s 39us/sample - loss: 0.1453 - acc: 0.9577 - val_loss: 0.1563 - val_acc: 0.9560\n",
      "Epoch 10/30\n",
      "55771/55771 [==============================] - 2s 41us/sample - loss: 0.1387 - acc: 0.9592 - val_loss: 0.1492 - val_acc: 0.9536\n",
      "Epoch 11/30\n",
      "55771/55771 [==============================] - 2s 40us/sample - loss: 0.1316 - acc: 0.9599 - val_loss: 0.1525 - val_acc: 0.9544\n",
      "Epoch 12/30\n",
      "55771/55771 [==============================] - 2s 38us/sample - loss: 0.1251 - acc: 0.9630 - val_loss: 0.1460 - val_acc: 0.9576\n",
      "Epoch 13/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.1205 - acc: 0.9633 - val_loss: 0.1622 - val_acc: 0.9534\n",
      "Epoch 14/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.1224 - acc: 0.9622 - val_loss: 0.1388 - val_acc: 0.9599\n",
      "Epoch 15/30\n",
      "55771/55771 [==============================] - 2s 38us/sample - loss: 0.1156 - acc: 0.9655 - val_loss: 0.1458 - val_acc: 0.9574\n",
      "Epoch 16/30\n",
      "55771/55771 [==============================] - 2s 37us/sample - loss: 0.1095 - acc: 0.9660 - val_loss: 0.1660 - val_acc: 0.9534\n",
      "Epoch 17/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.1086 - acc: 0.9677 - val_loss: 0.1365 - val_acc: 0.9597\n",
      "Epoch 18/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.1074 - acc: 0.9673 - val_loss: 0.1451 - val_acc: 0.9588\n",
      "Epoch 19/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.0997 - acc: 0.9696 - val_loss: 0.1382 - val_acc: 0.9602\n",
      "Epoch 20/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.1032 - acc: 0.9693 - val_loss: 0.1505 - val_acc: 0.9569\n",
      "Epoch 21/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.0969 - acc: 0.9696 - val_loss: 0.1552 - val_acc: 0.9547\n",
      "Epoch 22/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.0985 - acc: 0.9695 - val_loss: 0.1425 - val_acc: 0.9596\n",
      "Epoch 23/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.0967 - acc: 0.9703 - val_loss: 0.1463 - val_acc: 0.9591\n",
      "Epoch 24/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.0952 - acc: 0.9710 - val_loss: 0.1420 - val_acc: 0.9612\n",
      "Epoch 25/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.0940 - acc: 0.9710 - val_loss: 0.1395 - val_acc: 0.9600\n",
      "Epoch 26/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.0947 - acc: 0.9702 - val_loss: 0.1433 - val_acc: 0.9580\n",
      "Epoch 27/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.0912 - acc: 0.9715 - val_loss: 0.1441 - val_acc: 0.9586\n",
      "Epoch 28/30\n",
      "55771/55771 [==============================] - 2s 39us/sample - loss: 0.0887 - acc: 0.9714 - val_loss: 0.1341 - val_acc: 0.9618\n",
      "Epoch 29/30\n",
      "55771/55771 [==============================] - 2s 36us/sample - loss: 0.0885 - acc: 0.9727 - val_loss: 0.1568 - val_acc: 0.9555\n",
      "Epoch 30/30\n",
      "55771/55771 [==============================] - 2s 37us/sample - loss: 0.0873 - acc: 0.9734 - val_loss: 0.1435 - val_acc: 0.9593\n",
      "test loss for 9th run:  0.14354719973870086\n",
      "test accuracy for 9th run:  0.9593\n"
     ]
    }
   ],
   "source": [
    "results_list =[]\n",
    "for i in range(10):\n",
    "    gen_samples, gen_labels = generate_samples()\n",
    "    X = np.concatenate([train_x, gen_samples])\n",
    "    Y = np.concatenate([np.reshape(gr_labels, -1), gen_labels])\n",
    "\n",
    "    Y_oh = np.array(tf.keras.utils.to_categorical(Y, num_classes=10, dtype='float32'))\n",
    "\n",
    "    aug_model = build_model()\n",
    "    aug_history = aug_model.fit(X, Y_oh, batch_size=batch_size,\n",
    "                        epochs=epochs, validation_data=(test_images, test_y))\n",
    "\n",
    "    aug_score = aug_model.evaluate(test_images, test_y, verbose=0)\n",
    "    print('test loss for {}th run: '.format(i), aug_score[0])\n",
    "    print('test accuracy for {}th run: '.format(i), aug_score[1] )\n",
    "    \n",
    "    y_pred_aug_oh = bl_model.predict(test_images)\n",
    "    y_pred_aug = y_pred_aug_oh.argmax(axis=-1)\n",
    "    results_list.append(classification_report(test_labels, y_pred_aug, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c05bd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7e13359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.994457</td>\n",
       "      <td>0.915306</td>\n",
       "      <td>0.953241</td>\n",
       "      <td>980.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.994429</td>\n",
       "      <td>0.943612</td>\n",
       "      <td>0.968354</td>\n",
       "      <td>1135.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.943044</td>\n",
       "      <td>0.978682</td>\n",
       "      <td>0.960533</td>\n",
       "      <td>1032.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.954457</td>\n",
       "      <td>0.975248</td>\n",
       "      <td>0.964740</td>\n",
       "      <td>1010.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.965657</td>\n",
       "      <td>0.973523</td>\n",
       "      <td>0.969574</td>\n",
       "      <td>982.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.961883</td>\n",
       "      <td>0.962423</td>\n",
       "      <td>892.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.938247</td>\n",
       "      <td>0.983299</td>\n",
       "      <td>0.960245</td>\n",
       "      <td>958.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.970789</td>\n",
       "      <td>0.969844</td>\n",
       "      <td>0.970316</td>\n",
       "      <td>1028.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.948171</td>\n",
       "      <td>0.957906</td>\n",
       "      <td>0.953013</td>\n",
       "      <td>974.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.971259</td>\n",
       "      <td>0.965042</td>\n",
       "      <td>1009.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.962900</td>\n",
       "      <td>0.962900</td>\n",
       "      <td>0.962900</td>\n",
       "      <td>0.9629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.963112</td>\n",
       "      <td>0.963056</td>\n",
       "      <td>0.962748</td>\n",
       "      <td>10000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.963557</td>\n",
       "      <td>0.962900</td>\n",
       "      <td>0.962888</td>\n",
       "      <td>10000.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.994457  0.915306  0.953241    980.0000\n",
       "1              0.994429  0.943612  0.968354   1135.0000\n",
       "2              0.943044  0.978682  0.960533   1032.0000\n",
       "3              0.954457  0.975248  0.964740   1010.0000\n",
       "4              0.965657  0.973523  0.969574    982.0000\n",
       "5              0.962963  0.961883  0.962423    892.0000\n",
       "6              0.938247  0.983299  0.960245    958.0000\n",
       "7              0.970789  0.969844  0.970316   1028.0000\n",
       "8              0.948171  0.957906  0.953013    974.0000\n",
       "9              0.958904  0.971259  0.965042   1009.0000\n",
       "accuracy       0.962900  0.962900  0.962900      0.9629\n",
       "macro avg      0.963112  0.963056  0.962748  10000.0000\n",
       "weighted avg   0.963557  0.962900  0.962888  10000.0000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df = pd.DataFrame(results_list[0]).transpose()\n",
    "print(len(results_list))\n",
    "for r_dict in results_list[1:]:\n",
    "    temp = pd.DataFrame(r_dict).transpose()\n",
    "    total_df = total_df.add(temp)\n",
    "    \n",
    "average_10x = total_df/10.0\n",
    "average_10x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac420661",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4726aa8c3a011139ee9eae324612f94ded5d9a6c6a3363e2331a9b86c3055c02"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
